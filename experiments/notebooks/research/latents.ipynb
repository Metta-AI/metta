{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Policy Performance Factor Analysis\n",
    "\n",
    "This notebook performs factor analysis on policy performance across multiple training runs using evaluation stats from the Metta stats database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete! Auto-reload enabled.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "print('Setup complete! Auto-reload enabled.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run_names",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runs to analyze: 2\n"
     ]
    }
   ],
   "source": [
    "# Training run names to analyze\n",
    "run_names = [\n",
    "    \"jacke.cyclical_8tick_timing_study_20250728_134443\",\n",
    "    \"yudhister.2x4_arena_lp_02_july25_fix_seed_incl_steps_fix_2\",\n",
    "    # \"jacke.cyclical_8tick_timing_study_20250728_134805\",\n",
    "    # \"george.operantconditioning.smoketestthree.07-29\",\n",
    "    # \"jacke.cyclical_11tick_timing_study_20250729_095309\",\n",
    "    # \"jacke.sky_nav_base_20250725_143311\",\n",
    "    # \"george.operantconditioning.smoketestfour.07-29\",\n",
    "    # \"zfogg.devbox.arena_nav_combined.07-29\",\n",
    "    # \"daphne.operantconditioning.backchaining.earlyterm.any.2.07-29\",\n",
    "    # \"jacke.sky_nav_grid_20250725_154822\",\n",
    "    # \"zfogg.skypilot.sim_all.gpu8.07-29\",\n",
    "    # \"daphne.operantconditioning.smoketesttwo.earlyterm.all.2.07-29\",\n",
    "    # \"jacke.cyclical_8tick_timing_study_20250729_095333\",\n",
    "    # \"yudhister.2x4_arena_lp_03_july25_fix_seed_incl_steps_fix_2\",\n",
    "    # \"jacke.sky_nav_grid_20250725_154808\",\n",
    "    # \"jacke.sky_nav_spiral_20250725_154729\",\n",
    "    # \"yudhister.2x4_arena_random_07_july25_fix_seed_incl_steps_fix_2\",\n",
    "    # \"yudhister.2x4_arena_lp_05_july25_fix_seed_incl_steps_fix_2\",\n",
    "    # \"daphne.operantconditioning.smoketestfour.earlyterm.all.2.07-29\",\n",
    "    # \"jacke.sky_random_nav_grid_spiral_20250725_154829\",\n",
    "    # \"nishad-0726-1148\",\n",
    "    # \"yudhister.2x4_arena_lp_01_july25_fix_seed_incl_steps_fix_2\",\n",
    "    # \"daphne.operantconditioning.smoketestthree.earlyterm.any.2.07-29\",\n",
    "    # \"daphne.navigation.earlyterm.half.2.07-28\",\n",
    "    # \"jacke.cyclical_15tick_timing_study_20250729_095339\",\n",
    "    # \"daphne.operantconditioning.smoketestthree.earlyterm.all.2.07-29\",\n",
    "    # \"jacke.cyclical_10tick_timing_study_20250729_095335\",\n",
    "    # \"bullm.navigation.low_reward.with_context.07-24\",\n",
    "    # \"absurdlybasictest5\",\n",
    "    # \"zfogg.skypilot.learning_progress.07-29.2\",\n",
    "    # \"yudhister.2x4_arena_lp_07_july25_fix_seed_incl_steps_fix_2\",\n",
    "    # \"jacke.sky_nav_grid_spiral_20250725_154838\",\n",
    "    # \"george.operantconditioning.smoketesttwo.07-29\",\n",
    "    # \"jacke.cyclical_11tick_timing_study_20250729_095304\",\n",
    "    # \"jacke.cyclical_6tick_timing_study_20250728_134441\",\n",
    "    # \"jacke.sky_nav_spiral_20250725_154737\",\n",
    "    # \"yudhister.2x4_arena_random_05_july25_fix_seed_incl_steps_fix_2\",\n",
    "    # \"daphne.operantconditioning.smoketestfour.earlyterm.any.2.07-29\",\n",
    "    # \"george.operantconditioning.backchaining.07-29\"\n",
    "]\n",
    "\n",
    "print(f\"Total runs to analyze: {len(run_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wandb_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Metta API at: https://api.observatory.softmax-research.net\n"
     ]
    }
   ],
   "source": [
    "# Initialize API client\n",
    "from metta.common.util.constants import PROD_STATS_SERVER_URI\n",
    "from metta.common.client.metta_client import MettaAPIClient\n",
    "\n",
    "client = MettaAPIClient(PROD_STATS_SERVER_URI)\n",
    "\n",
    "print(f\"Connected to Metta API at: {PROD_STATS_SERVER_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "get_run_ids",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching evaluation data for 2 training runs...\n",
      "ðŸš€ HeatmapWidget initialized successfully!\n",
      "ðŸ“Š Multi-metric data set with 2 policies and 6 evaluations\n",
      "ðŸ“ˆ Available metrics: reward\n",
      "ðŸ“ˆ Selected metric: reward\n",
      "Fetched data\n"
     ]
    }
   ],
   "source": [
    "from experiments.notebooks.utils.heatmap_widget.heatmap_widget.util import fetch_real_heatmap_data\n",
    "# Fetch policy data using the training run names\n",
    "print(f\"Fetching evaluation data for {len(run_names)} training runs...\")\n",
    "\n",
    "# Fetch heatmap data for all runs\n",
    "heatmap_data = await fetch_real_heatmap_data(\n",
    "    search_texts=run_names,\n",
    "    api_base_url=PROD_STATS_SERVER_URI,\n",
    "    metrics=[\"reward\"],\n",
    "    policy_selector=\"latest\",  # Get best policy from each training run\n",
    "    max_policies=100\n",
    ")\n",
    "\n",
    "print(\"Fetched data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "031310d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52e426c7e9e41eeb9bad3548136f3bb",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "HeatmapWidget(heatmap_data={'cells': {'jacke.cyclical_8tick_timing_study_20250728_134443:v43': {'arena/advanceâ€¦"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fetch_metrics",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HeatmapWidget' object has no attribute 'policyNames'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(rows)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Create dataframe\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m df = \u001b[43mheatmap_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheatmap_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated dataframe with shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFirst few columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df.columns[:\u001b[32m10\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mheatmap_to_dataframe\u001b[39m\u001b[34m(heatmap_data)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert heatmap data structure to a dataframe suitable for factor analysis.\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m rows = []\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m policy_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mheatmap_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpolicyNames\u001b[49m:\n\u001b[32m      7\u001b[39m     row = {\u001b[33m'\u001b[39m\u001b[33mpolicy_name\u001b[39m\u001b[33m'\u001b[39m: policy_name}\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Extract metrics across all evaluations\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'HeatmapWidget' object has no attribute 'policyNames'"
     ]
    }
   ],
   "source": [
    "# Convert heatmap data to dataframe for factor analysis\n",
    "def heatmap_to_dataframe(heatmap_data):\n",
    "    \"\"\"Convert heatmap data structure to a dataframe suitable for factor analysis.\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for policy_name in heatmap_data.policyNames:\n",
    "        row = {'policy_name': policy_name}\n",
    "        \n",
    "        # Extract metrics across all evaluations\n",
    "        for eval_name in heatmap_data.evalNames:\n",
    "            cell = heatmap_data.cells.get(policy_name, {}).get(eval_name, {})\n",
    "            metrics = cell.get('metrics', {})\n",
    "            \n",
    "            # Add each metric with eval_name prefix\n",
    "            for metric_name, value in metrics.items():\n",
    "                col_name = f\"{eval_name}_{metric_name}\"\n",
    "                row[col_name] = value\n",
    "        \n",
    "        # Add average scores\n",
    "        row['average_score'] = heatmap_data.policyAverageScores.get(policy_name, 0)\n",
    "        rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Create dataframe\n",
    "df = heatmap_to_dataframe(heatmap_data)\n",
    "print(f\"Created dataframe with shape: {df.shape}\")\n",
    "print(f\"\\nFirst few columns: {list(df.columns[:10])}\")\n",
    "print(f\"\\nDataframe head:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data overview and cleaning\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"\\nMissing values per column:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "print(missing_counts[missing_counts > 0].head(20))\n",
    "\n",
    "# Fill missing values with 0 (indicating no performance on that evaluation)\n",
    "df_filled = df.fillna(0)\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df_filled.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for factor analysis\n",
    "# Select only numeric columns (exclude policy_name)\n",
    "numeric_cols = df_filled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Found {len(numeric_cols)} numeric features\")\n",
    "\n",
    "# Create feature matrix\n",
    "X = df_filled[numeric_cols]\n",
    "\n",
    "# Remove columns with zero variance\n",
    "variance_filter = X.var() > 0\n",
    "X_filtered = X.loc[:, variance_filter]\n",
    "selected_features = X_filtered.columns.tolist()\n",
    "\n",
    "print(f\"After removing zero-variance features: {len(selected_features)} features\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_filtered)\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X_scaled.shape}\")\n",
    "print(f\"Number of policies: {X_scaled.shape[0]}\")\n",
    "print(f\"Number of features: {X_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_dataframe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA to understand variance structure\n",
    "if X_scaled.shape[0] > 1 and X_scaled.shape[1] > 0:\n",
    "    # Determine number of components\n",
    "    n_components = min(X_scaled.shape[0], X_scaled.shape[1])\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot explained variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, min(20, len(pca.explained_variance_ratio_)) + 1), \n",
    "             pca.explained_variance_ratio_[:20], \n",
    "             'bo-')\n",
    "    plt.xlabel('Component Number')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('PCA Explained Variance (First 20 Components)')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, min(20, len(pca.explained_variance_ratio_)) + 1), \n",
    "             np.cumsum(pca.explained_variance_ratio_[:20]), \n",
    "             'ro-')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Cumulative Explained Variance')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print variance explained by first few components\n",
    "    print(\"Variance explained by first 10 components:\")\n",
    "    for i in range(min(10, len(pca.explained_variance_ratio_))):\n",
    "        print(f\"PC{i+1}: {pca.explained_variance_ratio_[i]:.3f} ({np.cumsum(pca.explained_variance_ratio_)[i]:.3f} cumulative)\")\n",
    "else:\n",
    "    print(\"Not enough data for PCA analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Factor Analysis\n",
    "if X_scaled.shape[0] > 3 and X_scaled.shape[1] > 3:\n",
    "    # Choose number of factors based on PCA results\n",
    "    n_factors = min(5, X_scaled.shape[0] - 1, X_scaled.shape[1])  # Start with 5 factors or less\n",
    "    \n",
    "    print(f\"Performing factor analysis with {n_factors} factors...\")\n",
    "    \n",
    "    fa = FactorAnalysis(n_components=n_factors, random_state=42)\n",
    "    factors = fa.fit_transform(X_scaled)\n",
    "    \n",
    "    # Get factor loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        fa.components_.T,\n",
    "        columns=[f'Factor_{i+1}' for i in range(n_factors)],\n",
    "        index=selected_features\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFactor analysis complete. Shape of factor scores: {factors.shape}\")\n",
    "    print(\"\\nTop loadings for each factor:\")\n",
    "    \n",
    "    for i in range(n_factors):\n",
    "        print(f\"\\nFactor {i+1}:\")\n",
    "        factor_col = f'Factor_{i+1}'\n",
    "        top_positive = loadings[factor_col].nlargest(5)\n",
    "        top_negative = loadings[factor_col].nsmallest(5)\n",
    "        \n",
    "        print(\"  Top positive loadings:\")\n",
    "        for feat, loading in top_positive.items():\n",
    "            print(f\"    {feat}: {loading:.3f}\")\n",
    "        \n",
    "        print(\"  Top negative loadings:\")\n",
    "        for feat, loading in top_negative.items():\n",
    "            print(f\"    {feat}: {loading:.3f}\")\n",
    "else:\n",
    "    print(\"Not enough data for factor analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize factor loadings heatmap\n",
    "if 'loadings' in locals():\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Select top features with highest absolute loadings\n",
    "    abs_loadings = loadings.abs()\n",
    "    max_loadings = abs_loadings.max(axis=1)\n",
    "    top_features_idx = max_loadings.nlargest(30).index\n",
    "    \n",
    "    # Create subset of loadings for visualization\n",
    "    loadings_subset = loadings.loc[top_features_idx]\n",
    "    \n",
    "    sns.heatmap(loadings_subset.T, cmap='RdBu_r', center=0, \n",
    "                annot=True, fmt='.2f', \n",
    "                cbar_kws={'label': 'Loading'})\n",
    "    plt.title('Factor Loadings Heatmap (Top 30 Features)')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Factors')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add factor scores to dataframe and analyze\n",
    "if 'factors' in locals():\n",
    "    # Add factor scores to original dataframe\n",
    "    for i in range(n_factors):\n",
    "        df[f'factor_{i+1}'] = factors[:, i]\n",
    "    \n",
    "    # Show policies with highest scores on each factor\n",
    "    print(\"Top policies by factor scores:\\n\")\n",
    "    for i in range(n_factors):\n",
    "        print(f\"Factor {i+1} - Top 5 policies:\")\n",
    "        top_policies = df.nlargest(5, f'factor_{i+1}')[['policy_name', f'factor_{i+1}']]\n",
    "        for _, row in top_policies.iterrows():\n",
    "            print(f\"  {row['policy_name'][:60]:60} {row[f'factor_{i+1}']:.3f}\")\n",
    "        print()\n",
    "    \n",
    "    # Also show bottom 5 for contrast\n",
    "    print(\"\\nBottom policies by factor scores:\\n\")\n",
    "    for i in range(n_factors):\n",
    "        print(f\"Factor {i+1} - Bottom 5 policies:\")\n",
    "        bottom_policies = df.nsmallest(5, f'factor_{i+1}')[['policy_name', f'factor_{i+1}']]\n",
    "        for _, row in bottom_policies.iterrows():\n",
    "            print(f\"  {row['policy_name'][:60]:60} {row[f'factor_{i+1}']:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "factor_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of factor scores\n",
    "if 'factors' in locals() and n_factors >= 2:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot first two factors\n",
    "    plt.scatter(df['factor_1'], df['factor_2'], alpha=0.6, s=50)\n",
    "    \n",
    "    # Annotate interesting points (high/low on either factor)\n",
    "    threshold = 1.5\n",
    "    for idx, row in df.iterrows():\n",
    "        if abs(row['factor_1']) > threshold or abs(row['factor_2']) > threshold:\n",
    "            # Extract just the run name part for cleaner labels\n",
    "            label = row['policy_name'].split('.')[-1][:20] if '.' in row['policy_name'] else row['policy_name'][:20]\n",
    "            plt.annotate(label, \n",
    "                        (row['factor_1'], row['factor_2']),\n",
    "                        fontsize=8, alpha=0.7,\n",
    "                        xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.xlabel('Factor 1')\n",
    "    plt.ylabel('Factor 2')\n",
    "    plt.title('Policy Performance Factor Analysis - Factor Space')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.2)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.2)\n",
    "    plt.show()\n",
    "    \n",
    "    # If we have more factors, show factor 1 vs 3\n",
    "    if n_factors >= 3:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(df['factor_1'], df['factor_3'], alpha=0.6, s=50)\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            if abs(row['factor_1']) > threshold or abs(row['factor_3']) > threshold:\n",
    "                label = row['policy_name'].split('.')[-1][:20] if '.' in row['policy_name'] else row['policy_name'][:20]\n",
    "                plt.annotate(label, \n",
    "                            (row['factor_1'], row['factor_3']),\n",
    "                            fontsize=8, alpha=0.7,\n",
    "                            xytext=(5, 5), textcoords='offset points')\n",
    "        \n",
    "        plt.xlabel('Factor 1')\n",
    "        plt.ylabel('Factor 3')\n",
    "        plt.title('Policy Performance Factor Analysis - Factors 1 vs 3')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.2)\n",
    "        plt.axvline(x=0, color='k', linestyle='-', alpha=0.2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_loadings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret factors by grouping features\n",
    "if 'loadings' in locals():\n",
    "    print(\"Factor Interpretation:\\n\")\n",
    "    \n",
    "    for i in range(n_factors):\n",
    "        print(f\"Factor {i+1} Interpretation:\")\n",
    "        factor_col = f'Factor_{i+1}'\n",
    "        \n",
    "        # Group features by evaluation task and metric type\n",
    "        eval_groups = {}\n",
    "        metric_groups = {}\n",
    "        \n",
    "        for feature in loadings.index:\n",
    "            loading = loadings.loc[feature, factor_col]\n",
    "            if abs(loading) > 0.3:  # Only consider significant loadings\n",
    "                # Parse feature name (format: eval_name_metric)\n",
    "                parts = feature.split('_')\n",
    "                if len(parts) >= 2:\n",
    "                    # Assume format is evalcategory/envname_metric\n",
    "                    eval_part = parts[0] if '/' not in parts[0] else parts[0].split('/')[0]\n",
    "                    metric_part = '_'.join(parts[1:]) if len(parts) > 1 else parts[-1]\n",
    "                    \n",
    "                    if eval_part not in eval_groups:\n",
    "                        eval_groups[eval_part] = []\n",
    "                    eval_groups[eval_part].append((feature, loading))\n",
    "                    \n",
    "                    if metric_part not in metric_groups:\n",
    "                        metric_groups[metric_part] = []\n",
    "                    metric_groups[metric_part].append((feature, loading))\n",
    "        \n",
    "        # Show patterns\n",
    "        print(f\"  Evaluation categories with high loadings:\")\n",
    "        for eval_cat, features in sorted(eval_groups.items(), key=lambda x: -len(x[1]))[:5]:\n",
    "            avg_loading = np.mean([f[1] for f in features])\n",
    "            print(f\"    {eval_cat}: {len(features)} features, avg loading: {avg_loading:.3f}\")\n",
    "        \n",
    "        print(f\"  Metric types with high loadings:\")\n",
    "        for metric, features in sorted(metric_groups.items(), key=lambda x: -len(x[1]))[:5]:\n",
    "            avg_loading = np.mean([f[1] for f in features])\n",
    "            print(f\"    {metric}: {len(features)} occurrences, avg loading: {avg_loading:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "factor_scores",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between factors and average performance\n",
    "if 'factors' in locals() and 'average_score' in df.columns:\n",
    "    correlations = []\n",
    "    for i in range(n_factors):\n",
    "        corr = df[f'factor_{i+1}'].corr(df['average_score'])\n",
    "        correlations.append(corr)\n",
    "        print(f\"Factor {i+1} correlation with average score: {corr:.3f}\")\n",
    "    \n",
    "    # Visualize correlations\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(range(1, n_factors + 1), correlations)\n",
    "    plt.xlabel('Factor')\n",
    "    plt.ylabel('Correlation with Average Score')\n",
    "    plt.title('Factor Correlations with Overall Performance')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scatter_factors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_dir = Path(\"./factor_analysis_results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save policy factor scores\n",
    "if 'factors' in locals():\n",
    "    factor_cols = ['policy_name'] + [f'factor_{i+1}' for i in range(n_factors)] + ['average_score']\n",
    "    df[factor_cols].to_csv(output_dir / \"policy_factor_scores.csv\", index=False)\n",
    "    print(f\"Saved policy factor scores to {output_dir / 'policy_factor_scores.csv'}\")\n",
    "\n",
    "# Save factor loadings\n",
    "if 'loadings' in locals():\n",
    "    loadings.to_csv(output_dir / \"factor_loadings.csv\")\n",
    "    print(f\"Saved factor loadings to {output_dir / 'factor_loadings.csv'}\")\n",
    "\n",
    "# Save summary statistics\n",
    "if 'factors' in locals():\n",
    "    with open(output_dir / \"factor_analysis_summary.txt\", \"w\") as f:\n",
    "        f.write(\"Factor Analysis Summary\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Number of policies analyzed: {len(df)}\\n\")\n",
    "        f.write(f\"Number of features: {len(selected_features)}\\n\")\n",
    "        f.write(f\"Number of factors extracted: {n_factors}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Factor correlations with average score:\\n\")\n",
    "        for i in range(n_factors):\n",
    "            corr = df[f'factor_{i+1}'].corr(df['average_score'])\n",
    "            f.write(f\"  Factor {i+1}: {corr:.3f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nVariance explained (from PCA):\\n\")\n",
    "        for i in range(min(n_factors, len(pca.explained_variance_ratio_))):\n",
    "            f.write(f\"  PC{i+1}: {pca.explained_variance_ratio_[i]:.3f}\\n\")\n",
    "    \n",
    "    print(f\"Saved analysis summary to {output_dir / 'factor_analysis_summary.txt'}\")\n",
    "\n",
    "print(f\"\\nAll results saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpret_factors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret factors by looking at highest loading features\n",
    "for i in range(n_factors):\n",
    "    print(f\"\\nFactor {i+1} - Top loading features:\")\n",
    "    factor_col = f'Factor_{i+1}'\n",
    "    top_positive = loadings[factor_col].nlargest(3)\n",
    "    top_negative = loadings[factor_col].nsmallest(3)\n",
    "    \n",
    "    print(\"  Positive loadings:\")\n",
    "    for feat, loading in top_positive.items():\n",
    "        print(f\"    {feat}: {loading:.3f}\")\n",
    "    \n",
    "    print(\"  Negative loadings:\")\n",
    "    for feat, loading in top_negative.items():\n",
    "        print(f\"    {feat}: {loading:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_dir = Path(\"./factor_analysis_results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save factor scores\n",
    "df.to_csv(output_dir / \"policy_factor_scores.csv\", index=False)\n",
    "\n",
    "# Save loadings\n",
    "loadings.to_csv(output_dir / \"factor_loadings.csv\")\n",
    "\n",
    "print(f\"Results saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
