name: "Test and Benchmark"

# This workflow uses metta ci as the single source of truth for CI checks.
# Each job calls metta ci with a specific stage:
#   --stage lint, --stage python-tests-and-benchmarks,
#   --stage cpp-tests, --stage cpp-benchmarks, --stage nim-tests, --stage recipe-tests
# Local development can run all stages with: metta ci

on:
  pull_request:
    types: [opened, synchronize, ready_for_review, labeled]
  push:
    branches: [main]
  merge_group:
    types: [checks_requested]
    branches: [main]
  workflow_dispatch:
    inputs:
      run_test:
        description: "Run tests"
        type: boolean
        default: true
      run_benchmark:
        description: "Run benchmarks (requires tests)"
        type: boolean
        default: true
      run_id:
        description: "Optional unique identifier for this run (for parallel CI benchmarking)"
        required: false
        type: string

concurrency:
  group: ${{ github.workflow }}-${{ github.event_name == 'merge_group' && github.event.merge_group.head_ref || github.ref }}-${{ inputs.run_id || 'default' }}
  cancel-in-progress: true

# Set default permissions
permissions:
  checks: write
  pull-requests: write

env:
  HYDRA_FULL_ERROR: 1
  VENV_PATH: .venv
  EVAL_SMOKE_TEST_POLICY: m.av.replay.probe.751 # just the name, no need to include any uri preamble like "wandb://run/"
  EVAL_SMOKE_TEST_MAX_ATTEMPTS: 3
  EVAL_SMOKE_TEST_MIN_REWARD: 0.0

jobs:
  echo-run-id:
    name: Echo run_id
    runs-on: ubuntu-latest
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.run_id != '' }}
    steps:
      - name: Echo run_id
        run: echo "RUN_ID=${{ inputs.run_id }}"

  # check if CI should run based on Graphite's stack position
  graphite-ci-optimizer:
    name: "Graphite CI Optimizer"
    if: |
      !contains(github.event.pull_request.labels.*.name, 'skip-ci') ||
      github.event_name == 'push' ||
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'merge_group'
    runs-on: ubuntu-latest
    outputs:
      should_skip: ${{ steps.graphite_ci.outputs.skip || 'false' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Detect PR context
        id: detect-pr
        uses: ./.github/actions/detect-external-pr

      - name: Graphite CI Optimizer
        id: graphite_ci
        if: steps.detect-pr.outputs.is_external != 'true'
        uses: withgraphite/graphite-ci-action@main
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          graphite_token: ${{ secrets.GRAPHITE_TOKEN }}

  # check if any source code files have changed
  setup-checks:
    name: "Set up for source code checks"
    needs: graphite-ci-optimizer
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (!contains(github.event.pull_request.labels.*.name, 'skip-ci') ||
       github.event_name == 'push' ||
       github.event_name == 'workflow_dispatch' ||
       github.event_name == 'merge_group')
    runs-on: ubuntu-latest
    outputs:
      has_relevant_changes: ${{ steps.check_py_files.outputs.has_relevant_changes }}
      app_backend_has_changes: ${{ steps.check_app_backend_files.outputs.has_relevant_changes }}
      run_test: ${{ steps.determine_tasks.outputs.run_test }}
      run_benchmark: ${{ steps.determine_tasks.outputs.run_benchmark }}
      is_external: ${{ steps.detect-pr.outputs.is_external }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Detect PR context
        id: detect-pr
        uses: ./.github/actions/detect-external-pr

      - name: Check for file changes
        id: check_py_files
        uses: ./.github/actions/file-changes
        with:
          patterns: "**/*.py,**/*.cpp,**/*.hpp,**/*.md,**/*.toml,**/conftest.py,**/pyproject.toml,**/pytest.ini,.github/workflows/*.yml"
          specific_files: "uv.lock"
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Check for app_backend file changes
        id: check_app_backend_files
        uses: ./.github/actions/file-changes
        with:
          patterns: "app_backend/**"
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Determine which tasks to run
        id: determine_tasks
        run: |
          # Default behavior based on event type and file changes
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            # Use workflow dispatch inputs for manual runs
            RUN_TEST="${{ github.event.inputs.run_test }}"
            RUN_BENCHMARK="${{ github.event.inputs.run_benchmark }}"
          elif [[ "${{ github.event_name }}" == "pull_request" && "${{ steps.check_py_files.outputs.has_relevant_changes }}" == "false" ]]; then
            # Skip everything for PRs with no relevant changes=
            RUN_TEST="false"
            RUN_BENCHMARK="false"
            echo "::notice title=Skipping Tasks::Skipping all tasks because no relevant files have changed"
          else
            # Default to running everything for other events or when changes exist
            RUN_TEST="true"
            RUN_BENCHMARK="true"
          fi

          # Output the decisions
          echo "run_test=${RUN_TEST}" >> $GITHUB_OUTPUT
          echo "run_benchmark=${RUN_BENCHMARK}" >> $GITHUB_OUTPUT

          echo "run test? ${RUN_TEST}"
          echo "run benchmark? ${RUN_BENCHMARK}"

  lint:
    name: "Lint"
    needs: [graphite-ci-optimizer, setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - uses: pnpm/action-setup@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 24
          cache: pnpm
          cache-dependency-path: pnpm-lock.yaml

      - name: Install pnpm dependencies
        run: pnpm install

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          install-mode: "testing"

      - name: Run linting and formatting
        run: uv run python .github/scripts/run_ci_stage.py lint

  python-tests:
    name: "Tests - Python"
    needs: [graphite-ci-optimizer, setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_test == 'true')
    runs-on: blacksmith-8vcpu-ubuntu-2404
    timeout-minutes: 30
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment

      - name: Cache PufferLib clone
        uses: actions/cache@v4
        id: cache-pufferlib
        with:
          path: ~/pufferlib-cache
          key: pufferlib-clone-${{ runner.os }}-${{ hashFiles('**/test_pufferlib_integration.py') }}
          restore-keys: |
            pufferlib-clone-${{ runner.os }}-

      - name: Set PufferLib cache path
        run: |
          echo "PUFFERLIB_CACHE_DIR=$HOME/pufferlib-cache" >> $GITHUB_ENV
          mkdir -p $HOME/pufferlib-cache

      - name: Configure Datadog
        if: needs.setup-checks.outputs.is_external != 'true'
        uses: datadog/test-visibility-github-action@v2
        with:
          languages: python
          api_key: ${{ secrets.DD_API_KEY }}

      - name: Run Python tests and benchmarks
        shell: bash
        env:
          DD_CIVISIBILITY_LOG_LEVEL: warning
        run: |
          set -eo pipefail
          ARGS=()
          if [[ "${{ needs.setup-checks.outputs.app_backend_has_changes }}" != "true" ]]; then
            ARGS+=(--skip-package app_backend)
          fi
          uv run python .github/scripts/run_ci_stage.py python-tests-and-benchmarks -- "${ARGS[@]}"

      - name: Python type checking
        shell: bash
        run: |
          set -eo pipefail
          uv run python .github/scripts/run_ci_stage.py pyright

  recipe-tests:
    name: "Tests - Recipes"
    needs: [graphite-ci-optimizer, setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_test == 'true')
    runs-on: ubuntu-8core
    timeout-minutes: 30
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          install-mode: "runtime"

      - name: Run Recipe tests
        shell: bash
        run: |
          set -eo pipefail
          uv run python .github/scripts/run_ci_stage.py recipe-tests --non-interactive

  cpp-tests:
    name: "Tests - C++ and Nim"
    needs: [graphite-ci-optimizer, setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_test == 'true')
    runs-on: ubuntu-4core
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          install-mode: "testing"

      - name: Setup Bazel caching
        run: |
          # Bazel's default ~/.cache/bazel contains internal state with symlinks,
          # lock files, and files with future timestamps (2035) that cause
          # "File exists" errors when actions/cache tries to extract them.
          #
          # Using --disk_cache and --repository_cache directories avoids these
          # issues by caching only reusable artifacts:
          # - Build outputs (disk_cache)
          # - Downloaded external dependencies (repository_cache)
          #
          # Documentation:
          # - https://bazel.build/reference/command-line-reference#flag--disk_cache
          # - https://bazel.build/reference/command-line-reference#flag--repository_cache
          # - https://bazel.build/remote/caching#disk-cache

          echo "build --disk_cache=$HOME/.bazel-disk-cache" >> packages/mettagrid/.bazelrc
          echo "build --repository_cache=$HOME/.bazel-repo-cache" >> packages/mettagrid/.bazelrc

          mkdir -p $HOME/.bazel-disk-cache
          mkdir -p $HOME/.bazel-repo-cache

      - name: Cache Bazel build artifacts
        uses: actions/cache@v4
        id: bazel-cache
        with:
          path: |
            ~/.bazel-disk-cache
            ~/.bazel-repo-cache
          key: bazel-${{ runner.os }}-${{ hashFiles('packages/mettagrid/MODULE.bazel', 'packages/mettagrid/.bazelrc', '.bazelversion') }}
          restore-keys: |
            bazel-${{ runner.os }}-

      - name: Run C++ tests
        id: mettagrid-cpp-tests
        run: uv run python .github/scripts/run_ci_stage.py cpp-tests

      - name: Run C++ benchmarks
        id: mettagrid-cpp-benchmarks
        run: uv run python .github/scripts/run_ci_stage.py cpp-benchmarks

      - name: Run Nim tests
        id: mettagrid-nim-tests
        run: uv run python .github/scripts/run_ci_stage.py nim-tests

  tests-summary:
    name: "Tests"
    needs: [python-tests, cpp-tests, recipe-tests]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Check unit tests did not fail
        uses: actions/github-script@v7
        with:
          script: |
            const needs = ${{ toJSON(needs) }};
            const results = Object.values(needs).map(j => j.result || '').join(',');
            core.info(`Test job results: ${results}`);

            if (results.includes('failure')) {
              core.setFailed('One or more test jobs failed');     // red
              return;
            }
            if (results.includes('cancelled')) {
              core.notice('One or more test jobs were cancelled');
            } else {
              core.info('All test jobs completed successfully');
            }
