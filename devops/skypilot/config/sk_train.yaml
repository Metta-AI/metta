resources:
  cloud: aws
  any_of:
    - region: us-east-1
      use_spot: true
      accelerators: "L4:1"
    - region: us-east-1
      use_spot: true
      accelerators: "A10G:1"
    - region: us-east-1
      accelerators: "L4:1"
    - region: us-east-1
      accelerators: "A10G:1"
    - region: us-west-2
      use_spot: true
      accelerators: "L4:1"
    - region: us-west-2
      use_spot: true
      accelerators: "A10G:1"
    - region: us-west-2
      accelerators: "L4:1"
    - region: us-west-2
      accelerators: "A10G:1"
  cpus: 8+
  image_id: docker:metta:latest
  job_recovery:
    strategy: EAGER_NEXT_REGION
    max_restarts_on_errors: 20

file_mounts:
  /mnt/s3/softmax-public:
    source: s3://softmax-public
    mode: MOUNT_CACHED
  /mnt/s3/train_dir:
    source: s3://softmax-train-dir
    mode: MOUNT_CACHED

setup: |
  set -e
  cd /workspace/metta

  echo "[SETUP] Fetching latest from origin..."
  git fetch origin "$METTA_GIT_REF" || git fetch --depth=1000 origin

  echo "[SETUP] Checking out ref: $METTA_GIT_REF"
  git checkout $METTA_GIT_REF

  uv sync
  mkdir -p $WANDB_DIR

  echo "[SETUP] Creating job secrets..."
  ./devops/skypilot/create_job_secrets.py \
    --wandb-password "$WANDB_PASSWORD" \
    --observatory-token "$OBSERVATORY_TOKEN"

  echo "[SETUP] Setup complete."

run: |
  set -euo pipefail

  cd /workspace/metta

  # Drop any preloaded venv; activate your own
  if [ -n "${VIRTUAL_ENV:-}" ]; then
    deactivate 2>/dev/null || true
  fi
  . .venv/bin/activate

  echo "[CONFIG] Run Configuration:"
  echo "  - METTA_RUN_ID: ${METTA_RUN_ID:-}"
  echo "  - SKYPILOT_TASK_ID: ${SKYPILOT_TASK_ID:-}"
  echo "  - HEARTBEAT_TIMEOUT: ${HEARTBEAT_TIMEOUT:-'NOT SET'}"
  echo "  - MAX_RUNTIME_HOURS: ${MAX_RUNTIME_HOURS:-'NOT SET'}"
  echo "  - METTA_CMD: ${METTA_CMD:-'NOT SET'}"
  echo "  - METTA_CMD_ARGS: ${METTA_CMD_ARGS:-'NOT SET'}"

  if [ -f common/src/metta/common/util/skypilot_latency.py ]; then
    echo "[RUN] Collecting skypilot latency..."
    uv run python common/src/metta/common/util/skypilot_latency.py || true
  else
    echo "[RUN] Latency script is missing!"
  fi

  export NUM_GPUS="${SKYPILOT_NUM_GPUS_PER_NODE:-1}"
  export NUM_NODES="${SKYPILOT_NUM_NODES:-1}"
  export MASTER_ADDR="$(echo "${SKYPILOT_NODE_IPS:-}" | head -n1)"
  export MASTER_PORT=8008
  export NODE_INDEX="${SKYPILOT_NODE_RANK:-0}"
  export NCCL_SHM_DISABLE=1

  METTA_ENV_FILE="$(uv run ./common/src/metta/common/util/constants.py METTA_ENV_FILE)"

  if [ -f common/src/metta/common/util/cost_monitor.py ]; then
    echo "[RUN] Collecting instance cost..."
    METTA_HOURLY_COST="$(uv run python common/src/metta/common/util/cost_monitor.py 2>/dev/null | tail -1 || true)"
    if [ -n "${METTA_HOURLY_COST:-}" ]; then
      echo "[RUN] METTA_HOURLY_COST set to: $METTA_HOURLY_COST in $METTA_ENV_FILE"
    else
      echo "[RUN] Cost monitor script failed to run or returned no value."
    fi
  else
    echo "[RUN] Cost monitor script is missing!"
  fi

  if [ -f "$METTA_ENV_FILE" ]; then
    source "$METTA_ENV_FILE"
  else
    echo "Warning: $METTA_ENV_FILE does not exist. Creating empty file."
    touch "$METTA_ENV_FILE"
  fi

  if [ -n "${GITHUB_PAT:-}" ] && [ -n "${GITHUB_REPOSITORY:-}" ] && [ -n "${METTA_GIT_REF:-}" ]; then
    ENABLE_GITHUB_STATUS=true
    echo "[RUN] GitHub status reporting is enabled"
    echo "[RUN] GITHUB_PAT: ${GITHUB_PAT:+<set>}"  # Don't print the actual token
    echo "[RUN] GITHUB_REPOSITORY: ${GITHUB_REPOSITORY:-<not set>}"
    echo "[RUN] METTA_GIT_REF: ${METTA_GIT_REF:-<not set>}"
    echo "[RUN] METTA_RUN_ID: ${METTA_RUN_ID:-<not set>}"

    GITHUB_STATUS_STATE=pending GITHUB_STATUS_DESCRIPTION="Queued on SkyPilotâ€¦" \
      uv run devops/skypilot/set_github_status.py || echo "[WARN] Pending status post failed; continuing"
  else
    ENABLE_GITHUB_STATUS=false
    echo "[RUN] GitHub status reporting is disabled (missing required credentials)"
  fi

  export HEARTBEAT_FILE=${HEARTBEAT_FILE:-$WANDB_DIR/heartbeat.txt}
  export TERMINATION_REASON=""  # Track how the job ended

  # Configurable intervals
  export TIMEOUT_CHECK_INTERVAL=${TIMEOUT_CHECK_INTERVAL:-30}
  export HEARTBEAT_CHECK_INTERVAL=${HEARTBEAT_CHECK_INTERVAL:-10}

  # Create a temp directory for IPC files
  export IPC_DIR="/tmp/metta_job_$$"
  mkdir -p "$IPC_DIR"
  export TERMINATION_REASON_FILE="$IPC_DIR/termination_reason"

  terminate_monitors() {
    if [[ -n "${HEARTBEAT_MONITOR_PID:-}" ]]; then
      kill "$HEARTBEAT_MONITOR_PID" 2>/dev/null || true
      wait "$HEARTBEAT_MONITOR_PID" 2>/dev/null || true
      echo "[INFO] Terminating heartbeat monitor"
    fi

    if [[ -n "${TIMEOUT_MONITOR_PID:-}" ]]; then
      kill "$TIMEOUT_MONITOR_PID" 2>/dev/null || true
      wait "$TIMEOUT_MONITOR_PID" 2>/dev/null || true
      echo "[INFO] Terminating timeout monitor"
    fi
  }

  terminate_process() {
    local pid=$1
    local reason=$2

    # Get the current PGID of the process (in case it changed)
    local current_pgid=$(ps -o pgid= -p "$pid" 2>/dev/null | tr -d ' ')

    if [[ -n "$current_pgid" ]] && kill -0 "$pid" 2>/dev/null; then
      echo "[INFO] Terminating process group $current_pgid (PID: $pid, reason: $reason)"
      echo "$reason" > "$TERMINATION_REASON_FILE"
      terminate_monitors

      # Kill the entire process group
      kill -TERM -$current_pgid 2>/dev/null || true

      # Give process time to clean up
      local count=0
      while kill -0 "$pid" 2>/dev/null && [ $count -lt 5 ]; do
        sleep 1
        count=$((count + 1))
      done

      # Force kill if still running
      if kill -0 "$pid" 2>/dev/null; then
        echo "[WARN] Process group $current_pgid didn't terminate gracefully, forcing kill"
        kill -KILL -$current_pgid 2>/dev/null || true
      fi
    fi
  }

  run_cmd() {
    echo "[INFO] Starting process $METTA_CMD"

    local START_TIME=$(date +%s)

    # Debug: Check if setsid is available
    if command -v setsid >/dev/null 2>&1; then
        echo "[DEBUG] setsid is available"
    else
        echo "[DEBUG] setsid is NOT available - falling back to standard process group"
    fi

    # CRITICAL: Start cmd process in a separate process group so that when we call
    # terminate_process() with kill -TERM -$PGID, it terminates only the training
    # processes and NOT this SkyPilot run script. This allows us to see the final
    # summary and exit cleanly.
    setsid bash -c 'exec "$@"' -- ./devops/"${METTA_CMD:?missing METTA_CMD}".sh run="${METTA_RUN_ID:?missing METTA_RUN_ID}" ${METTA_CMD_ARGS:-} 2>&1 &
    local CMD_PID=$!

    # Make the process a group leader if it isn't already
    local PGID=$(ps -o pgid= -p "$CMD_PID" 2>/dev/null | tr -d ' ')

    echo "[INFO] Started $METTA_CMD process with PID: $CMD_PID, PGID: $PGID"

    # Start timeout monitor if MAX_RUNTIME_HOURS is set
    if [[ -n "${MAX_RUNTIME_HOURS:-}" ]] && [[ "${MAX_RUNTIME_HOURS}" != "None" ]]; then
      (
        exec 2>&1
        max_seconds=$(awk "BEGIN {print ${MAX_RUNTIME_HOURS} * 3600}")
        echo "[INFO] Timeout monitor started - max runtime: ${MAX_RUNTIME_HOURS} hours (${max_seconds} seconds)"
        echo "[INFO] Checking every ${TIMEOUT_CHECK_INTERVAL} seconds"

        while true; do
          sleep "$TIMEOUT_CHECK_INTERVAL"

          # Check if main process is still alive
          if ! kill -0 "$CMD_PID" 2>/dev/null; then
            echo "[INFO] Timeout monitor: main process no longer running, exiting"
            break
          fi

          elapsed=$(($(date +%s) - START_TIME))
          remaining=$((max_seconds - elapsed))

          if [ $remaining -gt 0 ]; then
            elapsed_min=$((elapsed / 60))
            remaining_min=$((remaining / 60))
            echo "[INFO] Timeout Status: ${elapsed_min} minutes elapsed, ${remaining_min} minutes remaining (max: ${MAX_RUNTIME_HOURS}h)"
          else
            echo "[INFO] Timeout limit reached - terminating process group"
            # Use shared terminate function
            terminate_process "$CMD_PID" "max_runtime_reached"
            echo "[INFO] Timeout monitor exiting - process terminated"
            break
          fi
        done
      ) &
      TIMEOUT_MONITOR_PID=$!
      echo "[INFO] Started timeout monitor with PID: $TIMEOUT_MONITOR_PID"
    fi

    # Start heartbeat monitor if enabled
    if [[ "${HEARTBEAT_TIMEOUT}" != "0" ]]; then
      echo "[INFO] Starting heartbeat monitor ${HEARTBEAT_TIMEOUT}s on $HEARTBEAT_FILE"
      echo "[INFO] Checking every ${HEARTBEAT_CHECK_INTERVAL} seconds"

      (
        LAST_HEARTBEAT_TIME=$(date +%s)
        HEARTBEAT_COUNT=0

        while kill -0 "$CMD_PID" 2>/dev/null; do
          if [ -f "$HEARTBEAT_FILE" ]; then
            CURRENT_MTIME=$(stat -c %Y "$HEARTBEAT_FILE" 2>/dev/null || stat -f %m "$HEARTBEAT_FILE" 2>/dev/null || echo 0)
            CURRENT_TIME=$(date +%s)

            if [ "$CURRENT_MTIME" -gt "$LAST_HEARTBEAT_TIME" ]; then
              HEARTBEAT_COUNT=$((HEARTBEAT_COUNT + 1))
              LAST_HEARTBEAT_TIME=$CURRENT_MTIME

              # Print status occasionally
              if [ $((HEARTBEAT_COUNT % 10)) -eq 0 ]; then
                echo "[INFO] Heartbeat received! (Total: $HEARTBEAT_COUNT heartbeat checks)"
              fi
            fi

            # Check if timeout exceeded
            if [ $((CURRENT_TIME - LAST_HEARTBEAT_TIME)) -gt "$HEARTBEAT_TIMEOUT" ]; then
              echo "[ERROR] Heartbeat timeout! No heartbeat for $HEARTBEAT_TIMEOUT seconds"
              # Use shared terminate function
              terminate_process "$CMD_PID" "heartbeat_timeout"
              break
            fi
          fi

          sleep "$HEARTBEAT_CHECK_INTERVAL"
        done

        echo "[INFO] Heartbeat monitor exiting"
      ) &
      HEARTBEAT_MONITOR_PID=$!
      echo "[INFO] Started heartbeat monitor with PID: $HEARTBEAT_MONITOR_PID"
    fi

    # Wait for command to finish
    wait "$CMD_PID"
    local CMD_EXIT=$?

    terminate_monitors

    # Calculate total runtime
    local END_TIME=$(date +%s)
    local DURATION=$((END_TIME - START_TIME))
    echo "[SUMMARY] Total runtime: $DURATION seconds ($((DURATION / 60)) minutes)"

    return $CMD_EXIT
  }

  cleanup() {
      # Only run cleanup once
      if [ "${CLEANUP_DONE:-}" = "true" ]; then
          return
      fi
      export CLEANUP_DONE=true

      # Read termination reason from file if it exists
      if [ -f "$TERMINATION_REASON_FILE" ]; then
          TERMINATION_REASON=$(cat "$TERMINATION_REASON_FILE")
          echo "[INFO] Termination reason from monitor: $TERMINATION_REASON"
      fi

      # Check termination reason
      if [[ "${TERMINATION_REASON}" == "heartbeat_timeout" ]]; then
          echo "[ERROR] Job terminated due to heartbeat timeout"
          export GITHUB_STATUS_DESCRIPTION="Job failed - no heartbeat for ${HEARTBEAT_TIMEOUT} seconds"
          # Keep failure exit code for status reporting script -- but override when exiting to prevent skypilot retry
      elif [[ "${TERMINATION_REASON}" == "max_runtime_reached" ]]; then
          echo "[INFO] Job terminated due to max runtime limit"
          export GITHUB_STATUS_DESCRIPTION="Job ran successfully for ${MAX_RUNTIME_HOURS:-unknown} hours"
          # Map to success
          CMD_EXIT=0
      elif [[ $CMD_EXIT -eq 0 ]]; then
          echo "[SUCCESS] Job completed successfully"
          export GITHUB_STATUS_DESCRIPTION="Job completed successfully"
          export TERMINATION_REASON="completed"
      else
          echo "[ERROR] Job failed with exit code $CMD_EXIT"
          export GITHUB_STATUS_DESCRIPTION="Job failed with exit code $CMD_EXIT"
          export TERMINATION_REASON="exit_code_${CMD_EXIT}"
      fi

      # Final summary
      echo "[SUMMARY] ===== Job Summary ====="
      echo "[SUMMARY] Job ID: ${METTA_RUN_ID}"
      echo "[SUMMARY] Exit code: ${CMD_EXIT}"
      echo "[SUMMARY] Termination reason: ${TERMINATION_REASON:-unknown}"
      echo "[SUMMARY] ======================"

      if [ "$ENABLE_GITHUB_STATUS" = "true" ]; then
          # The set_github_status.py script uses CMD_EXIT to determine status
          export CMD_EXIT
          uv run devops/skypilot/set_github_status.py || echo "[WARN] Final status post failed"
      fi

      echo "[RUN] Job complete with exit code: $CMD_EXIT (reason: ${TERMINATION_REASON:-unknown})"

      # Set the final exit code for the script (don't exit here!)
      if [[ "${TERMINATION_REASON}" == "max_runtime_reached" ]] ||
        [[ "${TERMINATION_REASON}" == "completed" ]] ||
        [[ "${TERMINATION_REASON}" == "heartbeat_timeout" ]]; then
          echo "[INFO] Will exit with code 0 to prevent SkyPilot restart"
          FINAL_EXIT_CODE=0
      else
          echo "[INFO] Will exit with actual exit code: $CMD_EXIT"
          FINAL_EXIT_CODE=$CMD_EXIT
      fi
  }

  # Export variables needed by cleanup
  export TIMEOUT_MONITOR_PID=""
  export HEARTBEAT_MONITOR_PID=""
  export CMD_EXIT=1  # Default exit code
  export FINAL_EXIT_CODE=""
  export -f terminate_process
  export -f terminate_monitors

  # Trap SIGTERM in parent to prevent immediate exit
  trap 'echo "[INFO] Parent script received SIGTERM, ignoring to complete cleanup"' TERM

  # Set up cleanup trap
  trap cleanup EXIT

  # Run the command
  run_cmd
  CMD_EXIT=$?

  # Exit with the appropriate code (cleanup will run automatically)
  exit ${FINAL_EXIT_CODE:-$CMD_EXIT}

envs:
  GITHUB_REPOSITORY: Metta-AI/metta
  GITHUB_PAT: ""

  METTA_RUN_ID: ""
  METTA_CMD: train
  METTA_CMD_ARGS: ""
  METTA_GIT_REF: main
  WANDB_DIR: ./wandb
  HEARTBEAT_TIMEOUT: 600

  # s3 mount slows down uv, so we put DATA_DIR outside of /workspace/metta
  DATA_DIR: /mnt/s3/train_dir
  SKYPILOT_DOCKER_USERNAME: ""
  SKYPILOT_DOCKER_PASSWORD: ""
  SKYPILOT_DOCKER_SERVER: 751442549699.dkr.ecr.us-east-1.amazonaws.com

secrets:
  # configured by launch script based on local credentials
  WANDB_PASSWORD: ""
  OBSERVATORY_TOKEN: ""
