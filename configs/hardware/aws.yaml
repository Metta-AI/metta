# @package __global__

  device: cuda
  vectorization: multiprocessing
  train:
    total_timesteps: 50_000_000_000
    num_workers: 4 # num cores
    batch_size: 262144 # smaller seems to break
    minibatch_size: 4096
    forward_pass_minibatch_target_size: 4096
    async_factor: 2
    zero_copy: True
