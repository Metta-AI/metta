# @package __global__

device: cuda
vectorization: multiprocessing
trainer:
  total_timesteps: 50_000_000_000
  # batch_size: 262144 # smaller seems to break
  # minibatch_size: 4096
  # forward_pass_minibatch_target_size: 4096
  # async_factor: 2
  # zero_copy: True
