# Configuration for training with 1024-agent cyclical environments
# This config uses 16 workers for a good balance of parallelization and memory usage

defaults:
  - override /trainer: trainer

trainer:
  # 16 workers configuration
  # With 16,384 agents (16 workers × 1024 agents) and bptt_horizon=64
  num_workers: 16
  batch_size: 1_048_576  # Minimum required: 16 × 1024 × 64 = 1,048,576
  
  # You can also adjust num_envs per worker (default is usually 1)
  # num_envs: 1
  
  # Learning rate - slightly adjusted for the batch size
  learning_rate: 4e-5
  
  # Ensure we have enough steps
  total_timesteps: 10_000_000
  
  # Use the 1024-agent curriculum
  curriculum: /env/mettagrid/curriculum/cyclical_converter_timing_study_1024