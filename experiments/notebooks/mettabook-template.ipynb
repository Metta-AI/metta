{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mettabook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: confirm you're set up to connect to the services used in this notebook\n",
    "#    If the command does not run, run `./install.sh` from your terminal\n",
    "\n",
    "# !metta status --components=core,system,aws,wandb --non-interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from experiments.notebooks.utils.metrics import fetch_metrics\n",
    "from experiments.notebooks.utils.monitoring import monitor_training_statuses\n",
    "from experiments.notebooks.utils.replays import show_replay\n",
    "from experiments.notebooks.utils.training import launch_training\n",
    "from datetime import datetime\n",
    "from metta.common.wandb.wandb_runs import find_training_runs\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"default\")\n",
    "\n",
    "print(\"Setup complete! Auto-reload enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Launch training\n",
    "\n",
    "# run_name = f\"{os.environ.get('USER')}.training-run.{datetime.now().strftime('%Y-%m-%d_%H-%M')}\"\n",
    "# print(f\"Launching training with run name: {run_name}...\")\n",
    "\n",
    "# # # View `launch_training` function for all options\n",
    "# result = launch_training(\n",
    "#     run_name=run_name,\n",
    "#     curriculum=\"env/mettagrid/arena/basic\",\n",
    "#     wandb_tags=[f\"{os.environ.get('USER')}-arena-experiment\"],\n",
    "#     additional_args=[\"--skip-git-check\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Training Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor Training\n",
    "run_names = [\"daveey.navigation.low_reward.baseline.2\", \"daveey.navigation.low_reward.baseline.07-18\"]\n",
    "\n",
    "# Optional: instead, find all runs that meet some criteria\n",
    "# run_names = find_training_runs(\n",
    "#     # wandb_tags=[\"low_reward\"],\n",
    "#     # state=\"finished\",\n",
    "#     author=os.getenv(\"USER\"),\n",
    "#     limit=5,\n",
    "# )\n",
    "\n",
    "df = monitor_training_statuses(run_names, show_metrics=[\"_step\", \"overview/reward\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dfs = fetch_metrics(run_names, samples=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overview metrics for all fetched runs\n",
    "if not metrics_dfs:\n",
    "    print(\"No metrics data available. Please fetch metrics first.\")\n",
    "else:\n",
    "    print(f\"Plotting metrics for {len(metrics_dfs)} runs\")\n",
    "\n",
    "    # Find common metrics across all runs\n",
    "    all_columns = set()\n",
    "    for _, df in metrics_dfs.items():\n",
    "        all_columns.update(df.columns)\n",
    "\n",
    "    columns = [\"overview/reward\", \"losses/explained_variance\"]\n",
    "    plot_cols = []\n",
    "\n",
    "    for col in all_columns:\n",
    "        if col not in columns:\n",
    "            continue\n",
    "        # Check if this column exists in at least one run with numeric data\n",
    "        has_numeric_data = False\n",
    "        for df in metrics_dfs.values():\n",
    "            if col in df.columns and pd.api.types.is_numeric_dtype(df[col]) and df[col].nunique() > 1:\n",
    "                has_numeric_data = True\n",
    "                break\n",
    "        if has_numeric_data:\n",
    "            plot_cols.append(col)\n",
    "\n",
    "    if not plot_cols:\n",
    "        print(\"No plottable metrics found\")\n",
    "    else:\n",
    "        # Calculate grid dimensions\n",
    "        n_metrics = len(plot_cols)\n",
    "        n_cols = min(3, n_metrics)  # Max 3 columns\n",
    "        n_rows = (n_metrics + n_cols - 1) // n_cols\n",
    "\n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=n_rows,\n",
    "            cols=n_cols,\n",
    "            subplot_titles=[col.replace(\"overview/\", \"\").replace(\"_\", \" \") for col in plot_cols],\n",
    "            vertical_spacing=0.08,\n",
    "            horizontal_spacing=0.1,\n",
    "        )\n",
    "\n",
    "        # Color palette for different runs\n",
    "        colors = [\"blue\", \"red\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n",
    "\n",
    "        # Add traces for each metric and each run\n",
    "        for idx, col in enumerate(plot_cols):\n",
    "            row = (idx // n_cols) + 1\n",
    "            col_idx = (idx % n_cols) + 1\n",
    "\n",
    "            # Plot each run for this metric\n",
    "            for run_idx, (run_name, df) in enumerate(metrics_dfs.items()):\n",
    "                if col in df.columns and \"_step\" in df.columns:\n",
    "                    color = colors[run_idx % len(colors)]\n",
    "\n",
    "                    # Only show legend on first subplot to avoid clutter\n",
    "                    show_legend = idx == 0\n",
    "\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=df[\"_step\"],\n",
    "                            y=df[col],\n",
    "                            mode=\"lines\",\n",
    "                            name=run_name,\n",
    "                            line=dict(color=color, width=2),\n",
    "                            showlegend=show_legend,\n",
    "                            legendgroup=run_name,  # Group all traces from same run\n",
    "                        ),\n",
    "                        row=row,\n",
    "                        col=col_idx,\n",
    "                    )\n",
    "\n",
    "        # Update layout\n",
    "        runs_text = \"run\" if len(metrics_dfs) == 1 else \"runs\"\n",
    "        fig.update_layout(\n",
    "            height=250 * n_rows,\n",
    "            showlegend=True,\n",
    "            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "        )\n",
    "\n",
    "        # Update x-axes labels for bottom row\n",
    "        for col_idx in range(1, min(n_cols, n_metrics) + 1):\n",
    "            fig.update_xaxes(title_text=\"Steps\", row=n_rows, col=col_idx)\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Replays\n",
    "\n",
    "Display replay viewer for a specific run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available replays\n",
    "# replays = get_available_replays(\"daveey.lp.16x4.bptt8\")\n",
    "\n",
    "# Show the last replay for a run\n",
    "show_replay(\"daveey.lp.16x4.bptt8\", step=\"last\", width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
