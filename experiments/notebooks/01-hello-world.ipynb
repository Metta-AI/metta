{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c9c389",
      "metadata": {
        "lines_to_next_cell": 0
      },
      "outputs": [],
      "source": [
        "# ruff: noqa\n",
        "# fmt: off"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18f2f025",
      "metadata": {},
      "source": [
        "# Hello World: your first metta-ai experiment\n",
        "\n",
        "Welcome to your first reinforcement learning experiment in the metta-ai project. This notebook will guide you through creating, observing, evaluating, and training AI agents in a simple gridworld environment.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "By the end of this notebook, you'll be able to:\n",
        "- Create and understand ASCII maps\n",
        "- Choose and observe different agent policies\n",
        "- Evaluate agent performance quantitatively\n",
        "- Train a new agent from scratch\n",
        "- Compare the performance of two agents\n",
        "\n",
        "## 1. Setup\n",
        "\n",
        "Let's load dependencies and set up some scaffolding. Don't worry about the details here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9206378",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.insert(0, str(Path(__file__).resolve().parents[2]))\n",
        "# Setup imports for core notebook workflow\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import time\n",
        "import warnings\n",
        "import io, contextlib\n",
        "import os, json, subprocess, tempfile, yaml\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np  # used later\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from omegaconf import OmegaConf\n",
        "from typing import Any, Dict  # type: ignore\n",
        "from metta.common.util.fs import get_repo_root\n",
        "from tools.renderer import setup_environment, get_policy\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Suppress Pydantic deprecation warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"pydantic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2825a0c5",
      "metadata": {
        "lines_to_next_cell": 0
      },
      "source": [
        "## 2. Defining an Environment\n",
        "\n",
        "In Metta AI, an **environment** is the virtual world where our agents act and learn. It has 2 main elements:\n",
        "\n",
        "  1. A Map -- the physical layout of the environment where agents can move and what objects they encounter. One simple way to define a map is to use a simple ASCII string. This much ASCII will get us started:\n",
        "      - `#` = walls that block movement\n",
        "      - `@` = where the agent starts\n",
        "      - `.` = empty spaces where agents can walk\n",
        "      - `m` = a mine that generates collectible ore\n",
        "  2. Game Rules -- what actions are available and how rewards are calculated. We express these rules through environment configuration.\n",
        "\n",
        "For now, we'll mostly rely on the default set of game rules, as follows:\n",
        "\n",
        "**Agents Can Observe:**\n",
        "- **Vision**: Agents see an 11x11 grid around themselves\n",
        "- **Awareness**: Agents know what resources they're carrying\n",
        "- **Feedback**: Agents receive information about their last action's success\n",
        "\n",
        "**Agents Can Act:**\n",
        "- Navigate in 8 directions (cardinal + diagonal) and rotate\n",
        "- Pick up & carry resources from objects like mines, or drop items\n",
        "- Interact with other agents -- we'll ignore this for now and just start with a single agent\n",
        "\n",
        "**Agents Encounter Objects:**\n",
        "- **Walls**: Block movement and create boundaries\n",
        "- **Mines**: Automatically generate ore over time\n",
        "- **Ore**: Collectible resources that agents can carry and trade for rewards\n",
        "- **Rewards**: Collecting ore gives small positive rewards that can be used to reinforce behavior\n",
        "\n",
        "In our environment, we'll also need an agent (sometimes referred to as a \"policy\") who can take action.\n",
        "We'll try out a very simple \"opportunistic\" agent that moves randomly around the environment. If it encounters\n",
        "a resource, it will usually (but not always) pick it up.\n",
        "\n",
        "In the following cell we'll lay out the map in ASCII, configure the environment to use it, and choose the opportunistic agent. We'll also set:\n",
        "- How many steps to run the simulation for\n",
        "- How long to sleep between steps\n",
        "- Some other basic parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2518cc7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define ASCII map and environment configuration\n",
        "hallway_map = \"\"\"###########\n",
        "#@.......m#\n",
        "###########\"\"\"\n",
        "\n",
        "env_cfg = get_cfg(\"benchmark\")  # type: ignore\n",
        "# Convert to plain dict so we can edit\n",
        "env_dict: Dict[str, Any] = OmegaConf.to_container(env_cfg, resolve=True)  # type: ignore\n",
        "# Override for a single 11x3 hallway map\n",
        "env_dict[\"game\"][\"num_agents\"] = 1  # type: ignore\n",
        "env_dict[\"game\"][\"obs_width\"] = 11  # type: ignore\n",
        "env_dict[\"game\"][\"obs_height\"] = 11  # type: ignore\n",
        "env_dict[\"game\"][\"map_builder\"] = {\n",
        "    \"_target_\": \"mettagrid.mapgen.mapgen.MapGen\",\n",
        "    \"border_width\": 0,\n",
        "    \"root\": {\n",
        "        \"type\": \"mettagrid.mapgen.scenes.inline_ascii.InlineAscii\",\n",
        "        \"params\": {\"data\": hallway_map},\n",
        "    },\n",
        "}\n",
        "env_dict[\"game\"][\"objects\"][\"mine_red\"][\"initial_resource_count\"] = 1\n",
        "env_dict[\"game\"][\"objects\"][\"mine_red\"][\"conversion_ticks\"] = 4\n",
        "env_dict[\"game\"][\"objects\"][\"mine_red\"][\"cooldown\"] = 0\n",
        "env_dict[\"game\"][\"objects\"][\"mine_red\"][\"max_output\"] = 2  # type: ignore\n",
        "env_dict[\"game\"][\"objects\"][\"mine_red\"][\"max_conversions\"] = -1  # type: ignore\n",
        "env_dict[\"game\"][\"objects\"][\"generator_red\"][\"max_conversions\"] = -1  # type: ignore\n",
        "env_dict[\"game\"][\"agent\"][\"rewards\"][\"inventory\"][\"ore_red\"] = 1.0\n",
        "\n",
        "cfg = OmegaConf.create({\n",
        "    \"env\": env_dict,\n",
        "    \"renderer_job\": {\n",
        "        \"policy_type\": \"opportunistic\",\n",
        "        \"num_steps\": 100,\n",
        "        \"num_agents\": 1,\n",
        "        \"sleep_time\": 0.04,\n",
        "    },\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "989afcd4",
      "metadata": {
        "lines_to_next_cell": 0
      },
      "source": [
        "## 3. Observing a Simulation\n",
        "\n",
        " Now we'll actually run the simulation, using a \"game loop\" approach, where we:\n",
        "- Find out what action the agent wants to take\n",
        "- Step the environment forward one tick, taking the action into account\n",
        "- Render the environment to the screen (as an ASCII string)\n",
        "- Sleep for a bit\n",
        "\n",
        "We'll also track the agent's inventory and display the score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "225b06c3",
      "metadata": {
        "lines_to_next_cell": 0
      },
      "outputs": [],
      "source": [
        "with contextlib.redirect_stdout(io.StringIO()):\n",
        "    env, render_mode = setup_environment(cfg)\n",
        "    policy = get_policy(cfg.renderer_job.policy_type, env, cfg)\n",
        "\n",
        "header = widgets.HTML()\n",
        "map_box = widgets.HTML()\n",
        "display(header, map_box)\n",
        "\n",
        "# Run simulation loop\n",
        "obs, info = env.reset()\n",
        "for step in range(cfg.renderer_job.num_steps):\n",
        "    actions = policy.predict(obs)\n",
        "    obs, rewards, terminals, truncations, info = env.step(actions)\n",
        "    # Track ore in inventory for agent 0\n",
        "    agent_obj = next(o for o in env.grid_objects.values() if o.get(\"agent_id\") == 0)\n",
        "    inv = {env.resource_names[idx]: count for idx, count in agent_obj.get(\"inventory\", {}).items()}\n",
        "    header.value = f\"<b>Step:</b> {step+1}/{cfg.renderer_job.num_steps} <br/> <b>Inventory:</b> {inv}\"\n",
        "    with contextlib.redirect_stdout(io.StringIO()):\n",
        "        buffer_str = env.render()\n",
        "    map_box.value = f\"<pre>{buffer_str}</pre>\"\n",
        "    if cfg.renderer_job.sleep_time:\n",
        "        time.sleep(cfg.renderer_job.sleep_time)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c82a53",
      "metadata": {},
      "source": [
        "### What You Should See:\n",
        "- The agent (`0`) moving back and forth randomly in the hallway\n",
        "- The mine ('m') is continually generating ore (not shown)\n",
        "- When the agent reaches the mine, it should sometimes pick up ore\n",
        "- This will increase the agent's \"score\"\n",
        "\n",
        "\n",
        "## 4. Evaluation – defining “success” for our hallway task\n",
        "\n",
        "So far we've just watched the agent wander. Now we need a **quantitative** way to decide whether any\n",
        "policy is \"good\".\n",
        "\n",
        "### 4.1 Desired behavior\n",
        "• Reach the red mine and harvest as much red ore as possible.\n",
        "• Do it quickly – fewer steps means more ore before the episode ends.\n",
        "\n",
        "### 4.2 Choosing a metric\n",
        "The simplest measurable signal that captures that behavior is **how much `ore_red` the agent is carrying when the episode ends**.\n",
        "\n",
        "We therefore define:\n",
        "\n",
        "    score = total amount of `ore_red` in the agent's inventory\n",
        "\n",
        "Why this is a good choice:\n",
        "1. **Direct** – it counts exactly the thing we care about.\n",
        "2. **Monotonic** – more ore ⇒ higher score.\n",
        "3. **Reward-friendly** – the environment can hand out a small reward each time inventory grows, which is useful later when we train.\n",
        "\n",
        "### 4.3 Hooking the metric into the config\n",
        "Metta-ai's env config already supports inventory-based rewards. We enable it with:\n",
        "\n",
        "```yaml\n",
        "game:\n",
        "  agent:\n",
        "    rewards:\n",
        "      inventory:\n",
        "        ore_red: 1.0        # +1 for every unit of red ore held\n",
        "```\n",
        "\n",
        "During an episode the environment sums that reward, so the **episode return** equals the final ore count. That value is what we'll call *score*.\n",
        "\n",
        "### 4.4 Evaluation procedure\n",
        "1. Run *N* episodes (default 100) with the current policy.\n",
        "2. Record the episode return (our *score*) after each run.\n",
        "3. Report the mean and standard deviation.\n",
        "\n",
        "The same procedure works for any future policy, giving a fair apples-to-apples comparison.\n",
        "\n",
        "When you run the next code cell you'll see a table with:\n",
        "- episode index\n",
        "- score for that episode\n",
        "- running average\n",
        "\n",
        "This establishes a numeric baseline for the opportunistic agent. Later we'll train a policy and expect this number to rise significantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e59bfb5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "EVAL_EPISODES = 30\n",
        "scores: list[int] = []\n",
        "\n",
        "# Re-use the same cfg (contains ore_red reward = 1.0)\n",
        "with contextlib.redirect_stdout(io.StringIO()):\n",
        "    eval_env, _ = setup_environment(cfg)\n",
        "    eval_policy = get_policy(cfg.renderer_job.policy_type, eval_env, cfg)\n",
        "\n",
        "for ep in range(1, EVAL_EPISODES + 1):\n",
        "    obs, _ = eval_env.reset()\n",
        "    # Run fixed number of steps to make scores comparable to the observation cell\n",
        "    inv_count = 0\n",
        "    for step in range(cfg.renderer_job.num_steps):\n",
        "        actions = eval_policy.predict(obs)\n",
        "        obs, _, _, _, _ = eval_env.step(actions)\n",
        "    # After the episode, check inventory\n",
        "    agent_obj = next(o for o in eval_env.grid_objects.values() if o.get(\"agent_id\") == 0)\n",
        "    inv = {eval_env.resource_names[idx]: cnt for idx, cnt in agent_obj.get(\"inventory\", {}).items()}\n",
        "    inv_count = int(inv.get(\"ore_red\", 0))\n",
        "    scores.append(inv_count)\n",
        "    print(f\"Episode {ep:3d}/{EVAL_EPISODES}: ore_red = {inv_count}\")\n",
        "\n",
        "mean_score = np.mean(scores)\n",
        "std_score = np.std(scores)\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(f\"Mean ore_red: {mean_score:.2f} ± {std_score:.2f} (n={EVAL_EPISODES})\")\n",
        "\n",
        "# Display table inline\n",
        "running_avg = pd.Series(scores).expanding().mean()\n",
        "display(\n",
        "    pd.DataFrame({\"episode\": list(range(1, EVAL_EPISODES + 1)), \"ore_red\": scores, \"running_avg\": running_avg})\n",
        ")\n",
        "\n",
        "eval_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25215405",
      "metadata": {},
      "source": [
        "## 5. Training a New Agent\n",
        "\n",
        "We've measured how well the *hand-coded* opportunistic policy performs. Now we'll teach an agent **from scratch** using\n",
        "reinforcement learning (RL) and see if it can beat that baseline.\n",
        "\n",
        "### 5.1  What does \"training\" mean?\n",
        "In RL the agent initially acts at random. After each step the environment returns a *reward*. Over many episodes the\n",
        "learning algorithm (we'll use PPO – *Proximal Policy Optimization*) updates the policy so that actions leading to higher\n",
        "cumulative reward become more likely.\n",
        "\n",
        "In our hallway task the reward signal is already in place: every unit of `ore_red` in inventory is worth **+1**.\n",
        "Maximizing reward therefore means collecting as much ore as possible.\n",
        "\n",
        "### 5.2  Minimal training configuration\n",
        "A full-scale run might take millions of timesteps; for demonstration we'll run a *tiny* job just to prove the pipeline:\n",
        "- same hallway environment (so results stay comparable)\n",
        "- 10 000 environment steps on CPU (≈30 s)\n",
        "- checkpoints & logs saved under `train_dir/`\n",
        "\n",
        "### 5.3  Launching training\n",
        "The repo provides `tools/train.py` – a thin CLI around the trainer. We pass it:\n",
        "1. a unique run name (`run=`)\n",
        "2. an inline curriculum file that simply references our hallway config\n",
        "3. overrides (`trainer.total_timesteps`, etc.) to keep it small.\n",
        "\n",
        "Feel free to increase `trainer.total_timesteps` later for a stronger agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da23e8ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg_tmp_dir = get_repo_root() / \"configs\" / \"tmp\"\n",
        "cfg_tmp_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "curriculum_name = f\"hello_world_curriculum_{datetime.now():%Y%m%d_%H%M%S}.yaml\"\n",
        "temp_curriculum_path = cfg_tmp_dir / curriculum_name\n",
        "\n",
        "with temp_curriculum_path.open(\"w\") as f:\n",
        "    yaml.dump(\n",
        "        {\n",
        "            \"_pre_built_mg_config\": env_dict,\n",
        "            \"game\": env_dict[\"game\"],\n",
        "            \"name\": \"hallway_curriculum\",\n",
        "        },\n",
        "        f,\n",
        "        default_flow_style=False,\n",
        "        indent=2,\n",
        "    )\n",
        "\n",
        "# Unique run name (so multiple notebook runs don't collide)\n",
        "run_name = f\"hello_world_train.{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "# Build command\n",
        "repo_root = get_repo_root()\n",
        "train_cmd = [\n",
        "    str(repo_root / \"tools\" / \"train.py\"),\n",
        "    f\"run={run_name}\",\n",
        "    f\"training_env.curriculum=tmp/{curriculum_name}\",\n",
        "    \"wandb=off\",\n",
        "    \"device=cpu\",\n",
        "    \"trainer.total_timesteps=10000\",  # tiny demo run\n",
        "    \"trainer.batch_size=256\",\n",
        "    \"trainer.minibatch_size=256\",\n",
        "    \"trainer.num_workers=2\",\n",
        "    \"sim=sim\",\n",
        "    \"+train_job.evals.name=hallway\",\"+train_job.evals.num_episodes=1\",\"+train_job.evals.simulations={}\",\n",
        "]\n",
        "\n",
        "process = subprocess.Popen(train_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "for line in process.stdout or []:\n",
        "    print(line, end=\"\")\n",
        "process.wait()\n",
        "\n",
        "temp_curriculum_path.unlink(missing_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47d8eaeb",
      "metadata": {},
      "source": [
        "## 6. Understanding Training Results\n",
        "\n",
        "- **Logs** live in `train_dir/{run_name}/*.log`\n",
        "- **Checkpoints** (PyTorch `.pt` files) are in `train_dir/{run_name}/checkpoints/`\n",
        "  the latest one is the policy we’ll load next.\n",
        "- **Replays** (optional) would be in `train_dir/{run_name}/replays/`\n",
        "\n",
        "You can inspect the logs or open a checkpoint later to see the learned network weights."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d8b0cd0",
      "metadata": {},
      "source": [
        "## 7. Observing the Trained Agent\n",
        "\n",
        "Let’s load the newest checkpoint and watch the trained policy in the same hallway environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f937a009",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Locate latest checkpoint\n",
        "ckpt_dir = Path(\"train_dir\") / run_name / \"checkpoints\"\n",
        "latest_ckpt = max(ckpt_dir.glob(\"*.pt\"), key=lambda p: p.stat().st_mtime)\n",
        "print(\"Loading\", latest_ckpt.name)\n",
        "\n",
        "# Build cfg for trained policy\n",
        "auto_cfg = OmegaConf.create({\n",
        "    \"env\": env_dict,\n",
        "    \"policy_uri\": f\"file://{latest_ckpt.absolute()}\",\n",
        "    \"renderer_job\": {\n",
        "        \"policy_type\": \"trained\",\n",
        "        \"num_steps\": 100,\n",
        "        \"num_agents\": 1,\n",
        "        \"sleep_time\": 0.04,\n",
        "    },\n",
        "})\n",
        "\n",
        "with contextlib.redirect_stdout(io.StringIO()):\n",
        "    trained_env, _ = setup_environment(auto_cfg)\n",
        "    trained_policy = get_policy(\"trained\", trained_env, auto_cfg)\n",
        "\n",
        "header2 = widgets.HTML()\n",
        "map_box2 = widgets.HTML()\n",
        "display(header2, map_box2)\n",
        "\n",
        "obs, _ = trained_env.reset()\n",
        "for step in range(auto_cfg.renderer_job.num_steps):\n",
        "    actions = trained_policy.predict(obs)\n",
        "    obs, _, _, _, _ = trained_env.step(actions)\n",
        "    agent_obj = next(o for o in trained_env.grid_objects.values() if o.get(\"agent_id\") == 0)\n",
        "    inv = {trained_env.resource_names[i]: c for i, c in agent_obj.get(\"inventory\", {}).items()}\n",
        "    header2.value = f\"<b>Step:</b> {step+1}/{auto_cfg.renderer_job.num_steps} <br/> <b>Inventory:</b> {inv}\"\n",
        "    with contextlib.redirect_stdout(io.StringIO()):\n",
        "        buf = trained_env.render()\n",
        "    map_box2.value = f\"<pre>{buf}</pre>\"\n",
        "    if auto_cfg.renderer_job.sleep_time:\n",
        "        time.sleep(auto_cfg.renderer_job.sleep_time)\n",
        "\n",
        "trained_env.close()"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
