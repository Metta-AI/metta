# VAPOR (Variational Policy Optimization) Configuration
# Extends the base trainer config with VAPOR-specific settings

defaults:
  - trainer

# Override PPO section to enable VAPOR
ppo:
  # Enable VAPOR instead of standard PPO
  use_vapor: true

  # VAPOR-specific configuration
  vapor:
    beta: 1.0                    # KL regularization coefficient (temperature)
    beta_schedule: "linear"      # Anneal beta over training: "constant", "linear", "exponential"
    min_beta: 0.1               # Minimum beta value during annealing
    exploration_bonus: 0.1       # Coefficient for uncertainty-based exploration
    use_importance_weighting: true  # Use importance sampling for off-policy correction

  # Core PPO hyperparameters (inherited but can be overridden)
  clip_coef: 0.1              # Not used in VAPOR but kept for compatibility
  ent_coef: 0.001             # Reduced entropy coefficient since VAPOR has built-in exploration
  gae_lambda: 0.916
  gamma: 0.977

  # Training parameters
  max_grad_norm: 0.5
  vf_clip_coef: 0.1
  vf_coef: 0.44               # Value function coefficient
  l2_reg_loss_coef: 0
  l2_init_loss_coef: 0
  norm_adv: true
  clip_vloss: true
  target_kl: null
