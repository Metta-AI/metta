name: "Test and Benchmark"
concurrency:
  group: ${{ github.workflow }}-${{ github.event_name == 'merge_group' && github.event.merge_group.head_ref || github.ref }}
  cancel-in-progress: true
on:
  pull_request:
    types: [opened, synchronize, ready_for_review]
  push:
    branches: [main]
  merge_group:
    types: [checks_requested]
    branches: [main]
  workflow_dispatch:
    inputs:
      run_lint:
        description: "Run lint checks"
        type: boolean
        default: true
      run_test:
        description: "Run tests"
        type: boolean
        default: true
      run_benchmark:
        description: "Run benchmarks (requires tests)"
        type: boolean
        default: true

# Set default permissions
permissions:
  checks: write
  pull-requests: write

env:
  HYDRA_FULL_ERROR: 1
  VENV_PATH: .venv
  PYTEST_WORKERS: auto
  EVAL_SMOKE_TEST_POLICY: m.av.replay.probe.751 # just the name, no need to include any uri preamble like "wandb://run/"
  EVAL_SMOKE_TEST_MAX_ATTEMPTS: 3
  EVAL_SMOKE_TEST_MIN_REWARD: 0.0

jobs:
  # check if CI should run based on Graphite's stack position
  graphite-ci-optimizer:
    name: "Graphite CI Optimizer"
    if: |
      github.event.pull_request.draft == false ||
      github.event_name == 'push' ||
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'merge_group'
    runs-on: ubuntu-latest
    outputs:
      should_skip: ${{ steps.graphite_ci.outputs.skip || 'false' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Detect PR context
        id: detect-pr
        uses: ./.github/actions/detect-external-pr

      - name: Graphite CI Optimizer
        id: graphite_ci
        if: steps.detect-pr.outputs.is_external != 'true'
        uses: withgraphite/graphite-ci-action@main
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          graphite_token: ${{ secrets.GRAPHITE_TOKEN }}

  # check if any source code files have changed
  setup-checks:
    name: "Set up for source code checks"
    needs: graphite-ci-optimizer
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (github.event.pull_request.draft == false ||
       github.event_name == 'push' ||
       github.event_name == 'workflow_dispatch' ||
       github.event_name == 'merge_group')
    runs-on: ubuntu-latest
    outputs:
      has_relevant_changes: ${{ steps.check_py_files.outputs.has_relevant_changes }}
      observatory_has_changes: ${{ steps.check_observatory_files.outputs.has_relevant_changes }}
      run_lint: ${{ steps.determine_tasks.outputs.run_lint }}
      run_test: ${{ steps.determine_tasks.outputs.run_test }}
      run_benchmark: ${{ steps.determine_tasks.outputs.run_benchmark }}
      is_external: ${{ steps.detect-pr.outputs.is_external }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Detect PR context
        id: detect-pr
        uses: ./.github/actions/detect-external-pr

      - name: Check for file changes
        id: check_py_files
        uses: ./.github/actions/file-changes
        with:
          patterns: "**/*.py,**/*.cpp,**/*.hpp,**/conftest.py,**/pyproject.toml,**/pytest.ini,.github/workflows/*.yml"
          specific_files: "uv.lock"
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Check for observatory file changes
        id: check_observatory_files
        uses: ./.github/actions/file-changes
        with:
          patterns: "observatory/**/*.ts,observatory/**/*.tsx,observatory/**/*.js,observatory/**/*.jsx"
          specific_files: "observatory/package.json,observatory/tsconfig.json,observatory/tsconfig.node.json"
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Determine which tasks to run
        id: determine_tasks
        run: |
          # Default behavior based on event type and file changes
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            # Use workflow dispatch inputs for manual runs
            RUN_LINT="${{ github.event.inputs.run_lint }}"
            RUN_TEST="${{ github.event.inputs.run_test }}"
            RUN_BENCHMARK="${{ github.event.inputs.run_benchmark }}"
          elif [[ "${{ github.event_name }}" == "pull_request" && "${{ steps.check_py_files.outputs.has_relevant_changes }}" == "false" ]]; then
            # Skip everything for PRs with no relevant changes
            RUN_LINT="false"
            RUN_TEST="false"
            RUN_BENCHMARK="false"
            echo "::notice title=Skipping Tasks::Skipping all tasks because no relevant files have changed"
          else
            # Default to running everything for other events or when changes exist
            RUN_LINT="true"
            RUN_TEST="true"
            RUN_BENCHMARK="true"
          fi

          # Output the decisions
          echo "run_lint=${RUN_LINT}" >> $GITHUB_OUTPUT
          echo "run_test=${RUN_TEST}" >> $GITHUB_OUTPUT
          echo "run_benchmark=${RUN_BENCHMARK}" >> $GITHUB_OUTPUT

          echo "run lint? ${RUN_LINT}"
          echo "run test? ${RUN_TEST}"
          echo "run benchmark? ${RUN_BENCHMARK}"

  lint:
    name: "Lint"
    needs: [graphite-ci-optimizer, setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_lint == 'true')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup uv
        uses: ./.github/actions/setup-uv
        with:
          install-mode: "linting"

      - name: Run Ruff linter
        run: |
          uv run ruff format --check .

      - name: Run Ruff formatting
        run: |
          uv run ruff check --exit-non-zero-on-fix .

      - name: Install C++ linter
        run: |
          sudo apt-get install -y clang-format

      - name: Run cpplint
        run: ./mettagrid/tests/cpplint.sh

  observatory-typescript-check:
    name: "Observatory TypeScript Check"
    needs: [graphite-ci-optimizer, setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.observatory_has_changes == 'true')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - uses: pnpm/action-setup@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 24
          cache: pnpm
          cache-dependency-path: pnpm-lock.yaml

      - name: Install dependencies
        run: pnpm install --filter @softmax/observatory

      - name: TypeScript compilation check
        run: |
          cd observatory
          pnpm run type-check

      - name: Run linting
        run: |
          cd observatory
          pnpm run lint

  unit-tests:
    name: "Unit Tests - All Packages"
    needs: [graphite-ci-optimizer, setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_test == 'true')
    runs-on: ubuntu-16core
    timeout-minutes: 15
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup uv
        uses: ./.github/actions/setup-uv
        with:
          install-mode: "testing"

      - name: Run all package tests in parallel with live output
        run: |
          # Create directories
          mkdir -p test-results coverage-reports

          # Function to run tests with colored prefix
          run_package_tests() {
            local package=$1
            local color=$2

            echo -e "${color}[${package}]\033[0m ðŸš€ Starting tests..."

            PYTEST_BASE_ARGS="-n ${{ env.PYTEST_WORKERS }} --cov --cov-branch --benchmark-skip --maxfail=1 --disable-warnings --durations=10 -v"

            # Also save raw output for duration parsing
            local raw_output="test-results/${package}_raw.log"

            # Run tests and prefix each line with package name and color
            if [ "$package" == "core" ]; then
              pytest $PYTEST_BASE_ARGS \
                --cov-report=xml:coverage-reports/coverage-${package}.xml \
                2>&1 | tee "$raw_output" | while IFS= read -r line; do
                  echo -e "${color}[${package}]\033[0m $line"
                done
              # Capture PIPESTATUS immediately after the pipeline
              local exit_code=${PIPESTATUS[0]}
              echo $exit_code > test-results/${package}.exit
            else
              (cd "$package" && pytest $PYTEST_BASE_ARGS \
                --cov-report=xml:../coverage-reports/coverage-${package}.xml) \
                2>&1 | tee "$raw_output" | while IFS= read -r line; do
                  echo -e "${color}[${package}]\033[0m $line"
                done
              # Capture PIPESTATUS immediately after the pipeline
              local exit_code=${PIPESTATUS[0]}
              echo $exit_code > test-results/${package}.exit
            fi

            # Extract duration info for later summary
            grep -E "^[0-9]+\.[0-9]+s " "$raw_output" > "test-results/${package}_durations.txt" || true

            # Print completion status
            local exit_code=$(cat test-results/${package}.exit)
            if [ "$exit_code" -eq 0 ]; then
              echo -e "${color}[${package}]\033[0m âœ… Tests passed!"
            else
              echo -e "${color}[${package}]\033[0m âŒ Tests failed (exit code: $exit_code)"
            fi
          }

          # Export function
          export -f run_package_tests

          # Record start time
          START_TIME=$(date +%s)

          # Launch all packages in parallel with different colors
          run_package_tests "agent" "\033[1;31m" &        # Bold Red
          run_package_tests "common" "\033[1;32m" &       # Bold Green
          run_package_tests "app_backend" "\033[1;33m" & # Bold Yellow
          run_package_tests "mettagrid" "\033[1;34m" &   # Bold Blue
          run_package_tests "codebot" "\033[1;35m" &     # Bold Magenta
          run_package_tests "core" "\033[1;36m" &         # Bold Cyan

          # Wait for all to complete
          echo "â³ Running tests on $(nproc) cores..."
          wait

          # Calculate total time
          END_TIME=$(date +%s)
          TOTAL_TIME=$((END_TIME - START_TIME))

          # Summary
          echo -e "\n\033[1;37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\033[0m"
          echo -e "\033[1;37mðŸ“Š TEST SUMMARY\033[0m"
          echo -e "\033[1;37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\033[0m"

          OVERALL_FAILED=0
          FAILED_PACKAGES=""

          for package in agent common app_backend mettagrid codebot core; do
            if [ -f "test-results/${package}.exit" ]; then
              EXIT_CODE=$(cat "test-results/${package}.exit")
              # Count tests run
              TEST_COUNT=$(grep -c "PASSED\|FAILED\|SKIPPED" "test-results/${package}_raw.log" || echo "0")
              if [ "$EXIT_CODE" -eq 0 ]; then
                echo -e "  âœ… \033[1;32m${package}\033[0m - PASSED (${TEST_COUNT} tests)"
              else
                echo -e "  âŒ \033[1;31m${package}\033[0m - FAILED (exit code: $EXIT_CODE, ${TEST_COUNT} tests)"
                OVERALL_FAILED=1
                FAILED_PACKAGES="$FAILED_PACKAGES $package"
              fi
            else
              echo -e "  âš ï¸  \033[1;33m${package}\033[0m - NO RESULTS"
              OVERALL_FAILED=1
            fi
          done

          echo -e "\033[1;37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\033[0m"
          echo -e "\033[1;37mâ±ï¸  Total execution time: ${TOTAL_TIME}s\033[0m"
          echo -e "\033[1;37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\033[0m"

          # Show top 10 slowest tests across all packages
          echo -e "\n\033[1;37mðŸŒ TOP 10 SLOWEST TESTS (across all packages)\033[0m"
          echo -e "\033[1;37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\033[0m"

          # Combine all duration files and sort
          for package in agent common app_backend mettagrid codebot core; do
            if [ -f "test-results/${package}_durations.txt" ]; then
              # Add package name to each line
              sed "s/^/[${package}] /" "test-results/${package}_durations.txt"
            fi
          done | sort -rn | head -10 | nl -w2 -s'. '

          echo -e "\033[1;37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\033[0m"

          if [ $OVERALL_FAILED -ne 0 ]; then
            echo -e "\n\033[1;31mðŸ’¥ Tests failed in:$FAILED_PACKAGES\033[0m"
            exit 1
          else
            echo -e "\n\033[1;32mðŸŽ‰ All tests passed in ${TOTAL_TIME}s!\033[0m"
          fi

      - name: Upload python test coverage to Codecov
        if: needs.setup-checks.outputs.is_external != 'true'
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
        run: |
          chmod +x .github/scripts/upload_codecov.py

          # Upload all coverage files
          for package in agent common app_backend mettagrid codebot core; do
            if [ -f "coverage-reports/coverage-${package}.xml" ]; then
              echo "ðŸ“¤ Uploading coverage for $package..."
              SUBPACKAGES="$package" .github/scripts/upload_codecov.py
            fi
          done

  unit-tests-summary:
    name: "Tests"
    needs: [unit-tests]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Check test results
        run: |
          echo "Test job results: ${{ toJSON(needs) }}"
          if [ "${{ contains(join(needs.*.result, ','), 'failure') }}" == "true" ]; then
            echo "One or more test jobs failed"
            exit 1
          elif [ "${{ contains(join(needs.*.result, ','), 'cancelled') }}" == "true" ]; then
            echo "One or more test jobs were cancelled"
            exit 1
          else
            echo "All test jobs completed successfully"
          fi

  mettagrid-cpp-build:
    name: "MettaGrid C++ Build & Coverage"
    needs: [graphite-ci-optimizer, setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_test == 'true')
    runs-on: ubuntu-4core
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup uv
        uses: ./.github/actions/setup-uv
        with:
          install-mode: "testing"

      - name: Build MettaGrid C++ (with coverage)
        id: mettagrid_build_check
        run: |
          cd mettagrid
          make coverage VERBOSE=1

      - name: Upload MettaGrid C++ coverage to Codecov
        if: needs.setup-checks.outputs.is_external != 'true'
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./mettagrid/build-debug/coverage.info
          flags: mettagrid_cpp
          name: mettagrid-cpp-coverage
          fail_ci_if_error: false
          verbose: true

  python-benchmark:
    name: "Python Benchmarks"
    needs: [graphite-ci-optimizer, setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_benchmark == 'true')
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Setup uv
        uses: ./.github/actions/setup-uv
      - name: Run Python benchmarks
        env:
          PYTHONOPTIMIZE: 1 # Disable __debug__ blocks and asserts for accurate benchmarks
        run: |
          # Main benchmarks
          pytest -n 0 --benchmark-only --benchmark-json=main_benchmark_results.json

          # Mettagrid benchmarks
          cd mettagrid
          pytest -n 0 --benchmark-only --benchmark-json=../mettagrid_benchmark_results.json
      - name: Upload Python benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: python-benchmark-results
          path: |
            main_benchmark_results.json
            mettagrid_benchmark_results.json
          retention-days: 1

  cpp-benchmark:
    name: "C++ Benchmarks"
    needs: [graphite-ci-optimizer, setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_benchmark == 'true')
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup uv
        uses: ./.github/actions/setup-uv

      - name: Build C++ benchmarks
        id: benchmark_build_check
        run: |
          cd mettagrid
          make benchmark VERBOSE=1

      - name: Collect benchmarks
        working-directory: ./mettagrid
        run: |
          mkdir -p benchmark_output
          for f in build-release/*_benchmark; do
            "$f" --benchmark_format=json > benchmark_output/$(basename $f).json
          done

      - name: Upload C++ benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: cpp-benchmark-results
          path: mettagrid/benchmark_output/*.json
          retention-days: 1

  upload-benchmarks:
    name: "Upload Benchmarks"
    needs: [setup-checks, python-benchmark, cpp-benchmark]
    if: |
      (needs.setup-checks.outputs.run_benchmark == 'true') &&
      (always() && (needs.python-benchmark.result == 'success' || needs.cpp-benchmark.result == 'success'))
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download Python benchmark results
        uses: actions/download-artifact@v4
        with:
          name: python-benchmark-results
          path: ./benchmarks/
        continue-on-error: true

      - name: Download C++ benchmark results
        uses: actions/download-artifact@v4
        with:
          name: cpp-benchmark-results
          path: ./benchmarks/
        continue-on-error: true

      - name: Debug downloaded artifacts
        shell: bash
        run: |
          echo "=== Checking downloaded artifacts ==="
          ls -la ./benchmarks/ || echo "No benchmarks directory"

          echo "=== Python benchmark files content ==="
          for file in ./benchmarks/main_benchmark_results.json ./benchmarks/mettagrid_benchmark_results.json; do
            if [ -f "$file" ]; then
              echo "--- $file ---"
              echo "File size: $(wc -c < "$file") bytes"
              head -20 "$file" || echo "Could not read $file"
            else
              echo "$file not found"
            fi
          done

      - name: Combine all benchmark results
        uses: ./.github/actions/combine-all-benchmarks
        with:
          python_files: "./benchmarks/main_benchmark_results.json,./benchmarks/mettagrid_benchmark_results.json"
          cpp_files: "./benchmarks/*.json"
          output_file: "unified_benchmark_results.json"

      - name: Validate generated BMF files
        shell: bash
        run: |
          echo "=== Validating generated BMF files ==="

          if [ -f "unified_benchmark_results.json" ]; then
            if jq empty unified_benchmark_results.json; then
              echo "âœ… unified_benchmark_results.json is valid JSON"
              echo "File size: $(wc -c < unified_benchmark_results.json) bytes"
              echo "Number of benchmarks: $(jq 'length' unified_benchmark_results.json)"
              echo "Sample benchmark names:"
              jq -r 'keys[0:3][]' unified_benchmark_results.json || echo "No benchmarks found"
            else
              echo "âŒ unified_benchmark_results.json is invalid JSON"
              exit 1
            fi
          else
            echo "âŒ unified_benchmark_results.json not found"
            exit 1
          fi

      - name: Check for significant performance changes
        if: needs.setup-checks.outputs.is_external != 'true'
        uses: ./.github/actions/call-bencher-api
        with:
          bencher_token: ${{ secrets.BENCHER_API_TOKEN }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          benchmark_file: "unified_benchmark_results.json"
        continue-on-error: true

      - name: Upload benchmark artifacts for debugging
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: processed-benchmark-results
          path: |
            unified_benchmark_results.json
          retention-days: 7
