name: "Test and Benchmark"
concurrency:
  group: ${{ github.workflow }}-${{ github.event_name == 'merge_group' && github.event.merge_group.head_ref || github.ref }}
  cancel-in-progress: true
on:
  pull_request:
    types: [opened, synchronize, ready_for_review]
  push:
    branches: [main]
  merge_group:
    types: [checks_requested]
    branches: [main]
  workflow_dispatch:
    inputs:
      run_lint:
        description: "Run lint checks"
        type: boolean
        default: true
      run_test:
        description: "Run tests"
        type: boolean
        default: true
      run_benchmark:
        description: "Run benchmarks (requires tests)"
        type: boolean
        default: true

# Set default permissions
permissions:
  checks: write
  pull-requests: write

env:
  HYDRA_FULL_ERROR: 1
  VENV_PATH: .venv
  PYTEST_WORKERS: auto

jobs:
  # check if CI should run based on Graphite's stack position
  graphite-ci-optimizer:
    name: "Graphite CI Optimizer"
    if: |
      github.event.pull_request.draft == false ||
      github.event_name == 'push' ||
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'merge_group'
    runs-on: ubuntu-latest
    outputs:
      should_skip: ${{ steps.graphite_ci.outputs.skip }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Graphite CI Optimizer
        id: graphite_ci
        uses: withgraphite/graphite-ci-action@main
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          graphite_token: ${{ secrets.GRAPHITE_TOKEN }}

  # check if any source code files have changed
  setup-checks:
    name: "Set up for source code checks"
    needs: graphite-ci-optimizer
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (github.event.pull_request.draft == false ||
       github.event_name == 'push' ||
       github.event_name == 'workflow_dispatch' ||
       github.event_name == 'merge_group')
    runs-on: ubuntu-latest
    outputs:
      has_relevant_changes: ${{ steps.check_py_files.outputs.has_relevant_changes }}
      run_lint: ${{ steps.determine_tasks.outputs.run_lint }}
      run_test: ${{ steps.determine_tasks.outputs.run_test }}
      run_benchmark: ${{ steps.determine_tasks.outputs.run_benchmark }}
      is_external: ${{ steps.detect-pr.outputs.is_external }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Detect PR context
        id: detect-pr
        uses: ./.github/actions/detect-external-pr

      - name: Check for file changes
        id: check_py_files
        uses: ./.github/actions/file-changes
        with:
          patterns: "**/*.py,**/*.pyx,**/*.pxd,**/*.cpp,**/*.hpp,**/conftest.py,**/pyproject.toml,**/pytest.ini,.github/workflows/*.yml"
          specific_files: "uv.lock"
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Determine which tasks to run
        id: determine_tasks
        run: |
          # Default behavior based on event type and file changes
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            # Use workflow dispatch inputs for manual runs
            RUN_LINT="${{ github.event.inputs.run_lint }}"
            RUN_TEST="${{ github.event.inputs.run_test }}"
            RUN_BENCHMARK="${{ github.event.inputs.run_benchmark }}"
          elif [[ "${{ github.event_name }}" == "pull_request" && "${{ steps.check_py_files.outputs.has_relevant_changes }}" == "false" ]]; then
            # Skip everything for PRs with no relevant changes
            RUN_LINT="false"
            RUN_TEST="false"
            RUN_BENCHMARK="false"
            echo "::notice title=Skipping Tasks::Skipping all tasks because no relevant files have changed"
          else
            # Default to running everything for other events or when changes exist
            RUN_LINT="true"
            RUN_TEST="true"
            RUN_BENCHMARK="true"
          fi

          # Output the decisions
          echo "run_lint=${RUN_LINT}" >> $GITHUB_OUTPUT
          echo "run_test=${RUN_TEST}" >> $GITHUB_OUTPUT
          echo "run_benchmark=${RUN_BENCHMARK}" >> $GITHUB_OUTPUT

          echo "run lint? ${RUN_LINT}"
          echo "run test? ${RUN_TEST}"
          echo "run benchmark? ${RUN_BENCHMARK}"

  lint:
    name: "Lint"
    needs: [setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_lint == 'true')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup uv
        uses: ./.github/actions/setup-uv
        with:
          install-mode: "linting"

      - name: Run Ruff linter
        run: |
          uv run ruff format --check .

      - name: Run Ruff formatting
        run: |
          uv run ruff check --exit-non-zero-on-fix .

      - name: Install C++ linter
        run: |
          sudo apt-get install -y clang-format

      - name: Run cpplint
        run: ./mettagrid/tests/cpplint.sh

  test-matrix:
    name: "Unit Tests - ${{ matrix.name }}"
    needs: [setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_test == 'true')
    runs-on: ubuntu-latest
    timeout-minutes: 15
    strategy:
      fail-fast: false
      matrix:
        include:
          # Individual packages
          - name: "agent"
            package: "agent"
          - name: "common"
            package: "common"

          # app_backend with sharding
          - name: "app_backend-shard-0"
            package: "app_backend"
            shard_id: 0
            num_shards: 5
          - name: "app_backend-shard-1"
            package: "app_backend"
            shard_id: 1
            num_shards: 5
          - name: "app_backend-shard-2"
            package: "app_backend"
            shard_id: 2
            num_shards: 5
          - name: "app_backend-shard-3"
            package: "app_backend"
            shard_id: 3
            num_shards: 5
          - name: "app_backend-shard-4"
            package: "app_backend"
            shard_id: 4
            num_shards: 5

          # Mettagrid with sharding
          - name: "mettagrid-shard-0"
            package: "mettagrid"
            shard_id: 0
            num_shards: 2
          - name: "mettagrid-shard-1"
            package: "mettagrid"
            shard_id: 1
            num_shards: 2

          # Core with sharding
          - name: "core-shard-0"
            package: "core"
            shard_id: 0
            num_shards: 6
          - name: "core-shard-1"
            package: "core"
            shard_id: 1
            num_shards: 6
          - name: "core-shard-2"
            package: "core"
            shard_id: 2
            num_shards: 6
          - name: "core-shard-3"
            package: "core"
            shard_id: 3
            num_shards: 6
          - name: "core-shard-4"
            package: "core"
            shard_id: 4
            num_shards: 6
          - name: "core-shard-5"
            package: "core"
            shard_id: 5
            num_shards: 6

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup uv
        uses: ./.github/actions/setup-uv
        with:
          install-mode: "testing"

      - name: Run tests
        run: |
          # Base pytest arguments (without coverage file specification)
          PYTEST_BASE_ARGS="-n ${{ env.PYTEST_WORKERS }} --cov --cov-branch --benchmark-skip --maxfail=1 --disable-warnings --durations=10 -v"

          COVERAGE_SUFFIX="-shard${{ matrix.shard_id }}"

          # Handle sharding for larger packages
          if [ -n "${{ matrix.shard_id }}" ] && [ -n "${{ matrix.num_shards }}" ]; then
            echo "Running shard ${{ matrix.shard_id }} of ${{ matrix.num_shards }}"

            COVERAGE_SUFFIX="-shard${{ matrix.shard_id }}"

            # Set environment variables for the sharding plugin
            export PYTEST_SHARD_ID=${{ matrix.shard_id }}
            export PYTEST_NUM_SHARDS=${{ matrix.num_shards }}

            # Use the sharding plugin from common package
            PYTEST_BASE_ARGS="$PYTEST_BASE_ARGS -p no:cacheprovider -p metta.common.tests.pytest_shard"
          fi

          if [ "${{ matrix.package }}" == "core" ]; then
            # For core package, ignore other packages
            SUBPACKAGES=(app_backend agent mettagrid common)
            for pkg in "${SUBPACKAGES[@]}"; do
              PYTEST_BASE_ARGS="$PYTEST_BASE_ARGS --ignore=$pkg"
            done
          else
            # Navigate to package directory
            cd ${{ matrix.package }}
          fi

          # Phase 1: Run fast tests (not marked as slow)
          echo "::group::Phase 1 - Running fast tests"
          echo "Running tests not marked as slow..."

          # Capture pytest output
          PYTEST_OUTPUT=$(pytest $PYTEST_BASE_ARGS -m "not slow" --cov-report=xml:coverage-fast${COVERAGE_SUFFIX}.xml 2>&1)
          FAST_TESTS_FAILED=$?

          # Display the output
          echo "$PYTEST_OUTPUT"
          echo "::endgroup::"

          # Check for slow tests
          echo "::group::Checking for slow tests"
          echo "$PYTEST_OUTPUT" | awk '
          BEGIN {
              found_slow = 0
          }
          /===+ slowest [0-9]+ durations ===+/ {
              print "Found slow tests in Phase 1:"
              slow_section = 1
              next
          }
          slow_section && /^[0-9]+\.[0-9]+s/ {
              # Extract duration
              duration = $1
              gsub("s", "", duration)

              # Extract phase (setup, call, teardown, etc.)
              phase = $2

              # Get the test path (third field onwards)
              test_path = $3
              for (i = 4; i <= NF; i++) {
                  test_path = test_path " " $i
              }

              # Check if duration exceeds threshold
              if (duration > 2.0) {
                  print "::warning::" test_path " (" phase " phase) takes " duration " seconds - should be marked with @pytest.mark.slow"
                  found_slow = 1
              }
          }
          /^===/ && slow_section {
              slow_section = 0
          }
          END {
              if (found_slow == 0) {
                  print "✅ No slow tests found in Phase 1"
              }
          }
          '
          echo "::endgroup::"

          # Exit early if fast tests failed
          if [ "${FAST_TESTS_FAILED:-0}" -ne 0 ]; then
            echo "::error::Fast tests failed! Skipping slow tests."
            exit $FAST_TESTS_FAILED
          fi

          # Phase 2: Run slow tests (only if fast tests passed)
          echo "::group::Phase 2 - Running slow tests"
          echo "Fast tests passed! Now running slow tests..."

          # For slow tests, we need to append coverage data
          pytest $PYTEST_BASE_ARGS -m "slow" --maxfail=5 --cov-append --cov-report=xml:coverage-slow${COVERAGE_SUFFIX}.xml || SLOW_TESTS_EXIT=$?

          # Exit code 5 means no tests were collected (which is fine - not all packages have slow tests)
          if [ "${SLOW_TESTS_EXIT:-0}" -eq 5 ]; then
            echo "No slow tests found in this package/shard - that's OK!"
            SLOW_TESTS_EXIT=0
          elif [ "${SLOW_TESTS_EXIT:-0}" -ne 0 ]; then
            echo "::warning::Slow tests failed with exit code $SLOW_TESTS_EXIT"
          fi
          echo "::endgroup::"

          # Report final status
          if [ "${SLOW_TESTS_EXIT:-0}" -ne 0 ]; then
            echo "::warning::Some slow tests failed, but fast tests passed"
            # You can decide whether to fail the job on slow test failures
            # For now, let's make it non-blocking:
            # exit $SLOW_TESTS_EXIT
          fi

          echo "✅ Test run complete!"

      - name: Upload python test coverage to Codecov
        if: needs.setup-checks.outputs.is_external != 'true'
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
        run: |
          chmod +x .github/scripts/upload_codecov.py
          SUBPACKAGES="${{ matrix.package }}" .github/scripts/upload_codecov.py

  tests-summary:
    name: "Tests"
    needs: [test-matrix]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Check test results
        run: |
          echo "Test job results: ${{ toJSON(needs) }}"
          if [ "${{ contains(join(needs.*.result, ','), 'failure') }}" == "true" ]; then
            echo "One or more test jobs failed"
            exit 1
          elif [ "${{ contains(join(needs.*.result, ','), 'cancelled') }}" == "true" ]; then
            echo "One or more test jobs were cancelled"
            exit 1
          else
            echo "All test jobs completed successfully"
          fi

  mettagrid-cpp-build:
    name: "MettaGrid C++ Build & Coverage"
    needs: [setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_test == 'true')
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup uv
        uses: ./.github/actions/setup-uv
        with:
          install-mode: "testing"

      - name: Build MettaGrid C++ (with coverage)
        id: mettagrid_build_check
        run: |
          .github/scripts/check_mettagrid_build.py --with-coverage

      - name: Debug - Print full build output
        if: always()
        run: |
          echo "Full build output:"
          echo "${{ steps.mettagrid_build_check.outputs.full_output }}"

      - name: Check MettaGrid build warnings
        if: always()
        run: |
          errors=${{ steps.mettagrid_build_check.outputs.total_errors }}
          warnings=${{ steps.mettagrid_build_check.outputs.total_warnings }}
          runtime_issues=${{ steps.mettagrid_build_check.outputs.runtime_issues }}
          max_warnings=50

          # Fail if any of the values are not set
          if [ -z "$errors" ] || [ -z "$warnings" ] || [ -z "$runtime_issues" ]; then
            echo "❌ Missing value for errors, warnings, or runtime issues!"
            exit 1
          fi

          echo "Build Summary: $errors errors, $warnings warnings, $runtime_issues runtime issues"
          echo "Maximum allowed warnings: $max_warnings"

          if [ "$errors" -gt 0 ]; then
            echo "❌ MettaGrid has build errors!"
            exit 1
          elif [ "$runtime_issues" -gt 0 ]; then
            echo "❌ MettaGrid has runtime issues (segfaults, test failures)!"
            exit 1
          elif [ "$warnings" -gt "$max_warnings" ]; then
            echo "⚠️ Too many warnings in MettaGrid build ($warnings > $max_warnings)"
            exit 1
          else
            echo "✅ MettaGrid build quality check passed"
          fi

      - name: Upload MettaGrid C++ coverage to Codecov
        if: needs.setup-checks.outputs.is_external != 'true' && steps.mettagrid_build_check.outputs.build_success == 'true'
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./mettagrid/build-debug/coverage.info
          flags: mettagrid_cpp
          name: mettagrid-cpp-coverage
          fail_ci_if_error: false
          verbose: true

  smoke-test:
    name: "Smoke Tests"
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false')
    needs: [setup-checks]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    strategy:
      matrix:
        test_type: [train, replay, evals]
      fail-fast: false
    env:
      CHECKPOINT_PATH: ./train_dir/github_test/checkpoints/
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup uv
        uses: ./.github/actions/setup-uv

      # Training
      - name: Training smoke test
        if: matrix.test_type == 'train'
        id: train
        env:
          WANDB_API_KEY: set_but_not_used
          AWS_ACCESS_KEY_ID: set_but_not_used
          AWS_SECRET_ACCESS_KEY: set_but_not_used
        run: |
          mkdir -p train_dir
          uv run .github/scripts/train_smoke_test.py
          ls -la $CHECKPOINT_PATH || echo "Warning: Checkpoint directory not created"

      - name: Save training benchmark
        if: matrix.test_type == 'train'
        uses: ./.github/actions/save-benchmarks
        with:
          name: train_smoke_test
          metrics: '{"duration": ${{ steps.train.outputs.duration }}, "memory_usage": ${{ steps.train.outputs.memory_peak_mb }}}'
          filename: smoke_test_train_benchmark_results.json

      # Replay
      - name: Run super fast training
        if: matrix.test_type == 'replay'
        run: |
          uv run --no-sync tools/train.py trainer.total_timesteps=10 run=github_test +user=ci wandb=off

      - name: Replay smoke test
        if: matrix.test_type == 'replay'
        id: replay
        env:
          WANDB_API_KEY: set_but_not_used
          AWS_ACCESS_KEY_ID: set_but_not_used
          AWS_SECRET_ACCESS_KEY: set_but_not_used
        run: |
          uv run .github/scripts/replay_smoke_test.py

      - name: Save replay benchmark
        if: matrix.test_type == 'replay'
        uses: ./.github/actions/save-benchmarks
        with:
          name: replay_smoke_test
          metrics: '{"duration": ${{ steps.replay.outputs.duration }}, "memory_usage": ${{ steps.replay.outputs.memory_peak_mb }}}'
          filename: smoke_test_replay_benchmark_results.json

      # Eval
      - name: Check evals smoke test policy availability
        if: matrix.test_type == 'evals' && needs.setup-checks.outputs.is_external != 'true'
        id: check_evals_policy
        continue-on-error: true
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
          POLICY: ${{ vars.EVAL_SMOKE_TEST_POLICY }}
        run: |
          uv run .github/scripts/validate-wandb-run.py && echo "result=success" >> "$GITHUB_OUTPUT" || echo "result=skipped" >> "$GITHUB_OUTPUT"

      - name: Run evals smoke test
        if: matrix.test_type == 'evals' && steps.check_evals_policy.outputs.result == 'success' && needs.setup-checks.outputs.is_external != 'true'
        id: eval_smoke_test
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
          AWS_ACCESS_KEY_ID: set_but_not_used
          AWS_SECRET_ACCESS_KEY: set_but_not_used
          POLICY: ${{ vars.EVAL_SMOKE_TEST_POLICY }}
          MIN_REWARD: ${{ vars.EVAL_SMOKE_TEST_MIN_REWARD }}
          MAX_ATTEMPTS: ${{ vars.EVAL_SMOKE_TEST_MAX_ATTEMPTS }}
        run: |
          uv run .github/scripts/eval_smoke_test.py

      - name: Save evals benchmark
        if: matrix.test_type == 'evals' && steps.eval_smoke_test.conclusion == 'success'
        uses: ./.github/actions/save-benchmarks
        with:
          name: evals_smoke_test
          metrics: '{"duration": ${{ steps.eval_smoke_test.outputs.duration }}, "memory_usage": ${{ steps.eval_smoke_test.outputs.memory_peak_mb }}}'
          filename: smoke_test_evals_benchmark_results.json

      - name: Upload benchmark file
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: smoke-test-benchmark-${{ matrix.test_type }}
          path: |
            smoke_test_*_benchmark_results.json
          retention-days: 1
          if-no-files-found: warn

  collect-smoke-test-results:
    name: "Collect Smoke Test Results"
    if: always()
    needs: [smoke-test]
    runs-on: ubuntu-latest
    steps:
      - name: Download all benchmark artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: smoke-test-benchmark-*
          merge-multiple: true

      - name: Upload combined results
        uses: actions/upload-artifact@v4
        with:
          name: smoke-test-benchmark-results-combined
          path: "*.json"
          retention-days: 7

  smoke-tests-summary:
    name: "Smoke Tests"
    needs: [smoke-test]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Check test results
        run: |
          echo "Smoke test job results: ${{ toJSON(needs) }}"
          if [ "${{ contains(join(needs.*.result, ','), 'failure') }}" == "true" ]; then
            echo "One or more smoke test jobs failed"
            exit 1
          elif [ "${{ contains(join(needs.*.result, ','), 'cancelled') }}" == "true" ]; then
            echo "One or more smoke test jobs were cancelled"
            exit 1
          else
            echo "All smoke test jobs completed successfully"
          fi

  python-benchmark:
    name: "Python Benchmarks"
    needs: [setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_benchmark == 'true')
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup uv
        uses: ./.github/actions/setup-uv

      - name: Run Python benchmarks
        env:
          PYTHONOPTIMIZE: 1 # Disable __debug__ blocks and asserts for accurate benchmarks
        run: |
          # Main benchmarks
          pytest --benchmark-only --benchmark-json=main_benchmark_results.json

          # Mettagrid benchmarks
          cd mettagrid
          pytest --benchmark-only --benchmark-json=../mettagrid_benchmark_results.json

      - name: Upload Python benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: python-benchmark-results
          path: |
            main_benchmark_results.json
            mettagrid_benchmark_results.json
          retention-days: 1

  cpp-benchmark:
    name: "C++ Benchmarks"
    needs: [setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_benchmark == 'true')
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup uv
        uses: ./.github/actions/setup-uv

      - name: Build and run C++ benchmarks
        run: |
          source .venv/bin/activate
          cd mettagrid
          make benchmark

          mkdir -p benchmark_output
          for f in build-release/*_benchmark; do
            "$f" --benchmark_format=json > benchmark_output/$(basename $f).json
          done

      - name: Upload C++ benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: cpp-benchmark-results
          path: mettagrid/benchmark_output/*.json
          retention-days: 1

  upload-benchmarks:
    name: "Upload Benchmarks"
    needs: [python-benchmark, cpp-benchmark]
    if: |
      (needs.setup-checks.outputs.run_benchmark == 'true') &&
      (always() && (needs.python-benchmark.result == 'success' || needs.cpp-benchmark.result == 'success'))
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download Python benchmark results
        uses: actions/download-artifact@v4
        with:
          name: python-benchmark-results
          path: ./benchmarks/
        continue-on-error: true

      - name: Download C++ benchmark results
        uses: actions/download-artifact@v4
        with:
          name: cpp-benchmark-results
          path: ./benchmarks/
        continue-on-error: true

      - name: Debug downloaded artifacts
        shell: bash
        run: |
          echo "=== Checking downloaded artifacts ==="
          ls -la ./benchmarks/ || echo "No benchmarks directory"

          echo "=== Python benchmark files content ==="
          for file in ./benchmarks/main_benchmark_results.json ./benchmarks/mettagrid_benchmark_results.json; do
            if [ -f "$file" ]; then
              echo "--- $file ---"
              echo "File size: $(wc -c < "$file") bytes"
              head -20 "$file" || echo "Could not read $file"
            else
              echo "$file not found"
            fi
          done

      - name: Combine all benchmark results
        uses: ./.github/actions/combine-all-benchmarks
        with:
          python_files: "./benchmarks/main_benchmark_results.json,./benchmarks/mettagrid_benchmark_results.json"
          cpp_files: "./benchmarks/*.json"
          output_file: "unified_benchmark_results.json"

      - name: Validate generated BMF files
        shell: bash
        run: |
          echo "=== Validating generated BMF files ==="

          if [ -f "unified_benchmark_results.json" ]; then
            if jq empty unified_benchmark_results.json; then
              echo "✅ unified_benchmark_results.json is valid JSON"
              echo "File size: $(wc -c < unified_benchmark_results.json) bytes"
              echo "Number of benchmarks: $(jq 'length' unified_benchmark_results.json)"
              echo "Sample benchmark names:"
              jq -r 'keys[0:3][]' unified_benchmark_results.json || echo "No benchmarks found"
            else
              echo "❌ unified_benchmark_results.json is invalid JSON"
              exit 1
            fi
          else
            echo "❌ unified_benchmark_results.json not found"
            exit 1
          fi

      - name: Check for significant performance changes
        if: needs.setup-checks.outputs.is_external != 'true'
        uses: ./.github/actions/call-bencher-api
        with:
          bencher_token: ${{ secrets.BENCHER_API_TOKEN }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          benchmark_file: "unified_benchmark_results.json"
        continue-on-error: true

      - name: Upload benchmark artifacts for debugging
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: processed-benchmark-results
          path: |
            unified_benchmark_results.json
          retention-days: 7
