{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train Cogames\n",
        "\n",
        "this notebook will help you to get started with using cogames with the simplest training example.\n",
        "\n",
        "Make sure you have selected T4 runtime!"
      ],
      "metadata": {
        "id": "cjidF68qkVqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "MeQylF-oknqM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eeiO_R7u_1-",
        "outputId": "37260422-d0fd-44da-fce3-43445328cfda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'metta'...\n",
            "remote: Enumerating objects: 246671, done.\u001b[K\n",
            "remote: Counting objects: 100% (2633/2633), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1151/1151), done.\u001b[K\n",
            "remote: Total 246671 (delta 1923), reused 1545 (delta 1459), pack-reused 244038 (from 3)\u001b[K\n",
            "Receiving objects: 100% (246671/246671), 499.71 MiB | 31.24 MiB/s, done.\n",
            "Resolving deltas: 100% (195728/195728), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Metta-AI/metta.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd metta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA2Hnd9rvJI5",
        "outputId": "1302a73c-71bd-4ade-cb93-7c955758e44e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/metta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Bazel"
      ],
      "metadata": {
        "id": "TrLJ7-ozk0NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install apt-transport-https curl gnupg -y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yj4JbbVv6aA",
        "outputId": "fb5c1b4e-a0ef-4e72-b3cc-c41b2f5b149a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.81.0-1ubuntu1.21).\n",
            "gnupg is already the newest version (2.2.27-3ubuntu2.4).\n",
            "gnupg set to manually installed.\n",
            "The following NEW packages will be installed:\n",
            "  apt-transport-https\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 1,510 B of archives.\n",
            "After this operation, 170 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 apt-transport-https all 2.4.14 [1,510 B]\n",
            "Fetched 1,510 B in 0s (9,379 B/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package apt-transport-https.\n",
            "(Reading database ... 125079 files and directories currently installed.)\n",
            "Preparing to unpack .../apt-transport-https_2.4.14_all.deb ...\n",
            "Unpacking apt-transport-https (2.4.14) ...\n",
            "Setting up apt-transport-https (2.4.14) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install apt-transport-https curl gnupg -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMzaGjUzwDFX",
        "outputId": "ee28f30e-2c12-4f62-8014-72f7931fbc41"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.81.0-1ubuntu1.21).\n",
            "gnupg is already the newest version (2.2.27-3ubuntu2.4).\n",
            "apt-transport-https is already the newest version (2.4.14).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://bazel.build/bazel-release.pub.gpg | sudo apt-key add -"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-jqo-FzwE85",
        "outputId": "0ba8d542-0717-467b-eee5-3bfaa1765f69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NERgnmyNwIpZ",
        "outputId": "14cabd48-39f8-4d03-fb8b-4897a2ae282b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Update and install Bazel\n",
        "!sudo apt update && sudo apt install bazel -y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WplfE0mwNUz",
        "outputId": "f1e687d2-048c-4ab0-823c-ac7f318b240a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 https://storage.googleapis.com/bazel-apt stable InRelease [2,265 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:10 https://storage.googleapis.com/bazel-apt stable/jdk1.8 amd64 Packages [17.7 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,402 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,148 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,288 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,479 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,847 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,819 kB]\n",
            "Fetched 29.0 MB in 3s (10.2 MB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mhttps://storage.googleapis.com/bazel-apt/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  google-jdk | java8-sdk-headless | java8-jdk | java8-sdk\n",
            "  | oracle-java8-installer bash-completion\n",
            "The following NEW packages will be installed:\n",
            "  bazel\n",
            "0 upgraded, 1 newly installed, 0 to remove and 48 not upgraded.\n",
            "Need to get 57.7 MB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 https://storage.googleapis.com/bazel-apt stable/jdk1.8 amd64 bazel amd64 8.4.2 [57.7 MB]\n",
            "Fetched 57.7 MB in 1s (83.5 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package bazel.\n",
            "(Reading database ... 125083 files and directories currently installed.)\n",
            "Preparing to unpack .../archives/bazel_8.4.2_amd64.deb ...\n",
            "Unpacking bazel (8.4.2) ...\n",
            "Setting up bazel (8.4.2) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bazel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSXA-0P0wQ_Z",
        "outputId": "9bdeb7b6-30ef-4f7d-f6cb-e4a1d30cfe04"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a MODULE.bazel file).\n",
            "Extracting Bazel installation...\n",
            "OpenJDK 64-Bit Server VM warning: Options -Xverify:none and -noverify were deprecated in JDK 13 and will likely be removed in a future release.\n",
            "                                                           [bazel release 8.4.2]\n",
            "Usage: bazel <command> <options> ...\n",
            "\n",
            "Available commands:\n",
            "  analyze-profile     Analyzes build profile data.\n",
            "  aquery              Analyzes the given targets and queries the action graph.\n",
            "  build               Builds the specified targets.\n",
            "  canonicalize-flags  Canonicalizes a list of bazel options.\n",
            "  clean               Removes output files and optionally stops the server.\n",
            "  coverage            Generates code coverage report for specified test targets.\n",
            "  cquery              Loads, analyzes, and queries the specified targets w/ configurations.\n",
            "  dump                Dumps the internal state of the bazel server process.\n",
            "  fetch               Fetches external repositories that are prerequisites to the targets.\n",
            "  help                Prints help for commands, or the index.\n",
            "  info                Displays runtime info about the bazel server.\n",
            "  license             Prints the license of this software.\n",
            "  mobile-install      Installs targets to mobile devices.\n",
            "  mod                 Queries the Bzlmod external dependency graph\n",
            "  print_action        Prints the command line args for compiling a file.\n",
            "  query               Executes a dependency graph query.\n",
            "  run                 Runs the specified target.\n",
            "  shutdown            Stops the bazel server.\n",
            "  sync                Syncs all repositories specified in the workspace file\n",
            "  test                Builds and runs the specified test targets.\n",
            "  vendor              Fetches external repositories into a folder specified by the flag --vendor_dir.\n",
            "  version             Prints version information for bazel.\n",
            "\n",
            "Getting more help:\n",
            "  bazel help <command>\n",
            "                   Prints help and options for <command>.\n",
            "  bazel help startup_options\n",
            "                   Options for the JVM hosting bazel.\n",
            "  bazel help target-syntax\n",
            "                   Explains the syntax for specifying targets.\n",
            "  bazel help info-keys\n",
            "                   Displays a list of keys used by the info command.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Nim"
      ],
      "metadata": {
        "id": "pyJU9uEVk7M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y curl git build-essential\n",
        "\n",
        "# 2. Install Nim via choosenim (official installer)\n",
        "!curl https://nim-lang.org/choosenim/init.sh -sSf | sh -s -- -y\n",
        "\n",
        "# 3. Add Nim to PATH for this Colab session\n",
        "import os\n",
        "os.environ[\"PATH\"] += \":/root/.nimble/bin\"\n",
        "\n",
        "# 4. Verify installation\n",
        "!nim --version\n",
        "!nimble --version\n"
      ],
      "metadata": {
        "id": "wIIstuLy0dGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a83f1c-d17c-4da8-ce27-a405de082209"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to security.ub\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Con\r                                                                               \rHit:3 https://storage.googleapis.com/bazel-apt stable InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rHit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: https://storage.googleapis.com/bazel-apt/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.21).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n",
            "choosenim-init: Downloading choosenim-0.8.16_linux_amd64\n",
            "\u001b[36m\u001b[1mDownloading \u001b[0m\u001b[0mNim 2.2.6 from nim-lang.org\n",
            "\u001b[2K\u001b[1G[                                                  ] 0.08587% 5kb/s\u001b[2K\u001b[1G[#############################                     ] 59.68% 39434kb/s\u001b[2K\u001b[1G[##################################################] 100.0% 0kb/s\n",
            "\u001b[36m\u001b[1m Extracting \u001b[0m\u001b[0mnim-2.2.6-linux_x64.tar.xz\n",
            "\u001b[36m\u001b[1m Extracting \u001b[0m\u001b[0mnim-2.2.6-linux_x64.tar\n",
            "\u001b[36m\u001b[1m   Building \u001b[0m\u001b[0mNim 2.2.6\n",
            "\u001b[36m\u001b[1m  Compiler: \u001b[0m\u001b[0mAlready built\n",
            "\u001b[36m\u001b[1m  Installed \u001b[0m\u001b[0mcomponent 'nim'\n",
            "\u001b[36m\u001b[1m  Installed \u001b[0m\u001b[0mcomponent 'nimble'\n",
            "\u001b[36m\u001b[1m  Installed \u001b[0m\u001b[0mcomponent 'nimgrep'\n",
            "\u001b[36m\u001b[1m  Installed \u001b[0m\u001b[0mcomponent 'nimpretty'\n",
            "\u001b[36m\u001b[1m  Installed \u001b[0m\u001b[0mcomponent 'nimsuggest'\n",
            "\u001b[36m\u001b[1m  Installed \u001b[0m\u001b[0mcomponent 'testament'\n",
            "\u001b[36m\u001b[1m  Installed \u001b[0m\u001b[0mcomponent 'nim-gdb'\n",
            "\u001b[32m\u001b[1m   Switched \u001b[0m\u001b[0mto Nim 2.2.6\n",
            "choosenim-init: ChooseNim installed in /root/.nimble/bin\n",
            "choosenim-init: You must now ensure that the Nimble bin dir is in your PATH.\n",
            "choosenim-init: Place the following line in the ~/.profile or ~/.bashrc file.\n",
            "choosenim-init:     export PATH=/root/.nimble/bin:$PATH\n",
            "Nim Compiler Version 2.2.6 [Linux: amd64]\n",
            "Compiled at 2025-10-31\n",
            "Copyright (c) 2006-2025 by Andreas Rumpf\n",
            "\n",
            "git hash: ab00c56904e3126ad826bb520d243513a139436a\n",
            "active boot switches: -d:release\n",
            "nimble v0.20.1 compiled at 2025-10-31 02:15:47\n",
            "git hash: couldn't determine git hash\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install all the required packages"
      ],
      "metadata": {
        "id": "Ic3PnnxKlASi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XXCUuCF6y0A",
        "outputId": "840b515a-9ee4-40fa-8f04-f80e626530ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m286 packages\u001b[0m \u001b[2min 1.00s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 766ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m4 packages\u001b[0m \u001b[2min 12ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m4 packages\u001b[0m \u001b[2min 11ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgymnasium\u001b[0m\u001b[2m==0.29.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgymnasium\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mheavyball\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mheavyball\u001b[0m\u001b[2m==2.1.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mmetta\u001b[0m\u001b[2m==0.1 (from file:///content/metta)\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mpettingzoo\u001b[0m\u001b[2m==1.24.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpettingzoo\u001b[0m\u001b[2m==1.25.0\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install git+https://github.com/PufferAI/PufferLib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3RZbHD0yYNu",
        "outputId": "55834285-de16-4693-e53b-5f9aaca91185"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m111 packages\u001b[0m \u001b[2min 24.87s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m17 packages\u001b[0m \u001b[2min 2m 11s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m5 packages\u001b[0m \u001b[2min 66ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m18 packages\u001b[0m \u001b[2min 54ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbravado\u001b[0m\u001b[2m==11.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbravado-core\u001b[0m\u001b[2m==6.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgpytorch\u001b[0m\u001b[2m==1.14.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgym\u001b[0m\u001b[2m==0.25.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgym\u001b[0m\u001b[2m==0.23.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgymnasium\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgymnasium\u001b[0m\u001b[2m==0.29.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mheavyball\u001b[0m\u001b[2m==2.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mheavyball\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjaxtyping\u001b[0m\u001b[2m==0.3.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonref\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlinear-operator\u001b[0m\u001b[2m==0.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmonotonic\u001b[0m\u001b[2m==1.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mneptune\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mpettingzoo\u001b[0m\u001b[2m==1.25.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpettingzoo\u001b[0m\u001b[2m==1.24.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpufferlib\u001b[0m\u001b[2m==3.0.0 (from git+https://github.com/PufferAI/PufferLib@1267d5285a70126e747985a644289207bbc3dfa6)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyglet\u001b[0m\u001b[2m==1.5.11\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mshimmy\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mswagger-spec-validator\u001b[0m\u001b[2m==3.0.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwadler-lindig\u001b[0m\u001b[2m==0.1.7\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!puffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EllVE_TdzL4X",
        "outputId": "aec4ea4e-c24f-4c24-dec0-1327476272bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyro/ops/stats.py:527: SyntaxWarning: invalid escape sequence '\\g'\n",
            "  we have :math:`ES^{*}(P,Q) \\ge ES^{*}(Q,Q)` with equality holding if and only if :math:`P=Q`, i.e.\n",
            "terminate called after throwing an instance of 'std::bad_alloc'\n",
            "  what():  std::bad_alloc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ignore errors and warning in this.\n",
        "!pip install numpy --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fioeeuDPzZ4B",
        "outputId": "85e337cd-0374-4735-9d20-2ba831d93251"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Code"
      ],
      "metadata": {
        "id": "DhIusBoTlUUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mettagrid import MettaGridEnv\n",
        "import torch\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_cogames_mission(mission_name=\"training_facility.harvest\", variants=None):\n",
        "    from cogames.cli.mission import get_mission\n",
        "    _, config, _ = get_mission(mission_name, variants_arg=variants)\n",
        "    return config\n",
        "\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self, device, gamma, gae_lambda):\n",
        "        self.device = device\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.clear()\n",
        "\n",
        "    def add(self, obs, action, reward, done, log_prob, value):\n",
        "        self.observations.append(torch.as_tensor(obs, device=self.device, dtype=torch.float32).clone())\n",
        "        self.actions.append(torch.as_tensor(action, device=self.device, dtype=torch.long).clone())\n",
        "        self.rewards.append(torch.as_tensor(reward, device=self.device, dtype=torch.float32).clone())\n",
        "        self.dones.append(torch.as_tensor(done, device=self.device, dtype=torch.float32).clone())\n",
        "        self.log_probs.append(torch.as_tensor(log_prob, device=self.device, dtype=torch.float32).clone())\n",
        "        self.values.append(torch.as_tensor(value, device=self.device, dtype=torch.float32).clone())\n",
        "\n",
        "    def build_training_batch(self, last_value, last_done):\n",
        "        if not self.observations:\n",
        "            raise ValueError(\"RolloutBuffer is empty\")\n",
        "\n",
        "        obs = torch.stack(self.observations)\n",
        "        actions = torch.stack(self.actions)\n",
        "        rewards = torch.stack(self.rewards)\n",
        "        dones = torch.stack(self.dones)\n",
        "        old_log_probs = torch.stack(self.log_probs)\n",
        "        old_values = torch.stack(self.values)\n",
        "\n",
        "        values = old_values.view(old_values.shape[0], -1)\n",
        "        rewards = rewards.view(rewards.shape[0], -1)\n",
        "        dones = dones.view(dones.shape[0], -1)\n",
        "\n",
        "        last_value = torch.as_tensor(last_value, device=self.device, dtype=torch.float32).view(-1)\n",
        "        last_done = torch.as_tensor(last_done, device=self.device, dtype=torch.float32).view(-1)\n",
        "\n",
        "        advantages = torch.zeros_like(values)\n",
        "        next_advantage = torch.zeros_like(last_value)\n",
        "        next_value = last_value\n",
        "        next_nonterminal = 1.0 - last_done\n",
        "\n",
        "        for step in reversed(range(values.shape[0])):\n",
        "            reward = rewards[step]\n",
        "            value = values[step]\n",
        "            done = dones[step]\n",
        "            delta = reward + self.gamma * next_value * next_nonterminal - value\n",
        "            next_advantage = delta + self.gamma * self.gae_lambda * next_nonterminal * next_advantage\n",
        "            advantages[step] = next_advantage\n",
        "            next_value = value\n",
        "            next_nonterminal = 1.0 - done\n",
        "\n",
        "        returns = advantages + values\n",
        "\n",
        "        batch = {\n",
        "            \"obs\": obs,\n",
        "            \"actions\": actions,\n",
        "            \"old_log_probs\": old_log_probs,\n",
        "            \"old_values\": values,\n",
        "            \"advantages\": advantages,\n",
        "            \"returns\": returns,\n",
        "        }\n",
        "\n",
        "        return batch\n",
        "\n",
        "    def clear(self):\n",
        "        self.observations = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.log_probs = []\n",
        "        self.values = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.observations)\n",
        "\n",
        "\n",
        "def get_action(policy, obs, device, lstm_state=None):\n",
        "    obs_tensor = torch.from_numpy(obs).float().unsqueeze(0).to(device)\n",
        "\n",
        "    state_argument = None\n",
        "    if policy.is_recurrent():\n",
        "        if lstm_state is None:\n",
        "            state_argument = {\"lstm_h\": None, \"lstm_c\": None}\n",
        "        else:\n",
        "            h, c = lstm_state\n",
        "            state_argument = {\"lstm_h\": h, \"lstm_c\": c}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        policy.network().eval()\n",
        "        if policy.is_recurrent():\n",
        "            logits, values = policy.network().forward_eval(obs_tensor, state_argument)\n",
        "            state_h, state_c = state_argument.get(\"lstm_h\"), state_argument.get(\"lstm_c\")\n",
        "            if state_h is not None and state_c is not None:\n",
        "                new_state = (state_h.detach(), state_c.detach())\n",
        "            else:\n",
        "                new_state = None\n",
        "        else:\n",
        "            logits, values = policy.network().forward_eval(obs_tensor)\n",
        "            new_state = None\n",
        "\n",
        "    dist = Categorical(logits=logits)\n",
        "    actions = dist.sample()\n",
        "    log_probs = dist.log_prob(actions)\n",
        "\n",
        "    if actions.dim() == 0:\n",
        "        actions = actions.unsqueeze(0)\n",
        "        log_probs = log_probs.unsqueeze(0)\n",
        "\n",
        "    return actions.cpu().numpy(), log_probs.detach(), values.detach(), new_state\n",
        "\n",
        "\n",
        "def ppo_update(\n",
        "    policy,\n",
        "    optimizer,\n",
        "    batch,\n",
        "    device,\n",
        "    clip_coef,\n",
        "    vf_clip_coef,\n",
        "    vf_coef,\n",
        "    ent_coef,\n",
        "    max_grad_norm,\n",
        "    ppo_epochs,\n",
        "    minibatch_size,\n",
        "):\n",
        "    obs = batch[\"obs\"].to(device)\n",
        "    actions = batch[\"actions\"].to(device)\n",
        "    old_log_probs = batch[\"old_log_probs\"].to(device)\n",
        "    old_values = batch[\"old_values\"].to(device)\n",
        "    advantages = batch[\"advantages\"].to(device)\n",
        "    returns = batch[\"returns\"].to(device)\n",
        "\n",
        "    if advantages.numel() == 0:\n",
        "        raise ValueError(\"Advantages tensor is empty\")\n",
        "\n",
        "    policy.network().train()\n",
        "\n",
        "    advantages = advantages - advantages.mean()\n",
        "    advantages = advantages / (advantages.std(unbiased=False) + 1e-8)\n",
        "\n",
        "    returns = advantages + old_values\n",
        "\n",
        "    batch_size = obs.shape[0]\n",
        "\n",
        "    policy_losses = []\n",
        "    value_losses = []\n",
        "    entropies = []\n",
        "    clip_fractions = []\n",
        "    approx_kls = []\n",
        "\n",
        "    for _ in range(ppo_epochs):\n",
        "        indices = torch.randperm(batch_size, device=device)\n",
        "        for start in range(0, batch_size, minibatch_size):\n",
        "            mb_idx = indices[start : start + minibatch_size]\n",
        "            mb_obs = obs[mb_idx]\n",
        "            mb_actions = actions[mb_idx]\n",
        "            mb_old_log_probs = old_log_probs[mb_idx]\n",
        "            mb_old_values = old_values[mb_idx]\n",
        "            mb_advantages = advantages[mb_idx]\n",
        "            mb_returns = returns[mb_idx]\n",
        "\n",
        "            # Flatten agent dimension so PPO treats each agent-step as one sample\n",
        "            mb_actions = mb_actions.reshape(mb_actions.shape[0], -1)\n",
        "            mb_old_log_probs = mb_old_log_probs.reshape(mb_old_log_probs.shape[0], -1)\n",
        "            mb_old_values = mb_old_values.reshape(mb_old_values.shape[0], -1)\n",
        "            mb_advantages = mb_advantages.reshape(mb_advantages.shape[0], -1)\n",
        "            mb_returns = mb_returns.reshape(mb_returns.shape[0], -1)\n",
        "\n",
        "            if policy.is_recurrent():\n",
        "                logits, values = policy.network().forward_eval(mb_obs, None)\n",
        "            else:\n",
        "                logits, values = policy.network().forward_eval(mb_obs)\n",
        "\n",
        "            dist = Categorical(logits=logits)\n",
        "            new_log_probs = dist.log_prob(mb_actions.squeeze(-1) if mb_actions.shape[-1] == 1 else mb_actions)\n",
        "            entropy = dist.entropy()\n",
        "\n",
        "            new_log_probs = new_log_probs.unsqueeze(-1) if new_log_probs.dim() == 1 else new_log_probs\n",
        "            log_ratio = new_log_probs - mb_old_log_probs\n",
        "            ratio = log_ratio.exp()\n",
        "            surr1 = ratio * mb_advantages\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_coef, 1.0 + clip_coef) * mb_advantages\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "            value_pred = values.reshape(values.shape[0], -1)\n",
        "            value_pred_clipped = mb_old_values + (value_pred - mb_old_values).clamp(-vf_clip_coef, vf_clip_coef)\n",
        "            value_loss_unclipped = (value_pred - mb_returns) ** 2\n",
        "            value_loss_clipped = (value_pred_clipped - mb_returns) ** 2\n",
        "            value_loss = 0.5 * torch.max(value_loss_unclipped, value_loss_clipped).mean()\n",
        "\n",
        "            kld = 0.5 * log_ratio.pow(2).mean()\n",
        "            clip_fraction = (torch.abs(ratio - 1.0) > clip_coef).float().mean()\n",
        "\n",
        "            loss = policy_loss + vf_coef * value_loss - ent_coef * entropy.mean()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(policy.network().parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            policy_losses.append(policy_loss.detach())\n",
        "            value_losses.append(value_loss.detach())\n",
        "            entropies.append(entropy.mean().detach())\n",
        "            clip_fractions.append(clip_fraction.detach())\n",
        "            approx_kls.append(kld.detach())\n",
        "\n",
        "    metrics = {\n",
        "        \"policy_loss\": torch.stack(policy_losses).mean().item(),\n",
        "        \"value_loss\": torch.stack(value_losses).mean().item(),\n",
        "        \"entropy\": torch.stack(entropies).mean().item(),\n",
        "        \"clip_fraction\": torch.stack(clip_fractions).mean().item(),\n",
        "        \"approx_kl\": torch.stack(approx_kls).mean().item(),\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def train_cogames(\n",
        "    mission_name=\"training_facility.harvest\",\n",
        "    policy_type=\"simple\",\n",
        "    num_epochs=10,\n",
        "    batch_size=256,\n",
        "    minibatch_size=64,\n",
        "    ppo_epochs=4,\n",
        "    learning_rate=3e-4,\n",
        "    # Metta-tuned hyperparameters from PPOConfig\n",
        "    gamma=0.977,\n",
        "    gae_lambda=0.891477,\n",
        "    clip_coef=0.264407,\n",
        "    vf_clip_coef=0.1,\n",
        "    vf_coef=0.897619,\n",
        "    ent_coef=0.01,\n",
        "    max_grad_norm=0.5,\n",
        "    save_path=\"cogames_policy.pt\",\n",
        "    device=\"cpu\",\n",
        "    variants=None,\n",
        "    seed=42,\n",
        "):\n",
        "    if batch_size <= 0:\n",
        "        raise ValueError(\"batch_size must be positive\")\n",
        "    if minibatch_size <= 0:\n",
        "        raise ValueError(\"minibatch_size must be positive\")\n",
        "    if minibatch_size > batch_size:\n",
        "        raise ValueError(\"minibatch_size must be <= batch_size\")\n",
        "\n",
        "    device = torch.device(device)\n",
        "    config = get_cogames_mission(mission_name, variants=variants)\n",
        "    env = MettaGridEnv(env_cfg=config)\n",
        "    obs, _ = env.reset(seed=seed)\n",
        "\n",
        "    if policy_type == \"simple\":\n",
        "        from cogames.policy.simple import SimplePolicy\n",
        "\n",
        "        policy = SimplePolicy(env, device)\n",
        "    elif policy_type == \"lstm\":\n",
        "        from cogames.policy.lstm import LSTMPolicy\n",
        "\n",
        "        policy = LSTMPolicy(env, device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown policy type: {policy_type}\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(policy.network().parameters(), lr=learning_rate)\n",
        "    best_reward = -float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        buffer = RolloutBuffer(device=device, gamma=gamma, gae_lambda=gae_lambda)\n",
        "        obs, _ = env.reset(seed=seed + epoch)\n",
        "        epoch_reward = 0.0\n",
        "        hearts_collected = 0.0\n",
        "        lstm_state = None\n",
        "        steps_collected = 0\n",
        "        last_done_flags = np.zeros(getattr(env, \"num_agents\", 1), dtype=np.float32)\n",
        "\n",
        "        while len(buffer) < batch_size:\n",
        "            actions_np, log_probs, values, next_state = get_action(policy, obs, device, lstm_state)\n",
        "            next_obs, rewards, terminals, truncations, _ = env.step(actions_np)\n",
        "\n",
        "            dones = np.logical_or(terminals, truncations)\n",
        "            buffer.add(obs, actions_np, rewards, dones, log_probs, values)\n",
        "\n",
        "            step_reward = float(np.sum(rewards))\n",
        "            epoch_reward += step_reward\n",
        "            if step_reward > 0:\n",
        "                hearts_collected += step_reward\n",
        "\n",
        "            last_done_flags = np.asarray(dones, dtype=np.float32).copy()\n",
        "\n",
        "            if np.any(dones):\n",
        "                next_obs, _ = env.reset()\n",
        "                lstm_state = None\n",
        "            else:\n",
        "                lstm_state = next_state\n",
        "\n",
        "            obs = next_obs\n",
        "            steps_collected += 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            obs_tensor = torch.from_numpy(obs).float().unsqueeze(0).to(device)\n",
        "            if policy.is_recurrent():\n",
        "                state_arg = None\n",
        "                if lstm_state is not None:\n",
        "                    h, c = lstm_state\n",
        "                    state_arg = {\"lstm_h\": h, \"lstm_c\": c}\n",
        "                _, last_value_tensor = policy.network().forward_eval(obs_tensor, state_arg)\n",
        "            else:\n",
        "                _, last_value_tensor = policy.network().forward_eval(obs_tensor)\n",
        "\n",
        "        last_value = last_value_tensor.squeeze(0)\n",
        "        if last_value.dim() > 1:\n",
        "            last_value = last_value.squeeze(-1)\n",
        "        last_value = last_value.detach().cpu()\n",
        "\n",
        "        last_done_tensor = torch.as_tensor(last_done_flags, dtype=torch.float32)\n",
        "        if last_done_tensor.dim() == 0:\n",
        "            last_done_tensor = last_done_tensor.unsqueeze(0)\n",
        "\n",
        "        last_value = last_value * (1.0 - last_done_tensor)\n",
        "\n",
        "        batch = buffer.build_training_batch(last_value=last_value, last_done=last_done_tensor)\n",
        "\n",
        "        metrics = ppo_update(\n",
        "            policy=policy,\n",
        "            optimizer=optimizer,\n",
        "            batch=batch,\n",
        "            device=device,\n",
        "            clip_coef=clip_coef,\n",
        "            vf_clip_coef=vf_clip_coef,\n",
        "            vf_coef=vf_coef,\n",
        "            ent_coef=ent_coef,\n",
        "            max_grad_norm=max_grad_norm,\n",
        "            ppo_epochs=ppo_epochs,\n",
        "            minibatch_size=minibatch_size,\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}/{num_epochs} - Reward: {epoch_reward:.2f}, \"\n",
        "            f\"Policy loss: {metrics['policy_loss']:.4f}, Value loss: {metrics['value_loss']:.4f}\"\n",
        "        )\n",
        "\n",
        "        if epoch_reward > best_reward:\n",
        "            best_reward = epoch_reward\n",
        "            policy.save_policy_data(save_path)\n",
        "\n",
        "    env.close()\n"
      ],
      "metadata": {
        "id": "JiG5E2GcBnZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9f39ac4-693b-4636-c752-f25c406c621a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Policy"
      ],
      "metadata": {
        "id": "SKgx5Wt9lXyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_cogames(\n",
        "    mission_name=\"training_facility.harvest\",\n",
        "    policy_type=\"simple\", # or use lstm\n",
        "    num_epochs=50000,\n",
        "    batch_size=2048,\n",
        "    minibatch_size=512,\n",
        "    ppo_epochs=4,\n",
        "    learning_rate=3e-4,\n",
        "    save_path=\"cogames_policy.pt\",\n",
        "    variants=[\"neutral_faced\"], # Available variants: mined_out, dark_side, super_charged, rough_terrain, solar_flare, desert, forest, city, caves, store_base, extractor_base, both_base, lonely_heart, pack_rat, energized, neutral_faced\n",
        "    device=\"cuda\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "PHXUnV_cz6F2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b804917f-468d-4341-8de7-c1a3c10fb2ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50000 - Reward: 0.00, Policy loss: -0.0170, Value loss: 0.4780\n",
            "Epoch 2/50000 - Reward: 2.00, Policy loss: -0.0103, Value loss: 0.4743\n",
            "Epoch 3/50000 - Reward: 2.00, Policy loss: -0.0253, Value loss: 0.4733\n",
            "Epoch 4/50000 - Reward: 0.00, Policy loss: 0.0074, Value loss: 0.4599\n",
            "Epoch 5/50000 - Reward: 2.00, Policy loss: -0.0132, Value loss: 0.4835\n",
            "Epoch 6/50000 - Reward: 0.00, Policy loss: -0.0035, Value loss: 0.4949\n",
            "Epoch 7/50000 - Reward: 0.00, Policy loss: -0.0043, Value loss: 0.4693\n",
            "Epoch 8/50000 - Reward: 0.00, Policy loss: -0.0081, Value loss: 0.4688\n",
            "Epoch 9/50000 - Reward: 4.00, Policy loss: -0.0063, Value loss: 0.4885\n",
            "Epoch 10/50000 - Reward: 8.00, Policy loss: -0.0072, Value loss: 0.4879\n",
            "Epoch 11/50000 - Reward: 0.00, Policy loss: -0.0286, Value loss: 0.4649\n",
            "Epoch 12/50000 - Reward: 0.00, Policy loss: 0.0287, Value loss: 0.4587\n",
            "Epoch 13/50000 - Reward: 0.00, Policy loss: -0.0094, Value loss: 0.4945\n",
            "Epoch 14/50000 - Reward: 3.00, Policy loss: -0.0071, Value loss: 0.4958\n",
            "Epoch 15/50000 - Reward: 0.00, Policy loss: -0.0216, Value loss: 0.4589\n",
            "Epoch 16/50000 - Reward: 0.00, Policy loss: -0.0154, Value loss: 0.4624\n",
            "Epoch 17/50000 - Reward: 0.00, Policy loss: -0.0163, Value loss: 0.4722\n",
            "Epoch 18/50000 - Reward: 5.00, Policy loss: -0.0096, Value loss: 0.4919\n",
            "Epoch 19/50000 - Reward: 0.00, Policy loss: -0.0256, Value loss: 0.4661\n",
            "Epoch 20/50000 - Reward: 0.00, Policy loss: -0.0016, Value loss: 0.4666\n",
            "Epoch 21/50000 - Reward: 0.00, Policy loss: -0.0002, Value loss: 0.4669\n",
            "Epoch 22/50000 - Reward: 0.00, Policy loss: -0.0327, Value loss: 0.4574\n",
            "Epoch 23/50000 - Reward: 1.00, Policy loss: -0.0046, Value loss: 0.4862\n",
            "Epoch 24/50000 - Reward: 0.00, Policy loss: -0.0203, Value loss: 0.4748\n",
            "Epoch 25/50000 - Reward: 0.00, Policy loss: 0.0018, Value loss: 0.4671\n",
            "Epoch 26/50000 - Reward: 0.00, Policy loss: -0.0032, Value loss: 0.4806\n",
            "Epoch 27/50000 - Reward: 0.00, Policy loss: -0.0088, Value loss: 0.4806\n",
            "Epoch 28/50000 - Reward: 2.00, Policy loss: -0.0151, Value loss: 0.4951\n",
            "Epoch 29/50000 - Reward: 0.00, Policy loss: -0.0132, Value loss: 0.4807\n",
            "Epoch 30/50000 - Reward: 0.00, Policy loss: 0.0030, Value loss: 0.4798\n",
            "Epoch 31/50000 - Reward: 0.00, Policy loss: -0.0043, Value loss: 0.4735\n",
            "Epoch 32/50000 - Reward: 0.00, Policy loss: -0.0148, Value loss: 0.4812\n",
            "Epoch 33/50000 - Reward: 0.00, Policy loss: -0.0071, Value loss: 0.4748\n",
            "Epoch 34/50000 - Reward: 0.00, Policy loss: -0.0059, Value loss: 0.4814\n",
            "Epoch 35/50000 - Reward: 0.00, Policy loss: 0.0011, Value loss: 0.4879\n",
            "Epoch 36/50000 - Reward: 0.00, Policy loss: -0.0162, Value loss: 0.4918\n",
            "Epoch 37/50000 - Reward: 0.00, Policy loss: 0.0177, Value loss: 0.4849\n",
            "Epoch 38/50000 - Reward: 0.00, Policy loss: -0.0012, Value loss: 0.4907\n",
            "Epoch 39/50000 - Reward: 0.00, Policy loss: -0.0066, Value loss: 0.4969\n",
            "Epoch 40/50000 - Reward: 0.00, Policy loss: -0.0118, Value loss: 0.4871\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1031837785.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_cogames(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmission_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training_facility.harvest\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpolicy_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"simple\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1950678826.py\u001b[0m in \u001b[0;36mtrain_cogames\u001b[0;34m(mission_name, policy_type, num_epochs, batch_size, minibatch_size, ppo_epochs, learning_rate, gamma, gae_lambda, clip_coef, vf_clip_coef, vf_coef, ent_coef, max_grad_norm, save_path, device, variants, seed)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mactions_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1950678826.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(policy, obs, device, lstm_state)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "56JNCb5HAD9T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}