{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train Cogames\n",
        "\n",
        "this notebook will help you to get started with cogames with the simplest training example.\n",
        "\n",
        "Make sure you have selected T4 runtime!"
      ],
      "metadata": {
        "id": "cjidF68qkVqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "MeQylF-oknqM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eeiO_R7u_1-"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Metta-AI/metta.git\n",
        "%cd metta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Bazel"
      ],
      "metadata": {
        "id": "TrLJ7-ozk0NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Update and install Bazel\n",
        "!sudo apt update && sudo apt install bazel -y\n",
        "!bazel --version"
      ],
      "metadata": {
        "id": "7yj4JbbVv6aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Nim"
      ],
      "metadata": {
        "id": "pyJU9uEVk7M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y curl git build-essential\n",
        "\n",
        "#Install Nim via choosenim (official installer)\n",
        "!curl https://nim-lang.org/choosenim/init.sh -sSf | sh -s -- -y\n",
        "# Add Nim to PATH for this Colab session\n",
        "import os\n",
        "os.environ[\"PATH\"] += \":/root/.nimble/bin\"\n",
        "\n",
        "# Verify installation\n",
        "!nim --version\n",
        "!nimble --version\n"
      ],
      "metadata": {
        "id": "wIIstuLy0dGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install all the required packages"
      ],
      "metadata": {
        "id": "Ic3PnnxKlASi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install .\n",
        "!uv pip install git+https://github.com/PufferAI/PufferLib"
      ],
      "metadata": {
        "id": "8XXCUuCF6y0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ignore errors and warning in this.\n",
        "!pip install numpy --upgrade"
      ],
      "metadata": {
        "id": "fioeeuDPzZ4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Code + PPO Loss"
      ],
      "metadata": {
        "id": "DhIusBoTlUUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It includes a complete reinforcement learning training loop that implements a rollout buffer for generalized advantage estimation (GAE), a PPO update function with clipping, entropy regularization, and value loss computation, and supports both simple and LSTM-based policies. It interacts with the MettaGridEnv simulation, collecting observations, actions, and rewards over multiple epochs to optimize the policy network.\n",
        "\n",
        "You can modify mission configurations, hyperparameters, and policy types to experiment with different training setups."
      ],
      "metadata": {
        "id": "Su4IUPJOz3tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from mettagrid import MettaGridEnv\n",
        "import torch\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "dHVR0UQw0R3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cogames_mission(mission_name=\"training_facility.harvest\", variants=None):\n",
        "    from cogames.cli.mission import get_mission\n",
        "    _, config, _ = get_mission(mission_name, variants_arg=variants)\n",
        "    return config\n",
        "\n"
      ],
      "metadata": {
        "id": "IgV2JmRz0cHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rollout Buffer Class which stores all the Expereince\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self, device, gamma, gae_lambda):\n",
        "        self.device = device\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.clear()\n",
        "\n",
        "    def add(self, obs, action, reward, done, log_prob, value):\n",
        "        self.observations.append(torch.as_tensor(obs, device=self.device, dtype=torch.float32).clone())\n",
        "        self.actions.append(torch.as_tensor(action, device=self.device, dtype=torch.long).clone())\n",
        "        self.rewards.append(torch.as_tensor(reward, device=self.device, dtype=torch.float32).clone())\n",
        "        self.dones.append(torch.as_tensor(done, device=self.device, dtype=torch.float32).clone())\n",
        "        self.log_probs.append(torch.as_tensor(log_prob, device=self.device, dtype=torch.float32).clone())\n",
        "        self.values.append(torch.as_tensor(value, device=self.device, dtype=torch.float32).clone())\n",
        "\n",
        "    def build_training_batch(self, last_value, last_done):\n",
        "        if not self.observations:\n",
        "            raise ValueError(\"RolloutBuffer is empty\")\n",
        "\n",
        "        obs = torch.stack(self.observations)\n",
        "        actions = torch.stack(self.actions)\n",
        "        rewards = torch.stack(self.rewards)\n",
        "        dones = torch.stack(self.dones)\n",
        "        old_log_probs = torch.stack(self.log_probs)\n",
        "        old_values = torch.stack(self.values)\n",
        "\n",
        "        values = old_values.view(old_values.shape[0], -1)\n",
        "        rewards = rewards.view(rewards.shape[0], -1)\n",
        "        dones = dones.view(dones.shape[0], -1)\n",
        "\n",
        "        last_value = torch.as_tensor(last_value, device=self.device, dtype=torch.float32).view(-1)\n",
        "        last_done = torch.as_tensor(last_done, device=self.device, dtype=torch.float32).view(-1)\n",
        "\n",
        "        advantages = torch.zeros_like(values)\n",
        "        next_advantage = torch.zeros_like(last_value)\n",
        "        next_value = last_value\n",
        "        next_nonterminal = 1.0 - last_done\n",
        "\n",
        "        for step in reversed(range(values.shape[0])):\n",
        "            reward = rewards[step]\n",
        "            value = values[step]\n",
        "            done = dones[step]\n",
        "            delta = reward + self.gamma * next_value * next_nonterminal - value\n",
        "            next_advantage = delta + self.gamma * self.gae_lambda * next_nonterminal * next_advantage\n",
        "            advantages[step] = next_advantage\n",
        "            next_value = value\n",
        "            next_nonterminal = 1.0 - done\n",
        "\n",
        "        returns = advantages + values\n",
        "\n",
        "        batch = {\n",
        "            \"obs\": obs,\n",
        "            \"actions\": actions,\n",
        "            \"old_log_probs\": old_log_probs,\n",
        "            \"old_values\": values,\n",
        "            \"advantages\": advantages,\n",
        "            \"returns\": returns,\n",
        "        }\n",
        "\n",
        "        return batch\n",
        "\n",
        "    def clear(self):\n",
        "        self.observations = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.log_probs = []\n",
        "        self.values = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.observations)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J96sgkQb0qSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function give the action\n",
        "\n",
        "def get_action(policy, obs, device, lstm_state=None):\n",
        "    obs_tensor = torch.from_numpy(obs).float().unsqueeze(0).to(device)\n",
        "\n",
        "    state_argument = None\n",
        "    if policy.is_recurrent():\n",
        "        if lstm_state is None:\n",
        "            state_argument = {\"lstm_h\": None, \"lstm_c\": None}\n",
        "        else:\n",
        "            h, c = lstm_state\n",
        "            state_argument = {\"lstm_h\": h, \"lstm_c\": c}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        policy.network().eval()\n",
        "        if policy.is_recurrent():\n",
        "            logits, values = policy.network().forward_eval(obs_tensor, state_argument)\n",
        "            state_h, state_c = state_argument.get(\"lstm_h\"), state_argument.get(\"lstm_c\")\n",
        "            if state_h is not None and state_c is not None:\n",
        "                new_state = (state_h.detach(), state_c.detach())\n",
        "            else:\n",
        "                new_state = None\n",
        "        else:\n",
        "            logits, values = policy.network().forward_eval(obs_tensor)\n",
        "            new_state = None\n",
        "\n",
        "    dist = Categorical(logits=logits)\n",
        "    actions = dist.sample()\n",
        "    log_probs = dist.log_prob(actions)\n",
        "\n",
        "    if actions.dim() == 0:\n",
        "        actions = actions.unsqueeze(0)\n",
        "        log_probs = log_probs.unsqueeze(0)\n",
        "\n",
        "    return actions.cpu().numpy(), log_probs.detach(), values.detach(), new_state"
      ],
      "metadata": {
        "id": "VJr7Y4Cb0w9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PPO Update function\n",
        "\n",
        "\n",
        "def ppo_update(\n",
        "    policy,\n",
        "    optimizer,\n",
        "    batch,\n",
        "    device,\n",
        "    clip_coef,\n",
        "    vf_clip_coef,\n",
        "    vf_coef,\n",
        "    ent_coef,\n",
        "    max_grad_norm,\n",
        "    ppo_epochs,\n",
        "    minibatch_size,\n",
        "):\n",
        "    obs = batch[\"obs\"].to(device)\n",
        "    actions = batch[\"actions\"].to(device)\n",
        "    old_log_probs = batch[\"old_log_probs\"].to(device)\n",
        "    old_values = batch[\"old_values\"].to(device)\n",
        "    advantages = batch[\"advantages\"].to(device)\n",
        "    returns = batch[\"returns\"].to(device)\n",
        "\n",
        "    if advantages.numel() == 0:\n",
        "        raise ValueError(\"Advantages tensor is empty\")\n",
        "\n",
        "    policy.network().train()\n",
        "\n",
        "    advantages = advantages - advantages.mean()\n",
        "    advantages = advantages / (advantages.std(unbiased=False) + 1e-8)\n",
        "\n",
        "    returns = advantages + old_values\n",
        "\n",
        "    batch_size = obs.shape[0]\n",
        "\n",
        "    policy_losses = []\n",
        "    value_losses = []\n",
        "    entropies = []\n",
        "    clip_fractions = []\n",
        "    approx_kls = []\n",
        "\n",
        "    for _ in range(ppo_epochs):\n",
        "        indices = torch.randperm(batch_size, device=device)\n",
        "        for start in range(0, batch_size, minibatch_size):\n",
        "            mb_idx = indices[start : start + minibatch_size]\n",
        "            mb_obs = obs[mb_idx]\n",
        "            mb_actions = actions[mb_idx]\n",
        "            mb_old_log_probs = old_log_probs[mb_idx]\n",
        "            mb_old_values = old_values[mb_idx]\n",
        "            mb_advantages = advantages[mb_idx]\n",
        "            mb_returns = returns[mb_idx]\n",
        "\n",
        "            # Flatten agent dimension so PPO treats each agent-step as one sample\n",
        "            mb_actions = mb_actions.reshape(mb_actions.shape[0], -1)\n",
        "            mb_old_log_probs = mb_old_log_probs.reshape(mb_old_log_probs.shape[0], -1)\n",
        "            mb_old_values = mb_old_values.reshape(mb_old_values.shape[0], -1)\n",
        "            mb_advantages = mb_advantages.reshape(mb_advantages.shape[0], -1)\n",
        "            mb_returns = mb_returns.reshape(mb_returns.shape[0], -1)\n",
        "\n",
        "            if policy.is_recurrent():\n",
        "                logits, values = policy.network().forward_eval(mb_obs, None)\n",
        "            else:\n",
        "                logits, values = policy.network().forward_eval(mb_obs)\n",
        "\n",
        "            dist = Categorical(logits=logits)\n",
        "            new_log_probs = dist.log_prob(mb_actions.squeeze(-1) if mb_actions.shape[-1] == 1 else mb_actions)\n",
        "            entropy = dist.entropy()\n",
        "\n",
        "            new_log_probs = new_log_probs.unsqueeze(-1) if new_log_probs.dim() == 1 else new_log_probs\n",
        "            log_ratio = new_log_probs - mb_old_log_probs\n",
        "            ratio = log_ratio.exp()\n",
        "            surr1 = ratio * mb_advantages\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_coef, 1.0 + clip_coef) * mb_advantages\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "            value_pred = values.reshape(values.shape[0], -1)\n",
        "            value_pred_clipped = mb_old_values + (value_pred - mb_old_values).clamp(-vf_clip_coef, vf_clip_coef)\n",
        "            value_loss_unclipped = (value_pred - mb_returns) ** 2\n",
        "            value_loss_clipped = (value_pred_clipped - mb_returns) ** 2\n",
        "            value_loss = 0.5 * torch.max(value_loss_unclipped, value_loss_clipped).mean()\n",
        "\n",
        "            kld = 0.5 * log_ratio.pow(2).mean()\n",
        "            clip_fraction = (torch.abs(ratio - 1.0) > clip_coef).float().mean()\n",
        "\n",
        "            loss = policy_loss + vf_coef * value_loss - ent_coef * entropy.mean()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(policy.network().parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            policy_losses.append(policy_loss.detach())\n",
        "            value_losses.append(value_loss.detach())\n",
        "            entropies.append(entropy.mean().detach())\n",
        "            clip_fractions.append(clip_fraction.detach())\n",
        "            approx_kls.append(kld.detach())\n",
        "\n",
        "    metrics = {\n",
        "        \"policy_loss\": torch.stack(policy_losses).mean().item(),\n",
        "        \"value_loss\": torch.stack(value_losses).mean().item(),\n",
        "        \"entropy\": torch.stack(entropies).mean().item(),\n",
        "        \"clip_fraction\": torch.stack(clip_fractions).mean().item(),\n",
        "        \"approx_kl\": torch.stack(approx_kls).mean().item(),\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "tgKK6rmx05S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Training loop\n",
        "\n",
        "def train_cogames(\n",
        "    mission_name=\"training_facility.harvest\",\n",
        "    policy_type=\"simple\",\n",
        "    num_epochs=10,\n",
        "    batch_size=256,\n",
        "    minibatch_size=64,\n",
        "    ppo_epochs=4,\n",
        "    learning_rate=3e-4,\n",
        "    # Metta-tuned hyperparameters from PPOConfig\n",
        "    gamma=0.977,\n",
        "    gae_lambda=0.891477,\n",
        "    clip_coef=0.264407,\n",
        "    vf_clip_coef=0.1,\n",
        "    vf_coef=0.897619,\n",
        "    ent_coef=0.01,\n",
        "    max_grad_norm=0.5,\n",
        "    save_path=\"cogames_policy.pt\",\n",
        "    device=\"cpu\",\n",
        "    variants=None,\n",
        "    seed=42,\n",
        "):\n",
        "    if batch_size <= 0:\n",
        "        raise ValueError(\"batch_size must be positive\")\n",
        "    if minibatch_size <= 0:\n",
        "        raise ValueError(\"minibatch_size must be positive\")\n",
        "    if minibatch_size > batch_size:\n",
        "        raise ValueError(\"minibatch_size must be <= batch_size\")\n",
        "\n",
        "    device = torch.device(device)\n",
        "    config = get_cogames_mission(mission_name, variants=variants)\n",
        "    env = MettaGridEnv(env_cfg=config)\n",
        "    obs, _ = env.reset(seed=seed)\n",
        "\n",
        "    if policy_type == \"simple\":\n",
        "        from cogames.policy.simple import SimplePolicy\n",
        "\n",
        "        policy = SimplePolicy(env, device)\n",
        "    elif policy_type == \"lstm\":\n",
        "        from cogames.policy.lstm import LSTMPolicy\n",
        "\n",
        "        policy = LSTMPolicy(env, device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown policy type: {policy_type}\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(policy.network().parameters(), lr=learning_rate)\n",
        "    best_reward = -float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        buffer = RolloutBuffer(device=device, gamma=gamma, gae_lambda=gae_lambda)\n",
        "        obs, _ = env.reset(seed=seed + epoch)\n",
        "        epoch_reward = 0.0\n",
        "        hearts_collected = 0.0\n",
        "        lstm_state = None\n",
        "        steps_collected = 0\n",
        "        last_done_flags = np.zeros(getattr(env, \"num_agents\", 1), dtype=np.float32)\n",
        "\n",
        "        while len(buffer) < batch_size:\n",
        "            actions_np, log_probs, values, next_state = get_action(policy, obs, device, lstm_state)\n",
        "            next_obs, rewards, terminals, truncations, _ = env.step(actions_np)\n",
        "\n",
        "            dones = np.logical_or(terminals, truncations)\n",
        "            buffer.add(obs, actions_np, rewards, dones, log_probs, values)\n",
        "\n",
        "            step_reward = float(np.sum(rewards))\n",
        "            epoch_reward += step_reward\n",
        "            if step_reward > 0:\n",
        "                hearts_collected += step_reward\n",
        "\n",
        "            last_done_flags = np.asarray(dones, dtype=np.float32).copy()\n",
        "\n",
        "            if np.any(dones):\n",
        "                next_obs, _ = env.reset()\n",
        "                lstm_state = None\n",
        "            else:\n",
        "                lstm_state = next_state\n",
        "\n",
        "            obs = next_obs\n",
        "            steps_collected += 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            obs_tensor = torch.from_numpy(obs).float().unsqueeze(0).to(device)\n",
        "            if policy.is_recurrent():\n",
        "                state_arg = None\n",
        "                if lstm_state is not None:\n",
        "                    h, c = lstm_state\n",
        "                    state_arg = {\"lstm_h\": h, \"lstm_c\": c}\n",
        "                _, last_value_tensor = policy.network().forward_eval(obs_tensor, state_arg)\n",
        "            else:\n",
        "                _, last_value_tensor = policy.network().forward_eval(obs_tensor)\n",
        "\n",
        "        last_value = last_value_tensor.squeeze(0)\n",
        "        if last_value.dim() > 1:\n",
        "            last_value = last_value.squeeze(-1)\n",
        "        last_value = last_value.detach().cpu()\n",
        "\n",
        "        last_done_tensor = torch.as_tensor(last_done_flags, dtype=torch.float32)\n",
        "        if last_done_tensor.dim() == 0:\n",
        "            last_done_tensor = last_done_tensor.unsqueeze(0)\n",
        "\n",
        "        last_value = last_value * (1.0 - last_done_tensor)\n",
        "\n",
        "        batch = buffer.build_training_batch(last_value=last_value, last_done=last_done_tensor)\n",
        "\n",
        "        metrics = ppo_update(\n",
        "            policy=policy,\n",
        "            optimizer=optimizer,\n",
        "            batch=batch,\n",
        "            device=device,\n",
        "            clip_coef=clip_coef,\n",
        "            vf_clip_coef=vf_clip_coef,\n",
        "            vf_coef=vf_coef,\n",
        "            ent_coef=ent_coef,\n",
        "            max_grad_norm=max_grad_norm,\n",
        "            ppo_epochs=ppo_epochs,\n",
        "            minibatch_size=minibatch_size,\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}/{num_epochs} - Reward: {epoch_reward:.2f}, \"\n",
        "            f\"Policy loss: {metrics['policy_loss']:.4f}, Value loss: {metrics['value_loss']:.4f}\"\n",
        "        )\n",
        "\n",
        "        if epoch_reward > best_reward:\n",
        "            best_reward = epoch_reward\n",
        "            policy.save_policy_data(save_path)\n",
        "\n",
        "    env.close()\n"
      ],
      "metadata": {
        "id": "JiG5E2GcBnZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Policy"
      ],
      "metadata": {
        "id": "SKgx5Wt9lXyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cogames(\n",
        "    mission_name=\"training_facility.harvest\",\n",
        "    policy_type=\"simple\", # or use lstm\n",
        "    num_epochs=50000,\n",
        "    batch_size=2048,\n",
        "    minibatch_size=512,\n",
        "    ppo_epochs=4,\n",
        "    learning_rate=3e-4,\n",
        "    save_path=\"cogames_policy.pt\",\n",
        "    variants=[\"neutral_faced\"], # Available variants: mined_out, dark_side, super_charged, rough_terrain, solar_flare, desert, forest, city, caves, store_base, extractor_base, both_base, lonely_heart, pack_rat, energized, neutral_faced\n",
        "    device=\"cuda\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "PHXUnV_cz6F2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}