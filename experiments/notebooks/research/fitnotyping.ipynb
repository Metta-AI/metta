{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Mettabook"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "print(\"Setup complete! Auto-reload enabled.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Fetch Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from metta.app_backend.clients.scorecard_client import ScorecardClient\n",
                "\n",
                "client = ScorecardClient()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "result = await client.sql_query(\"\"\"WITH policy_performance AS (\n",
                "    SELECT \n",
                "        e.eval_category,\n",
                "        e.primary_policy_id,\n",
                "        AVG(eam.value) as avg_reward\n",
                "    FROM episodes e\n",
                "    JOIN episode_agent_metrics eam ON e.internal_id = eam.episode_internal_id\n",
                "    join policies p on p.id = e.primary_policy_id\n",
                "    WHERE eam.metric = 'reward'\n",
                "        AND e.eval_category IS NOT NULL\n",
                "        AND p.created_at > now() - '5d'::interval\n",
                "        AND p.url ilike 'wandb://%'\n",
                "    GROUP BY e.eval_category, e.primary_policy_id\n",
                "),\n",
                "ranked_policies AS (\n",
                "    SELECT \n",
                "        eval_category,\n",
                "        primary_policy_id,\n",
                "        avg_reward,\n",
                "        ROW_NUMBER() OVER (PARTITION BY eval_category ORDER BY avg_reward DESC) as rank\n",
                "    FROM policy_performance\n",
                "),\n",
                "ranked_training_runs AS (\n",
                "SELECT \n",
                "    rp.eval_category,\n",
                "    rp.primary_policy_id,\n",
                "    p.name as policy_name,\n",
                "    rp.avg_reward,\n",
                "    rp.rank,\n",
                "    tr.id as training_run_id,\n",
                "    tr.name as training_run_name,\n",
                "    tr.status as training_run_status,\n",
                "    tr.created_at as training_run_created_at\n",
                "FROM ranked_policies rp\n",
                "JOIN policies p ON rp.primary_policy_id = p.id\n",
                "JOIN epochs ep ON p.epoch_id = ep.id\n",
                "JOIN training_runs tr ON ep.run_id = tr.id\n",
                "WHERE rp.rank <= 10\n",
                "ORDER BY rp.eval_category, rp.rank\n",
                ")\n",
                "select distinct(training_run_name) from ranked_training_runs;\n",
                "\"\"\")\n",
                "training_run_names = [a[0] for a in result.rows]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(training_run_names[:2])\n",
                "print(', '.join(training_run_names))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def get_all_results(client, training_run_names: list[str]) -> list:\n",
                "      all_results = []\n",
                "      page = 1\n",
                "      page_size = 1000\n",
                "\n",
                "      while True:\n",
                "          print(f\"Fetching page {page}\")\n",
                "          result = await client.sql_query(f\"\"\"\n",
                "              SELECT \n",
                "                  p.name as policy_name,\n",
                "                  e.eval_name,\n",
                "                  AVG(eam.value) as avg_reward\n",
                "              FROM training_runs tr\n",
                "              JOIN epochs ep ON ep.run_id = tr.id\n",
                "              JOIN policies p ON p.epoch_id = ep.id\n",
                "              JOIN episodes e ON e.primary_policy_id = p.id\n",
                "              JOIN episode_agent_metrics eam ON eam.episode_internal_id = e.internal_id\n",
                "              WHERE tr.name IN ({', '.join((\"'\" + name + \"'\" for name in training_run_names))})\n",
                "                  AND eam.metric = 'reward'\n",
                "              GROUP BY p.id, p.name, e.eval_name\n",
                "              ORDER BY p.id, e.eval_name\n",
                "              LIMIT {page_size} OFFSET {(page - 1) * page_size}\n",
                "          \"\"\")\n",
                "\n",
                "          if not result.rows:\n",
                "              break\n",
                "\n",
                "          all_results.extend(result.rows)\n",
                "\n",
                "          if len(result.rows) < page_size:\n",
                "              break\n",
                "\n",
                "          page += 1\n",
                "\n",
                "      return all_results\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = await get_all_results(client, training_run_names)\n",
                "print(len(results))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "def create_scorecard_dataframe(data: list[list]) -> pd.DataFrame:\n",
                "    df_data = {}\n",
                "    for policy_name, eval_name, score in data:\n",
                "        if policy_name not in df_data:\n",
                "            df_data[policy_name] = {}\n",
                "        df_data[policy_name][eval_name] = score\n",
                "\n",
                "    return pd.DataFrame(df_data).fillna(float('nan'))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = create_scorecard_dataframe(results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
