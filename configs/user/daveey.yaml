# @package __global__

defaults:
  - /sim/all@sim_job.simulation_suite
  - _self_

trainer:
  env: /env/mettagrid/terrain_from_numpy
  evaluate_interval: 200
  # optimizer:
  #   type: muon
  # env_overrides:
  #   sampling: 0.7
  #   game:
  #     num_agents: 36
  #     max_steps: 1000

# policy_uri: puffer:///tmp/puffer_metta.pt
policy_uri: ${trained_policy_uri}
# policy_uri: puffer://./train_dir/puffer/puffer_metta.pt

npc_policy_uri: ${policy_uri}
# npc_policy_uri: ${trained_policy_uri}

eval_db_uri: ${run_dir}/eval_stats

analyzer:
  policy_uri: ${..policy_uri}
  view_type: latest
  analysis:
    metrics:
      - metric: episode_reward
      - metric: "heart.get"

replay_job:
  sim:
    env: /env/mettagrid/laser_tag
    env_overrides:
      game:
        max_steps: 300
        group_reward_pct: 0.5
      sampling: 1

sim_job:
  # policy_agents_pct: 1

  # env: /env/mettagrid/reward_dr
  # env_overrides:
  #   # sampling: 0.7
  #   game:
  #     num_agents: 16
  #     max_steps: 1000
  #     map_builder:
  #       room:
  #         agents: 4
  #       num_rooms: 4


<<<<<<< HEAD
run_id: 21
=======
run_id: 22
>>>>>>> b8cb4627 (cp)
run: ${oc.env:USER}.local.${run_id}
trained_policy_uri: ${run_dir}/checkpoints

sweep_params: "sweep/fast"
sweep_name: "${oc.env:USER}.local.sweep.${run_id}"
seed: null
