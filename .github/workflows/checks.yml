name: "Python"
concurrency:
  group: ${{ github.workflow }}-${{ github.event_name == 'merge_group' && github.event.merge_group.head_ref || github.ref }}
  cancel-in-progress: true
on:
  pull_request:
    types: [opened, synchronize, ready_for_review]
  push:
    branches: [main]
  merge_group:
    types: [checks_requested]
    branches: [main]
  workflow_dispatch:
    inputs:
      run_lint:
        description: "Run lint checks"
        type: boolean
        default: true
      run_test:
        description: "Run tests"
        type: boolean
        default: true
      run_benchmark:
        description: "Run benchmarks (requires tests)"
        type: boolean
        default: true

# Set default permissions
permissions:
  checks: write
  pull-requests: write

# We should switch to using uv projects rather than requirements.txt
# but for now, this installs in the global venv
env:
  HYDRA_FULL_ERROR: 1
  VENV_PATH: .venv

jobs:
  # check if CI should run based on Graphite's stack position
  graphite-ci-optimizer:
    name: "Graphite CI Optimizer"
    if: |
      github.event.pull_request.draft == false ||
      github.event_name == 'push' ||
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'merge_group'
    runs-on: ubuntu-latest
    outputs:
      should_skip: ${{ steps.graphite_ci.outputs.skip }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Graphite CI Optimizer
        id: graphite_ci
        uses: withgraphite/graphite-ci-action@main
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          graphite_token: ${{ secrets.GRAPHITE_TOKEN }}

  # check if any source code files have changed
  setup-checks:
    name: "Set up for source code checks"
    needs: graphite-ci-optimizer
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (github.event.pull_request.draft == false ||
       github.event_name == 'push' ||
       github.event_name == 'workflow_dispatch' ||
       github.event_name == 'merge_group')
    runs-on: ubuntu-latest
    outputs:
      has_relevant_changes: ${{ steps.check_py_files.outputs.has_relevant_changes }}
      run_lint: ${{ steps.determine_tasks.outputs.run_lint }}
      run_test: ${{ steps.determine_tasks.outputs.run_test }}
      run_benchmark: ${{ steps.determine_tasks.outputs.run_benchmark }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check for file changes
        id: check_py_files
        uses: ./.github/actions/file-changes
        with:
          patterns: "**/*.py,**/*.pyx,**/*.pxd,**/*.cpp,**/*.hpp"
          specific_files: "requirements.txt,requirements_pinned.txt,setup.py"
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Determine which tasks to run
        id: determine_tasks
        run: |
          # Default behavior based on event type and file changes
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            # Use workflow dispatch inputs for manual runs
            RUN_LINT="${{ github.event.inputs.run_lint }}"
            RUN_TEST="${{ github.event.inputs.run_test }}"
            RUN_BENCHMARK="${{ github.event.inputs.run_benchmark }}"
          elif [[ "${{ github.event_name }}" == "pull_request" && "${{ steps.check_py_files.outputs.has_relevant_changes }}" == "false" ]]; then
            # Skip everything for PRs with no relevant changes
            RUN_LINT="false"
            RUN_TEST="false"
            RUN_BENCHMARK="false"
            echo "::notice title=Skipping Tasks::Skipping all tasks because no relevant files have changed"
          else
            # Default to running everything for other events or when changes exist
            RUN_LINT="true"
            RUN_TEST="true"
            RUN_BENCHMARK="true"
          fi

          # Output the decisions
          echo "run_lint=${RUN_LINT}" >> $GITHUB_OUTPUT
          echo "run_test=${RUN_TEST}" >> $GITHUB_OUTPUT
          echo "run_benchmark=${RUN_BENCHMARK}" >> $GITHUB_OUTPUT

          echo "run lint? ${RUN_LINT}"
          echo "run test? ${RUN_TEST}"
          echo "run benchmark? ${RUN_BENCHMARK}"

  lint:
    name: "Lint"
    needs: [graphite-ci-optimizer, setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_lint == 'true')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python and uv
        uses: ./.github/actions/setup-python-uv

      - name: Run Ruff linter
        run: |
          ruff format --check .

      - name: Run Ruff formatting
        run: |
          ruff check --exit-non-zero-on-fix .

      - name: Install C++ linter
        run: |
          sudo apt-get install -y clang-format

      - name: Run clang-format check
        run: |
          echo "Checking C++ formatting..."
          find metta mettagrid/mettagrid mettagrid/tests mettagrid/benchmarks -type f \( -name "*.cpp" -o -name "*.h" -o -name "*.hpp" \) \
            -not -path "*/build/*" -not -path "*/venv/*" \
            -exec clang-format --dry-run --Werror {} +

      - name: Run cpplint
        run: bash mettagrid/tests/run_cpplint.sh

  test:
    name: "Tests"
    needs: [graphite-ci-optimizer, setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_test == 'true')
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python and uv
        uses: ./.github/actions/setup-python-uv

      - name: Run Pytest on core tests
        run: |
          uv run --active pytest --maxfail=1 --disable-warnings -q

      - name: Run Pytest on MettaGrid tests
        working-directory: mettagrid
        run: |
          uv run --active pytest --maxfail=1 --disable-warnings -q

      - name: Run MettaGrid C++ tests
        run: |
          source .venv/bin/activate
          cd mettagrid
          cmake --preset debug
          cmake --build build-debug
          ctest --test-dir build-debug -L test

  smoke-test:
    name: "Smoke Tests"
    # temporarily disabled
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false')
    needs: [graphite-ci-optimizer, setup-checks]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    env:
      CHECKPOINT_PATH: ./train_dir/github_test/checkpoints/
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python and uv
        uses: ./.github/actions/setup-python-uv

      - name: Training smoke test
        id: train
        env:
          WANDB_API_KEY: set_but_not_used
          AWS_ACCESS_KEY_ID: set_but_not_used
          AWS_SECRET_ACCESS_KEY: set_but_not_used
        run: |
          mkdir -p train_dir
          source .github/scripts/benchmark.sh
          benchmark "train" "uv run --active python -m tools.train +hardware=github wandb=off"
          ls -la $CHECKPOINT_PATH || echo "Warning: Checkpoint directory not created"

      - name: Save training benchmark
        uses: ./.github/actions/save-benchmarks
        with:
          name: train_smoke_test
          metrics: '{"duration": ${{ steps.train.outputs.duration }}, "memory_usage": ${{ steps.train.outputs.memory_usage }}}'
          filename: smoke_test_train_benchmark_results.json

      - name: Upload training benchmark file
        uses: actions/upload-artifact@v4
        with:
          name: train-benchmark-results
          path: |
            smoke_test_train_benchmark_results.json
          retention-days: 1
          if-no-files-found: warn

      - name: Upload training output
        uses: actions/upload-artifact@v4
        with:
          name: train-output
          path: train_dir/
          retention-days: 1
          if-no-files-found: error

      - name: Download training output
        uses: actions/download-artifact@v4
        with:
          name: train-output
          path: train_dir/

      - name: Verify training artifacts
        run: |
          ls -la train_dir/
          ls -la $CHECKPOINT_PATH || echo "Checkpoint directory not found!"

      - name: Replay smoke test
        id: replay
        env:
          WANDB_API_KEY: set_but_not_used
          AWS_ACCESS_KEY_ID: set_but_not_used
          AWS_SECRET_ACCESS_KEY: set_but_not_used
        run: |
          source .github/scripts/benchmark.sh
          benchmark "replay" "uv run --active python -m tools.replay +hardware=github wandb=off"

      - name: Save replay benchmark
        uses: ./.github/actions/save-benchmarks
        with:
          name: replay_smoke_test
          metrics: '{"duration": ${{ steps.replay.outputs.duration }}, "memory_usage": ${{ steps.replay.outputs.memory_usage }}}'
          filename: smoke_test_replay_benchmark_results.json

      - name: Upload replay benchmark file
        uses: actions/upload-artifact@v4
        with:
          name: replay-benchmark-results
          path: |
            smoke_test_replay_benchmark_results.json
          retention-days: 1
          if-no-files-found: warn

      - name: Run evals smoke test
        id: eval_smoke_test
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
          AWS_ACCESS_KEY_ID: set_but_not_used
          AWS_SECRET_ACCESS_KEY: set_but_not_used
        # We retry evals a few times because they're not deterministic (despite our best efforts)
        # Our smoke tests should pass at least 90% of the time if they're working, so this should have
        # a spurious failure rate of less than 0.01%. We assume that spurious successes should be
        # extremely rare.
        run: |
          source .github/scripts/benchmark.sh
          for i in {1..4}; do
            if benchmark "evals_try$i" "./run_evals.sh smoke_test"; then
              exit 0
            fi
          done
          exit 1

      - name: Save evals benchmark
        id: save_evals_benchmark
        uses: ./.github/actions/save-benchmarks
        with:
          name: evals_smoke_test
          metrics: '{"duration": ${{ steps.eval_smoke_test.outputs.duration }}, "memory_usage": ${{ steps.eval_smoke_test.outputs.memory_usage }}}'
          filename: smoke_test_evals_benchmark_results.json

      - name: Upload evals benchmark file
        uses: actions/upload-artifact@v4
        with:
          name: evals-benchmark-results
          path: |
            smoke_test_evals_benchmark_results.json
          retention-days: 1
          if-no-files-found: warn

  benchmark:
    name: "Python Benchmarks"
    needs: [graphite-ci-optimizer, setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_benchmark == 'true')
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python and uv
        uses: ./.github/actions/setup-python-uv

      # - name: Download benchmark results from smoke-test train-benchmark-results job
      #   uses: actions/download-artifact@v4
      #   with:
      #     name: train-benchmark-results
      #     path: ./
      #     if-no-files-found: ignore

      # - name: Download benchmark results from smoke-test replay-benchmark-results job
      #   uses: actions/download-artifact@v4
      #   with:
      #     name: replay-benchmark-results
      #     path: ./
      #     if-no-files-found: ignore

      - name: Run Main Python benchmarks
        run: |
          uv run --active pytest --benchmark-only --benchmark-json=main_benchmark_results.json

      - name: Run Mettagrid Python benchmarks
        working-directory: mettagrid
        run: |
          uv run --active pytest --benchmark-only --benchmark-json=../mettagrid_benchmark_results.json

      - name: Combine benchmark results
        run: |
          uv run --active python - <<EOF

          # Python Script to combine benchmark data
          import json
          import os

          # Function to safely load JSON, handling empty files
          def safe_load_json(file_path):
              try:
                  if os.path.exists(file_path):
                      with open(file_path, 'r') as f:
                          content = f.read().strip()
                          if content:  # Check if file has content
                              return json.loads(content)
                  return {}  # Return empty dict for non-existent or empty files
              except json.JSONDecodeError:
                  print(f"Warning: {file_path} contains invalid JSON or is empty. Using empty dict instead.")
                  return {}

          # List of benchmark files to combine
          benchmark_files = [
              'main_benchmark_results.json',
              'mettagrid_benchmark_results.json',
              # 'smoke_test_train_benchmark_results.json',
              # 'smoke_test_replay_benchmark_results.json'
          ]

          # Initialize combined results structure
          combined_results = {
              "machine_info": {},
              "commit_info": {},
              "benchmarks": []
          }

          # Load and combine all benchmark files
          valid_files_found = False
          for file_path in benchmark_files:
              results = safe_load_json(file_path)
              if not results:
                  print(f"Skipping empty or invalid file: {file_path}")
                  continue

              valid_files_found = True

              # Add benchmarks to the combined list
              if "benchmarks" in results and isinstance(results["benchmarks"], list):
                  combined_results["benchmarks"].extend(results["benchmarks"])

              # Use the first valid file's machine_info and commit_info if not already set
              if "machine_info" in results and not combined_results["machine_info"]:
                  combined_results["machine_info"] = results["machine_info"]

              if "commit_info" in results and not combined_results["commit_info"]:
                  combined_results["commit_info"] = results["commit_info"]

          # If no valid files were found, use an empty structure
          if not valid_files_found:
              print("No valid benchmark files found. Creating empty combined results.")
              combined_results = {}

          # Write combined results
          with open('combined_benchmark_results.json', 'w') as f:
              json.dump(combined_results, f, indent=2)

          print("Successfully combined benchmark results.")

          # Extract agent_steps_per_second and write Bencher Metric Format file
          AGENT_BENCH = "benchmarks/test_mettagrid_env_benchmark.py::test_step_performance_no_reset"
          agent_data = safe_load_json('mettagrid_benchmark_results.json')
          agent_steps = None
          for bench in agent_data.get("benchmarks", []):
              if bench.get("fullname") == AGENT_BENCH:
                  agent_steps = bench.get("extra_info", {}).get("agent_steps_per_second")
                  break

          bmf_file = 'agent_steps.bmf.json'
          if agent_steps is not None:
              bmf = {AGENT_BENCH: {"agent_steps_per_second": {"value": agent_steps}}}
              with open(bmf_file, 'w') as f:
                  json.dump(bmf, f, indent=2)
              print(f"Wrote agent steps metric to {bmf_file}")
          else:
              print("agent_steps_per_second not found; creating empty BMF file")
              with open(bmf_file, 'w') as f:
                  json.dump({}, f)

          EOF

      - name: Install Bencher CLI
        uses: bencherdev/bencher@main

      # Main Branch Upload section
      # note that "threshold-max-sample-size" has a minimum value of 2 - the new data and the reference
      - name: Upload to Bencher (Main Branch Baseline)
        if: github.ref == 'refs/heads/main'
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          bencher run \
            --project mettagrid-sv3f5i2k \
            --token "$BENCHER_API_TOKEN" \
            --branch main \
            --threshold-measure latency \
            --threshold-test percentage \
            --threshold-max-sample-size 2 \
            --threshold-upper-boundary 0.20 \
            --thresholds-reset \
            --testbed ubuntu-latest \
            --adapter python_pytest \
            --github-actions "$GITHUB_TOKEN" \
            --file combined_benchmark_results.json > /dev/null

      - name: Upload Agent Steps Per Second (Main Branch)
        if: github.ref == 'refs/heads/main'
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          bencher run \
            --project mettagrid-sv3f5i2k \
            --token "$BENCHER_API_TOKEN" \
            --branch main \
            --threshold-measure agent_steps_per_second \
            --threshold-test percentage \
            --threshold-max-sample-size 2 \
            --threshold-lower-boundary 0.20 \
            --threshold-upper-boundary _ \
            --thresholds-reset \
            --testbed ubuntu-latest \
            --adapter json \
            --github-actions "$GITHUB_TOKEN" \
            --file agent_steps.bmf.json > /dev/null

      - name: Install jq
        run: |
          sudo apt-get install -y jq

      - name: Check if main branch has benchmark data
        if: github.event_name == 'pull_request' && !github.event.pull_request.head.repo.fork
        id: check-main-benchmark
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
        run: |
          # Run the command and capture output in a variable
          bencher_result=$(bencher branch view mettagrid-sv3f5i2k main --token "$BENCHER_API_TOKEN")

          # Print the captured output
          echo "$bencher_result"

          # Save the output to a file for further processing
          echo "$bencher_result" > /tmp/branch_output.json

          # Continue with checking if branch exists and has data
          if [ $? -eq 0 ]; then
            echo "Branch exists, checking if it has benchmark data..."

            # Check if branch output contains a head key
            if jq -e '.head' /tmp/branch_output.json > /dev/null; then
              echo "main_benchmark_exists=true" >> $GITHUB_OUTPUT
              echo "Main branch has benchmark data. Will proceed with PR performance comparison."
            else
              echo "main_benchmark_exists=false" >> $GITHUB_OUTPUT
              echo "Warning: Main branch exists but does not have benchmark data yet. Will skip PR performance comparison."
            fi
          else
            echo "main_benchmark_exists=false" >> $GITHUB_OUTPUT
            echo "Warning: Main branch does not exist or cannot be accessed. Will skip PR performance comparison."
          fi

      # First PR Performance run - Check for improvements (will NOT fail on alerts)
      - name: Check for Performance Improvements
        if: |
          (github.event_name == 'pull_request') &&
          (!github.event.pull_request.head.repo.fork) &&
          (steps.check-main-benchmark.outputs.main_benchmark_exists == 'true')
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          bencher run \
            --project mettagrid-sv3f5i2k \
            --token "$BENCHER_API_TOKEN" \
            --branch "$GITHUB_HEAD_REF" \
            --start-point "main" \
            --start-point-reset \
            --thresholds-reset \
            --threshold-measure latency \
            --threshold-test percentage \
            --threshold-max-sample-size 2 \
            --threshold-lower-boundary 0.10 \
            --testbed ubuntu-latest \
            --adapter python_pytest \
            --github-actions "$GITHUB_TOKEN" \
            --file combined_benchmark_results.json > /dev/null

      - name: Upload Agent Steps Per Second (PR Improvement)
        if: |
          (github.event_name == 'pull_request') &&
          (!github.event.pull_request.head.repo.fork) &&
          (steps.check-main-benchmark.outputs.main_benchmark_exists == 'true')
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          bencher run \
            --project mettagrid-sv3f5i2k \
            --token "$BENCHER_API_TOKEN" \
            --branch "$GITHUB_HEAD_REF" \
            --start-point "main" \
            --start-point-reset \
            --thresholds-reset \
            --threshold-measure agent_steps_per_second \
            --threshold-test percentage \
            --threshold-max-sample-size 2 \
            --threshold-upper-boundary 0.10 \
            --threshold-lower-boundary _ \
            --testbed ubuntu-latest \
            --adapter json \
            --github-actions "$GITHUB_TOKEN" \
            --file agent_steps.bmf.json > /dev/null

      # Second PR Performance run - Check for regressions (WILL fail on alerts)
      - name: Check for Performance Regressions
        if: |
          (github.event_name == 'pull_request') &&
          (!github.event.pull_request.head.repo.fork) &&
          (steps.check-main-benchmark.outputs.main_benchmark_exists == 'true')
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          bencher run \
            --project mettagrid-sv3f5i2k \
            --token "$BENCHER_API_TOKEN" \
            --branch "$GITHUB_HEAD_REF" \
            --start-point "main" \
            --start-point-reset \
            --start-point-clone-thresholds \
            --err \
            --testbed ubuntu-latest \
            --adapter python_pytest \
            --github-actions "$GITHUB_TOKEN" \
            --file combined_benchmark_results.json

      - name: Upload Agent Steps Per Second (PR Regression)
        if: |
          (github.event_name == 'pull_request') &&
          (!github.event.pull_request.head.repo.fork) &&
          (steps.check-main-benchmark.outputs.main_benchmark_exists == 'true')
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          bencher run \
            --project mettagrid-sv3f5i2k \
            --token "$BENCHER_API_TOKEN" \
            --branch "$GITHUB_HEAD_REF" \
            --start-point "main" \
            --start-point-reset \
            --thresholds-reset \
            --threshold-measure agent_steps_per_second \
            --threshold-test percentage \
            --threshold-max-sample-size 2 \
            --threshold-lower-boundary 0.20 \
            --threshold-upper-boundary _ \
            --err \
            --testbed ubuntu-latest \
            --adapter json \
            --github-actions "$GITHUB_TOKEN" \
            --file agent_steps.bmf.json

  cpp-benchmark:
    name: "C++ Benchmarks"
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [graphite-ci-optimizer, setup-checks]
    if: |
      (needs.graphite-ci-optimizer.outputs.should_skip == 'false') &&
      (needs.setup-checks.outputs.run_benchmark == 'true')
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python and uv
        uses: ./.github/actions/setup-python-uv

      - name: CMake
        run: |
          source .venv/bin/activate
          cd mettagrid
          cmake --preset release
          cmake --build build-release

      - name: Run benchmarks with JSON output
        working-directory: mettagrid
        run: |
          mkdir -p benchmark_output
          echo "Running benchmarks with JSON output..."
          for f in build-release/benchmarks/*_benchmark; do
            echo "Running $f"
            "$f" --benchmark_format=json >benchmark_output/$(basename $f).json
          done

          echo "Benchmark output files:"
          ls -la benchmark_output/

      - name: Install Bencher CLI
        uses: bencherdev/bencher@main

      # Main Branch Upload section
      # note that "threshold-max-sample-size" has a minimum value of 2 - the new data and the reference
      - name: Upload to Bencher (Main Branch Baseline)
        if: github.ref == 'refs/heads/main'
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          cd mettagrid/benchmark_output
          ls -al
          if ls *.json 1> /dev/null 2>&1; then
            for file in *.json; do
              bencher run \
                --project mettagrid-sv3f5i2k \
                --token "$BENCHER_API_TOKEN" \
                --branch main \
                --threshold-measure latency \
                --threshold-test percentage \
                --threshold-max-sample-size 2 \
                --threshold-upper-boundary 0.20 \
                --thresholds-reset \
                --testbed ubuntu-latest \
                --adapter cpp_google \
                --github-actions "$GITHUB_TOKEN" \
                --file "$file" > /dev/null
            done
          else
            echo "NO TEST RESULTS FOUND"
          fi

      - name: Check if main branch has benchmark data
        if: github.event_name == 'pull_request' && !github.event.pull_request.head.repo.fork
        id: check-main-benchmark
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
        run: |
          # Run the command and capture output in a variable
          bencher_result=$(bencher branch view mettagrid-sv3f5i2k main --token "$BENCHER_API_TOKEN")

          # Print the captured output
          echo "$bencher_result"

          # Save the output to a file for further processing
          echo "$bencher_result" > /tmp/branch_output.json

          # Continue with checking if branch exists and has data
          if [ $? -eq 0 ]; then
            echo "Branch exists, checking if it has benchmark data..."
            # Check if branch output contains a head key
            if jq -e '.head' /tmp/branch_output.json > /dev/null; then
              echo "main_benchmark_exists=true" >> $GITHUB_OUTPUT
              echo "Main branch has benchmark data. Will proceed with PR performance comparison."
            else
              echo "main_benchmark_exists=false" >> $GITHUB_OUTPUT
              echo "Warning: Main branch exists but does not have benchmark data yet. Will skip PR performance comparison."
            fi
          else
            echo "main_benchmark_exists=false" >> $GITHUB_OUTPUT
            echo "Warning: Main branch does not exist or cannot be accessed. Will skip PR performance comparison."
          fi

      # First PR Performance run - Check for improvements (will NOT fail on alerts)
      - name: Check for Performance Improvements
        if: |
          github.event_name == 'pull_request' &&
          !github.event.pull_request.head.repo.fork &&
          steps.check-main-benchmark.outputs.main_benchmark_exists == 'true'
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          cd mettagrid/benchmark_output
          ls -al
          if ls *.json 1> /dev/null 2>&1; then
            for file in *.json; do
              bencher run \
                --project mettagrid-sv3f5i2k \
                --token "$BENCHER_API_TOKEN" \
                --branch "$GITHUB_HEAD_REF" \
                --start-point "main" \
                --start-point-reset \
                --threshold-measure latency \
                --threshold-test percentage \
                --threshold-max-sample-size 2 \
                --threshold-lower-boundary 0.10 \
                --thresholds-reset \
                --testbed ubuntu-latest \
                --adapter cpp_google \
                --github-actions "$GITHUB_TOKEN" \
                --file "$file" > /dev/null
            done
          else
            echo "NO TEST RESULTS FOUND"
          fi

      # Second PR Performance run - Check for regressions (WILL fail on alerts)
      - name: Check for Performance Regressions
        if: |
          github.event_name == 'pull_request' && 
          !github.event.pull_request.head.repo.fork && 
          steps.check-main-benchmark.outputs.main_benchmark_exists == 'true'
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          cd mettagrid/benchmark_output
          ls -al
          if ls *.json 1> /dev/null 2>&1; then
            for file in *.json; do
              bencher run \
                --project mettagrid-sv3f5i2k \
                --token "$BENCHER_API_TOKEN" \
                --branch "$GITHUB_HEAD_REF" \
                --start-point "main" \
                --start-point-reset \
                --start-point-clone-thresholds \
                --testbed ubuntu-latest \
                --err \
                --adapter cpp_google \
                --github-actions "$GITHUB_TOKEN" \
                --file "$file" 
            done
          else
            echo "NO TEST RESULTS FOUND"
          fi
