# @package __global__

defaults:
  - override /agent: simple
  - override /eval: eval


trainer:
  env: /env/mettagrid/reward_dr
  evaluate_interval: 200
  env_overrides:
    sampling: 0.7
    game:
      num_agents: 36
      max_steps: 1000

# policy: wandb://run/b.daveey.train.maze.sm.dr.warm.0
# baselines: wandb://run/b.daveey.train.maze.sm.11x11.0

# policy_uri: wandb://run/b.daveey.t.1.bl
# policy_uri: wandb://run/b.daveey.t.16.dr0
# policy_uri: wandb://run/b.daveey.t.64.dr90.1
policy_uri: wandb://run/b.daveey.t.64.dr90.e3b

npc_policy_uri: ${policy_uri}
# npc_policy_uri: wandb://run/b.daveey.t.16.dr0
# policy_uri: ${trained_policy_uri}
# npc_policy_uri: ${trained_policy_uri}
# eval_db_uri: wandb://artifacts/daveey_eval_testing

analyzer:
  eval_stats_uri: ${run_dir}/eval_stats
  analysis:
    metrics:
      - metric: episode_reward
      - metric: "*.get"

eval:
  num_envs: 10
  num_episodes: 10
  max_time_s: 600
  # policy_agents_pct: 0.5

  policy_uri: ${..policy_uri}
  npc_policy_uri: ${..npc_policy_uri}
  # eval_db_uri: ${..eval_db_uri} #file://daphne/sweep_stats
  env: /env/mettagrid/reward_dr
  # env_overrides:
  #   sampling: 0.7
  #   game:
  #     # num_agents: 360
  #     max_steps: 1000

wandb:
  checkpoint_interval: 1

run_id: 13
run: ${oc.env:USER}.local.${run_id}
trained_policy_uri: ${run_dir}/checkpoints

sweep_params: "sweep/fast"
sweep_name: "${oc.env:USER}.local.sweep.${run_id}"
seed: null
