name: 'Run Unified Bencher Analysis'
description: 'Run Bencher analysis with measure-specific thresholds for bidirectional alerts'
inputs:
  bencher_token:
    description: 'Bencher API token'
    required: true
  github_token:
    description: 'GitHub token'
    required: true
  benchmark_file:
    description: 'Unified benchmark results file (BMF format)'
    required: true
  project_id:
    description: 'Bencher project ID'
    default: 'mettagrid-sv3f5i2k'

runs:
  using: 'composite'
  steps:
    - name: Install Bencher CLI
      uses: bencherdev/bencher@main

    - name: Install jq
      shell: bash
      run: sudo apt-get install -y jq

    - name: Check main branch status
      id: check-main
      shell: bash
      env:
        BENCHER_API_TOKEN: ${{ inputs.bencher_token }}
      run: |
        if bencher branch view ${{ inputs.project_id }} main --token "$BENCHER_API_TOKEN" > /tmp/branch_output.json 2>/dev/null; then
          if jq -e '.head' /tmp/branch_output.json > /dev/null; then
            echo "main_exists=true" >> $GITHUB_OUTPUT
          else
            echo "main_exists=false" >> $GITHUB_OUTPUT
          fi
        else
          echo "main_exists=false" >> $GITHUB_OUTPUT
        fi

    # Main branch: Upload baseline and configure bidirectional thresholds
    - name: Upload baseline with bidirectional thresholds (main branch)
      if: github.ref == 'refs/heads/main'
      shell: bash
      env:
        BENCHER_API_TOKEN: ${{ inputs.bencher_token }}
        GITHUB_TOKEN: ${{ inputs.github_token }}
      run: |
        echo "üìä Uploading baseline data with bidirectional thresholds..."
        
        if [ -f "${{ inputs.benchmark_file }}" ] && [ -s "${{ inputs.benchmark_file }}" ]; then
          # Define your metrics and their threshold types
          # Format: "metric_name:threshold_type" where threshold_type is "upper", "lower", or "both"
          declare -A METRIC_CONFIGS=(
            ["latency"]="upper"                    # Lower is better - only alert on increases
            ["cpu_time"]="upper"                   # Lower is better - only alert on increases
            ["agent_steps_per_second"]="both"      # Higher is better KPI - alert on any 20% change
            ["env_steps_per_second"]="both"        # Higher is better KPI - alert on any 20% change
            ["resets_per_second"]="both"           # Higher is better KPI - alert on any 20% change
            ["iterations"]="upper"                 # Lower is better - only alert on increases
          )
          
          echo "üéØ Setting up measure-specific thresholds..."
          
          # Upload data and set thresholds for each known metric
          for metric in "${!METRIC_CONFIGS[@]}"; do
            config="${METRIC_CONFIGS[$metric]}"
            echo "Configuring $metric with $config boundaries..."
            
            case $config in
              "upper")
                # Only upper boundary - alerts on increases (regressions for latency-type metrics)
                bencher run \
                  --project ${{ inputs.project_id }} \
                  --token "$BENCHER_API_TOKEN" \
                  --branch main \
                  --threshold-measure "$metric" \
                  --threshold-test percentage \
                  --threshold-max-sample-size 2 \
                  --threshold-upper-boundary 0.20 \
                  --testbed ubuntu-latest \
                  --adapter json \
                  --github-actions "$GITHUB_TOKEN" \
                  --file "${{ inputs.benchmark_file }}" 2>/dev/null || echo "No $metric data found"
                ;;
              
              "lower")
                # Only lower boundary - alerts on decreases (regressions for throughput-type metrics)
                bencher run \
                  --project ${{ inputs.project_id }} \
                  --token "$BENCHER_API_TOKEN" \
                  --branch main \
                  --threshold-measure "$metric" \
                  --threshold-test percentage \
                  --threshold-max-sample-size 2 \
                  --threshold-lower-boundary 0.20 \
                  --threshold-upper-boundary _ \
                  --testbed ubuntu-latest \
                  --adapter json \
                  --github-actions "$GITHUB_TOKEN" \
                  --file "${{ inputs.benchmark_file }}" 2>/dev/null || echo "No $metric data found"
                ;;
              
              "both")
                # Both boundaries - alerts on ANY significant change (bidirectional KPI monitoring)
                bencher run \
                  --project ${{ inputs.project_id }} \
                  --token "$BENCHER_API_TOKEN" \
                  --branch main \
                  --threshold-measure "$metric" \
                  --threshold-test percentage \
                  --threshold-max-sample-size 2 \
                  --threshold-lower-boundary 0.20 \
                  --threshold-upper-boundary 0.20 \
                  --testbed ubuntu-latest \
                  --adapter json \
                  --github-actions "$GITHUB_TOKEN" \
                  --file "${{ inputs.benchmark_file }}" 2>/dev/null || echo "No $metric data found"
                ;;
            esac
          done
          
          # Handle any unknown metrics with default upper threshold
          echo "Setting default thresholds for any unknown metrics..."
          measures=$(jq -r 'to_entries[] | .value | keys[]' "${{ inputs.benchmark_file }}" | sort -u)
          
          for measure in $measures; do
            # Skip if we already configured this metric
            if [[ ! " ${!METRIC_CONFIGS[@]} " =~ " ${measure} " ]]; then
              echo "Setting default upper threshold for unknown metric: $measure"
              bencher run \
                --project ${{ inputs.project_id }} \
                --token "$BENCHER_API_TOKEN" \
                --branch main \
                --threshold-measure "$measure" \
                --threshold-test percentage \
                --threshold-max-sample-size 2 \
                --threshold-upper-boundary 0.20 \
                --testbed ubuntu-latest \
                --adapter json \
                --github-actions "$GITHUB_TOKEN" \
                --file "${{ inputs.benchmark_file }}" 2>/dev/null || echo "No $measure data found"
            fi
          done
          
          echo "‚úÖ All thresholds configured"
        else
          echo "‚ö†Ô∏è  No benchmark file found: ${{ inputs.benchmark_file }}"
        fi

    # PR: Check for ANY significant changes (improvements OR regressions)
    - name: Check for significant performance changes
      if: |
        github.event_name == 'pull_request' &&
        !github.event.pull_request.head.repo.fork &&
        steps.check-main.outputs.main_exists == 'true'
      shell: bash
      env:
        BENCHER_API_TOKEN: ${{ inputs.bencher_token }}
        GITHUB_TOKEN: ${{ inputs.github_token }}
      run: |
        echo "üö® Checking for significant performance changes (¬±20% = alert + test failure)..."
        
        if [ -f "${{ inputs.benchmark_file }}" ]; then
          echo "Running unified performance check with pre-configured thresholds..."
          
          # Single run that checks all metrics using their pre-configured bidirectional thresholds
          # This will fail if ANY metric changes by 20% in either direction
          bencher run \
            --project ${{ inputs.project_id }} \
            --token "$BENCHER_API_TOKEN" \
            --branch "$GITHUB_HEAD_REF" \
            --start-point main \
            --start-point-reset \
            --start-point-clone-thresholds \
            --err \
            --testbed ubuntu-latest \
            --adapter json \
            --github-actions "$GITHUB_TOKEN" \
            --file "${{ inputs.benchmark_file }}"
            
          echo "‚úÖ No significant performance changes detected"
        else
          echo "‚ö†Ô∏è  No benchmark file found: ${{ inputs.benchmark_file }}"
          exit 1
        fi