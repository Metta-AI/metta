{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mettabook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[34mComponent | Installed  | Connected As              | Expected             | Status\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[34m----------------------------------------------------------------------------------\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[34mcore     | Yes        | -                         | -                    |\u001b[0m\u001b[0m\u001b[0m\u001b[32mOK\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[34msystem   | No         | -                         | -                    |\u001b[0m\u001b[0m\u001b[0m\u001b[31mNOT INSTALLED\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[34maws      | Yes        | 767406518141              | 751442549699         |\u001b[0m\u001b[0m\u001b[0m\u001b[33mWRONG ACCOUNT\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[34mwandb    | Yes        | metta-research            | metta-research       |\u001b[0m\u001b[0m\u001b[0m\u001b[32mOK\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[34m----------------------------------------------------------------------------------\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[33mSome components are not installed. Run 'metta install' to set them up.\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[33mComponents not installed: system\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[34mTo fix: metta install system\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Optional: confirm you're set up to connect to the services used in this notebook\n",
    "#    If the command does not run, run `./install.sh` from your terminal\n",
    "\n",
    "!metta status --components=core,system,aws,wandb --non-interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Setup complete! Auto-reload enabled.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from experiments.notebooks.utils.metrics import fetch_metrics\n",
    "from experiments.notebooks.utils.monitoring import monitor_training_statuses\n",
    "from experiments.notebooks.utils.replays import show_replay\n",
    "from experiments.notebooks.utils.training import launch_training\n",
    "from datetime import datetime\n",
    "from metta.common.wandb.wandb_runs import find_training_runs\n",
    "from metta.rl.trainer_config import TrainerConfig, CheckpointConfig, TorchProfilerConfig, SimulationConfig\n",
    "from datetime import datetime\n",
    "from metta.rl.trainer_config import TrainerConfig, CheckpointConfig, TorchProfilerConfig, SimulationConfig\n",
    "from experiments.notebooks.utils.training import launch_training\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"default\")\n",
    "\n",
    "print(\"Setup complete! Auto-reload enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up MettaGrid Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for PyActionsConfig\nnoop\n  Input should be a valid dictionary or instance of PyActionConfig [type=model_type, input_value=PyActionConfig(enabled=Tr..., consumed_resources={}), input_type=PyActionConfig]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmetta\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmettagrid\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m builder\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m env_cfg = \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43marena\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_agents\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# map_cfg = builder.maps.arena.basic()\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(env_cfg.model_dump())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:2\u001b[39m, in \u001b[36marena\u001b[39m\u001b[34m(num_agents)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/metta/.venv/lib/python3.11/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for PyActionsConfig\nnoop\n  Input should be a valid dictionary or instance of PyActionConfig [type=model_type, input_value=PyActionConfig(enabled=Tr..., consumed_resources={}), input_type=PyActionConfig]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type"
     ]
    }
   ],
   "source": [
    "from metta.mettagrid.config import builder\n",
    "\n",
    "env_cfg = builder.arena(num_agents=2)\n",
    "# map_cfg = builder.maps.arena.basic()\n",
    "\n",
    "\n",
    "print(env_cfg.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training with run name: daveey.training-run.2025-08-08_21-25...\n",
      "total_timesteps=10000000000 ppo=PPOConfig(clip_coef=0.1, ent_coef=0.0021, gae_lambda=0.916, gamma=0.977, max_grad_norm=0.5, vf_clip_coef=0.1, vf_coef=0.44, l2_reg_loss_coef=0, l2_init_loss_coef=0, norm_adv=True, clip_vloss=True, target_kl=None) optimizer=OptimizerConfig(type='adam', learning_rate=0.000457, beta1=0.9, beta2=0.999, eps=1e-12, weight_decay=0) prioritized_experience_replay=PrioritizedExperienceReplayConfig(prio_alpha=0.0, prio_beta0=0.6) vtrace=VTraceConfig(vtrace_rho_clip=1.0, vtrace_c_clip=1.0) zero_copy=True require_contiguous_env_ids=False verbose=True batch_size=524288 minibatch_size=16384 bptt_horizon=64 update_epochs=1 scale_batches_by_world_size=False cpu_offload=False compile=False compile_mode='reduce-overhead' profiler=TorchProfilerConfig(interval_epochs=10000, profile_dir='s3://softmax-public/profiles/${run}') forward_pass_minibatch_target_size=4096 async_factor=2 hyperparameter_scheduler=HyperparameterSchedulerConfig(learning_rate_schedule=None, ppo_clip_schedule=None, ppo_ent_coef_schedule=None, ppo_vf_clip_schedule=None, ppo_l2_reg_loss_schedule=None, ppo_l2_init_loss_schedule=None) kickstart=KickstartConfig(teacher_uri=None, action_loss_coef=1, value_loss_coef=1, anneal_ratio=0.65, kickstart_steps=1000000000, additional_teachers=None) num_workers=6 env=None curriculum='env/mettagrid/arena/basic' env_overrides={} initial_policy=InitialPolicyConfig(uri=None, type='top', range=1, metric='epoch', filters={}) checkpoint=CheckpointConfig(checkpoint_interval=50, wandb_checkpoint_interval=50, checkpoint_dir='s3://softmax-public/checkpoints/${run}') simulation=SimulationConfig(evaluate_interval=200, replay_dir='s3://softmax-public/replays/${run}', evaluate_remote=True, evaluate_local=True, skip_git_check=False, git_hash=None) grad_mean_variance_interval=0\n"
     ]
    }
   ],
   "source": [
    "# Example: Launch training\n",
    "\n",
    "\n",
    "\n",
    "run_name = f\"{os.environ.get('USER')}.training-run.{datetime.now().strftime('%Y-%m-%d_%H-%M')}\"\n",
    "print(f\"Launching training with run name: {run_name}...\")\n",
    "\n",
    "trainer_cfg = TrainerConfig(\n",
    "    num_workers=6,\n",
    "    profiler=TorchProfilerConfig(\n",
    "        profile_dir=\"s3://softmax-public/profiles/${run}\"\n",
    "    ),\n",
    "    checkpoint=CheckpointConfig(\n",
    "        checkpoint_dir=\"s3://softmax-public/checkpoints/${run}\"\n",
    "    ),\n",
    "    simulation=SimulationConfig(\n",
    "        replay_dir=\"s3://softmax-public/replays/${run}\"\n",
    "    ),\n",
    "    curriculum=\"env/mettagrid/arena/basic\",\n",
    ")\n",
    "\n",
    "print(trainer_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training with run name: daveey.training-run.2025-08-08_21-12...\n",
      "total_timesteps=50000000000 ppo=PPOConfig(clip_coef=0.1, ent_coef=0.0021, gae_lambda=0.916, gamma=0.977, max_grad_norm=0.5, vf_clip_coef=0.1, vf_coef=0.44, l2_reg_loss_coef=0, l2_init_loss_coef=0, norm_adv=True, clip_vloss=True, target_kl=None) optimizer=OptimizerConfig(type='adam', learning_rate=0.0004573146765703167, beta1=0.9, beta2=0.999, eps=1e-12, weight_decay=0) prioritized_experience_replay=PrioritizedExperienceReplayConfig(prio_alpha=0.0, prio_beta0=0.6) vtrace=VTraceConfig(vtrace_rho_clip=1.0, vtrace_c_clip=1.0) zero_copy=True require_contiguous_env_ids=False verbose=True batch_size=524288 minibatch_size=16384 bptt_horizon=64 update_epochs=1 scale_batches_by_world_size=False cpu_offload=False compile=False compile_mode='reduce-overhead' profiler=TorchProfilerConfig(interval_epochs=10000, profile_dir='s3://softmax-public/profiles/${run}') forward_pass_minibatch_target_size=4096 async_factor=2 hyperparameter_scheduler=HyperparameterSchedulerConfig(learning_rate_schedule=None, ppo_clip_schedule=None, ppo_ent_coef_schedule=None, ppo_vf_clip_schedule=None, ppo_l2_reg_loss_schedule=None, ppo_l2_init_loss_schedule=None) kickstart=KickstartConfig(teacher_uri=None, action_loss_coef=1, value_loss_coef=1, anneal_ratio=0.65, kickstart_steps=1000000000, additional_teachers=None) num_workers=6 env=None curriculum='/env/mettagrid/curriculum/simple' env_overrides={} initial_policy=InitialPolicyConfig(uri=None, type='top', range=1, metric='epoch', filters={}) checkpoint=CheckpointConfig(checkpoint_interval=60, wandb_checkpoint_interval=300, checkpoint_dir='s3://softmax-public/checkpoints/${run}') simulation=SimulationConfig(evaluate_interval=300, replay_dir='s3://softmax-public/replays/${run}', evaluate_remote=True, evaluate_local=True, skip_git_check=False, git_hash=None) grad_mean_variance_interval=0\n"
     ]
    }
   ],
   "source": [
    "print(trainer_cfg)\n",
    "# # View `launch_training` function for all options\n",
    "# result = launch_training(\n",
    "#     run_name=run_name,\n",
    "#     curriculum=\"env/mettagrid/arena/basic\",\n",
    "#     wandb_tags=[f\"{os.environ.get('USER')}-arena-experiment\"],\n",
    "#     additional_args=[\"--skip-git-check\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Training Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor Training\n",
    "run_names = [\"daveey.navigation.low_reward.baseline.2\", \"daveey.navigation.low_reward.baseline.07-18\"]\n",
    "\n",
    "# Optional: instead, find all runs that meet some criteria\n",
    "# run_names = find_training_runs(\n",
    "#     # wandb_tags=[\"low_reward\"],\n",
    "#     # state=\"finished\",\n",
    "#     author=os.getenv(\"USER\"),\n",
    "#     limit=5,\n",
    "# )\n",
    "\n",
    "df = monitor_training_statuses(run_names, show_metrics=[\"_step\", \"overview/reward\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dfs = fetch_metrics(run_names, samples=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overview metrics for all fetched runs\n",
    "if not metrics_dfs:\n",
    "    print(\"No metrics data available. Please fetch metrics first.\")\n",
    "else:\n",
    "    print(f\"Plotting metrics for {len(metrics_dfs)} runs\")\n",
    "\n",
    "    # Find common metrics across all runs\n",
    "    all_columns = set()\n",
    "    for _, df in metrics_dfs.items():\n",
    "        all_columns.update(df.columns)\n",
    "\n",
    "    columns = [\"overview/reward\", \"losses/explained_variance\"]\n",
    "    plot_cols = []\n",
    "\n",
    "    for col in all_columns:\n",
    "        if col not in columns:\n",
    "            continue\n",
    "        # Check if this column exists in at least one run with numeric data\n",
    "        has_numeric_data = False\n",
    "        for df in metrics_dfs.values():\n",
    "            if col in df.columns and pd.api.types.is_numeric_dtype(df[col]) and df[col].nunique() > 1:\n",
    "                has_numeric_data = True\n",
    "                break\n",
    "        if has_numeric_data:\n",
    "            plot_cols.append(col)\n",
    "\n",
    "    if not plot_cols:\n",
    "        print(\"No plottable metrics found\")\n",
    "    else:\n",
    "        # Calculate grid dimensions\n",
    "        n_metrics = len(plot_cols)\n",
    "        n_cols = min(3, n_metrics)  # Max 3 columns\n",
    "        n_rows = (n_metrics + n_cols - 1) // n_cols\n",
    "\n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=n_rows,\n",
    "            cols=n_cols,\n",
    "            subplot_titles=[col.replace(\"overview/\", \"\").replace(\"_\", \" \") for col in plot_cols],\n",
    "            vertical_spacing=0.08,\n",
    "            horizontal_spacing=0.1,\n",
    "        )\n",
    "\n",
    "        # Color palette for different runs\n",
    "        colors = [\"blue\", \"red\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n",
    "\n",
    "        # Add traces for each metric and each run\n",
    "        for idx, col in enumerate(plot_cols):\n",
    "            row = (idx // n_cols) + 1\n",
    "            col_idx = (idx % n_cols) + 1\n",
    "\n",
    "            # Plot each run for this metric\n",
    "            for run_idx, (run_name, df) in enumerate(metrics_dfs.items()):\n",
    "                if col in df.columns and \"_step\" in df.columns:\n",
    "                    color = colors[run_idx % len(colors)]\n",
    "\n",
    "                    # Only show legend on first subplot to avoid clutter\n",
    "                    show_legend = idx == 0\n",
    "\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=df[\"_step\"],\n",
    "                            y=df[col],\n",
    "                            mode=\"lines\",\n",
    "                            name=run_name,\n",
    "                            line=dict(color=color, width=2),\n",
    "                            showlegend=show_legend,\n",
    "                            legendgroup=run_name,  # Group all traces from same run\n",
    "                        ),\n",
    "                        row=row,\n",
    "                        col=col_idx,\n",
    "                    )\n",
    "\n",
    "        # Update layout\n",
    "        runs_text = \"run\" if len(metrics_dfs) == 1 else \"runs\"\n",
    "        fig.update_layout(\n",
    "            height=250 * n_rows,\n",
    "            showlegend=True,\n",
    "            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "        )\n",
    "\n",
    "        # Update x-axes labels for bottom row\n",
    "        for col_idx in range(1, min(n_cols, n_metrics) + 1):\n",
    "            fig.update_xaxes(title_text=\"Steps\", row=n_rows, col=col_idx)\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Replays\n",
    "\n",
    "Display replay viewer for a specific run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available replays\n",
    "# replays = get_available_replays(\"daveey.lp.16x4.bptt8\")\n",
    "\n",
    "# Show the last replay for a run\n",
    "show_replay(\"daveey.lp.16x4.bptt8\", step=\"last\", width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
