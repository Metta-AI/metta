#!/usr/bin/env python3
"""
Buffer sharing regression tests to prevent the GPU 8-epoch stall bug.

These tests ensure that the critical buffer sharing optimization that prevents
GPU stalls remains functional. The original bug was caused by returning new
arrays instead of shared PufferEnv buffers, leading to memory copying and
performance degradation over training epochs.
"""

import inspect
import time

import numpy as np
import pytest

from metta.mettagrid.mettagrid_c import (
    dtype_actions,
    dtype_observations,
    dtype_rewards,
    dtype_terminals,
    dtype_truncations,
)
from metta.mettagrid.mettagrid_env import MettaGridEnv


class TestBufferSharingRegression:
    """Essential tests to prevent buffer sharing regressions."""

    @pytest.mark.skip(reason="Flaky test")
    def test_buffer_sharing_performance_benchmark(self):
        """
        Test that buffer sharing provides significant performance advantage over copying.

        This test catches regressions where buffer sharing is accidentally broken,
        which would cause the GPU 8-epoch stall bug to return.
        """
        # Simulate environment buffer sizes typical in training
        num_agents = 4
        obs_shape = (32, 32, 3)

        # Create shared buffers (the fix)
        shared_obs = np.zeros((num_agents,) + obs_shape, dtype=np.uint8)
        shared_rewards = np.zeros(num_agents, dtype=np.float32)

        # Test copying approach (the bug that was fixed)
        copying_times = []
        for _ in range(50):
            start_time = time.perf_counter()
            # Simulate creating new arrays every step (the bug)
            _ = np.random.randint(0, 255, (num_agents,) + obs_shape, dtype=np.uint8)
            _ = np.random.random(num_agents).astype(np.float32)
            copying_times.append(time.perf_counter() - start_time)

        # Test sharing approach (the fix)
        sharing_times = []
        for _ in range(50):
            start_time = time.perf_counter()
            # Simulate modifying shared buffers in place (the fix)
            shared_obs.fill(42)
            shared_rewards.fill(1.0)
            # Return same objects (no new allocation)
            # In the real code, these would be returned from step()
            sharing_times.append(time.perf_counter() - start_time)

        # Analyze performance difference
        copying_avg = np.mean(copying_times)
        sharing_avg = np.mean(sharing_times)
        speedup = copying_avg / sharing_avg if sharing_avg > 0 else 1

        print("Buffer sharing performance:")
        print(f"  Copying: {copying_avg * 1000:.3f}ms avg")
        print(f"  Sharing: {sharing_avg * 1000:.3f}ms avg")
        print(f"  Speedup: {speedup:.1f}x")

        # Buffer sharing should be significantly faster
        min_speedup = 1.1
        assert speedup >= min_speedup, (
            f"Buffer sharing regression detected: only {speedup:.1f}x speedup "
            f"(expected at least {min_speedup}x). This may indicate a return to "
            f"the buffer copying behavior that caused GPU training stalls."
        )

    def test_step_method_signature_stability(self):
        """
        Test that MettaGridEnv.step method signature hasn't changed unexpectedly.

        This catches regressions where the step method is accidentally modified
        in a way that could break buffer sharing.
        """
        # Verify step method exists and has expected signature
        assert hasattr(MettaGridEnv, "step"), "MettaGridEnv missing step method"

        step_method = MettaGridEnv.step
        sig = inspect.signature(step_method)
        params = list(sig.parameters.keys())

        # Verify critical parameters exist
        assert "self" in params, "Step method missing self parameter"
        assert "actions" in params, "Step method missing actions parameter"

        print("✅ Step method signature verified")

    def test_buffer_type_compatibility(self):
        """
        Test that buffer dtypes match PufferLib expectations.

        This ensures that any changes to buffer handling maintain compatibility
        with PufferLib's zero-copy optimization requirements.
        """

        # Create test arrays with expected dtypes
        test_obs = np.zeros((2, 32, 32, 3), dtype=dtype_observations)
        test_rewards = np.zeros(2, dtype=dtype_rewards)
        test_terminals = np.zeros(2, dtype=dtype_terminals)
        test_truncations = np.zeros(2, dtype=dtype_truncations)
        test_actions = np.zeros((2, 2), dtype=dtype_actions)

        # Verify dtypes match PufferLib expectations
        assert test_obs.dtype == np.uint8, f"Observations dtype mismatch: {test_obs.dtype}"
        assert test_rewards.dtype == np.float32, f"Rewards dtype mismatch: {test_rewards.dtype}"
        assert test_terminals.dtype == bool, f"Terminals dtype mismatch: {test_terminals.dtype}"
        assert test_truncations.dtype == bool, f"Truncations dtype mismatch: {test_truncations.dtype}"
        assert test_actions.dtype == np.int32, f"Actions dtype mismatch: {test_actions.dtype}"

        print("✅ Buffer dtypes compatible with PufferLib requirements")

    def test_memory_allocation_pattern_detection(self):
        """
        Test that detects patterns consistent with buffer copying vs sharing.

        This test verifies that we can detect when arrays are being copied
        instead of shared, which was the root cause of the GPU stall bug.
        """
        # Simulate shared buffer behavior (the fix)
        shared_buffer = np.zeros((4, 32, 32, 3), dtype=np.uint8)
        initial_address = shared_buffer.__array_interface__["data"][0]

        # Simulate multiple steps with shared buffer
        for step in range(10):
            # Modify buffer in place (proper sharing)
            shared_buffer.fill(step % 255)

            # Verify memory address stays the same (no reallocation)
            current_address = shared_buffer.__array_interface__["data"][0]
            assert current_address == initial_address, (
                f"Buffer memory address changed at step {step}, indicating potential buffer copying regression"
            )

        # Test that we can detect the difference between sharing and copying
        # by using object identity instead of memory addresses (which can be reused)
        shared_results = []
        for _ in range(5):
            # Return the same shared buffer object (proper sharing)
            shared_results.append(shared_buffer)

        # All should be the same object (buffer sharing)
        for i, result in enumerate(shared_results):
            assert result is shared_buffer, f"Buffer sharing broken at step {i}"

        # Test copying behavior detection
        copied_results = []
        for _ in range(5):
            # Create new array each time (copying behavior - the bug)
            new_buffer = np.copy(shared_buffer)
            copied_results.append(new_buffer)

        # Each should be a different object (indicates copying)
        for i in range(len(copied_results)):
            for j in range(i + 1, len(copied_results)):
                assert copied_results[i] is not copied_results[j], (
                    f"Arrays at indices {i} and {j} are the same object, but should be different when copying"
                )

        print("✅ Memory allocation pattern detection working correctly")

    def test_gpu_compatibility_check(self):
        """
        Test GPU compatibility when CUDA is available.

        This test only runs when PyTorch with CUDA is available and verifies
        that buffer sharing works in GPU environments.
        """
        torch = pytest.importorskip("torch", reason="PyTorch not available for GPU test")

        if not torch.cuda.is_available():
            pytest.skip("CUDA not available for GPU compatibility test")

        # Basic GPU compatibility test
        device_name = torch.cuda.get_device_name(0)
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3

        print(f"GPU compatibility check on: {device_name} ({gpu_memory_gb:.1f}GB)")

        # Test that we can create tensors and move them to GPU
        # This simulates what happens in training with shared buffers
        test_obs = np.random.randint(0, 255, (4, 32, 32, 3), dtype=np.uint8)
        obs_tensor = torch.from_numpy(test_obs).cuda()

        assert obs_tensor.is_cuda, "Failed to move tensor to GPU"
        assert obs_tensor.dtype == torch.uint8, "GPU tensor dtype mismatch"

        print("✅ GPU compatibility verified")


if __name__ == "__main__":
    # Allow running as standalone script
    pytest.main([__file__, "-v"])
