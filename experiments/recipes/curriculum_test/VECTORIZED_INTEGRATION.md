# Vectorized Environment Integration (Option C) - Complete! âœ…

## What Was Implemented

The task dependency simulator now uses **Option C** from the integration recommendations - full vectorized environment simulation matching production training.

## Key Changes

### 1. Created TaskDependencyEnv (PufferEnv Interface)

```python
class TaskDependencyEnv(PufferEnv):
    """Mock environment for task dependency simulation."""
    
    def reset(self, *args, **kwargs):
        # Extracts task class from config label
        # Returns dummy observation
        
    def step(self, action):
        # Samples task from simulator
        # Emits stats via info dict
        # Returns obs, reward, terminal, truncated, info
        
    def get_episode_rewards(self):
        # Called by CurriculumEnv to get performance
        
    def set_mg_config(self, config):
        # Called by CurriculumEnv to switch tasks
```

### 2. Wrapped with CurriculumEnv

```python
# Create vectorized environments wrapped with CurriculumEnv
envs = []
for i in range(num_envs):
    base_env = TaskDependencyEnv(simulator, initial_task.get_env_cfg())
    curriculum_env = CurriculumEnv(base_env, curriculum)  # Just like real training!
    envs.append(curriculum_env)
```

### 3. Stats Through accumulate_rollout_stats()

```python
for epoch in range(num_epochs):
    rollout_stats = defaultdict(list)
    
    for _ in range(samples_per_epoch):
        info_batch = []
        for env in envs:
            obs, reward, terminal, truncated, info = env.step(0)
            info_batch.append(info)
        
        # Real stats pipeline!
        accumulate_rollout_stats(info_batch, rollout_stats)
```

## What This Achieves

### âœ… Exact Match to Real Training

- **Same code paths**: Uses CurriculumEnv wrapper exactly as production training does
- **Same stats pipeline**: Stats flow through `accumulate_rollout_stats()`
- **Same task management**: CurriculumEnv handles task switching, completion, updates
- **Same vectorization**: Multiple parallel environments simulate real training

### âœ… Automatic CurriculumEnv Stats

All CurriculumEnv stats are automatically included:
- `env_curriculum_stats/pool_occupancy_gini`
- `env_curriculum_stats/pool_lp_gini`
- `env_curriculum_stats/per_label_lp_scores`
- `env_curriculum_stats/tracked_task_lp_scores`
- `env_curriculum_stats/sampling_gini`

### âœ… Apples-to-Apples Comparison

Metrics now match real training format exactly:
- `metric/agent_step`, `metric/epoch` (x-axis)
- `overview/*` (high-level metrics)
- `env_curriculum_stats/*` (curriculum metrics)
- `env_task_dependency/*` (simulator-specific metrics)

### âœ… Test Real Scenarios

Can now test:
- Single vs multi-environment behavior
- Shared memory backend (set `use_shared_memory=True`)
- Task eviction with CurriculumEnv's automatic management
- Stats aggregation across vectorized environments

## Usage

### Basic Usage

```bash
uv run ./tools/run.py experiments.recipes.curriculum_test.task_dependency_simulator.train \
    run=msb_nav_cc_taskgraph_v13
```

### Test Vectorization

```bash
# Single environment
uv run ./tools/run.py experiments.recipes.curriculum_test.task_dependency_simulator.train \
    num_envs=1 run=single_env_test

# 8 parallel environments (like real training)
uv run ./tools/run.py experiments.recipes.curriculum_test.task_dependency_simulator.train \
    num_envs=8 run=vectorized_test
```

### Test Shared Memory

```bash
# Test shared memory backend (like multi-worker training)
uv run python -c "
from experiments.recipes.curriculum_test.task_dependency_simulator import train

tool = train(num_tasks=10, num_epochs=100, num_envs=4, run='test_shared_memory')
tool.use_shared_memory = True
tool.session_id = 'test_session_123'
tool.invoke({})
"
```

## Comparison to Real Training

### Real Training Code

```python
# In metta/rl/vecenv.py
def make_env_func(curriculum, ...):
    env = MettaGridEnv(curriculum.get_task().get_env_cfg(), ...)
    env = CurriculumEnv(env, curriculum)  # Wrap with CurriculumEnv
    return env

# Multiple workers call this
for step in range(total_steps):
    obs, reward, terminal, truncated, info = vecenv.step(actions)
    accumulate_rollout_stats([info], stats)  # Collect stats
```

### Simulator Code (Now Matching!)

```python
# In experiments/recipes/curriculum_test/task_dependency_simulator.py
def simulate_task_dependencies(num_envs=4, ...):
    # Create vectorized environments
    envs = []
    for i in range(num_envs):
        base_env = TaskDependencyEnv(simulator, initial_task.get_env_cfg())
        curriculum_env = CurriculumEnv(base_env, curriculum)  # Same!
        envs.append(curriculum_env)
    
    # Rollout loop
    for epoch in range(num_epochs):
        for _ in range(samples_per_epoch):
            info_batch = []
            for env in envs:
                obs, reward, terminal, truncated, info = env.step(0)
                info_batch.append(info)
            accumulate_rollout_stats(info_batch, rollout_stats)  # Same!
```

## Benefits

### 1. Validates Curriculum Changes

Test curriculum algorithm changes in the simulator before expensive GPU runs:
- Change LP scoring parameters
- Test different eviction strategies
- Validate task generation logic
- Debug curriculum behavior

### 2. Perfect Metrics Comparison

Compare simulator runs to real training runs in WandB:
- Same metric names
- Same x-axis (agent_step)
- Same groupings (overview/, env_curriculum_stats/, etc.)
- Direct apples-to-apples comparison

### 3. Tests Real Code Paths

The simulator now tests the actual CurriculumEnv code:
- CurriculumEnv.step() logic
- CurriculumEnv.reset() logic
- Task completion handling
- Stats emission patterns
- Task eviction triggers

### 4. Reproducible Behavior

Simulator behavior matches real training:
- Same task selection logic
- Same stats aggregation
- Same curriculum updates
- Same vectorization patterns

## Next Steps

### For Your Experiment

Run your experiment with the updated simulator:

```bash
uv run ./tools/run.py experiments.recipes.curriculum_test.task_dependency_simulator.train \
    run=msb_nav_cc_taskgraph_v13 \
    num_tasks=10 \
    num_epochs=1000 \
    num_envs=8
```

This will give you curriculum behavior metrics that directly compare to real training!

### Comparing to Real Training

1. Run simulator with curriculum parameters
2. Run real training with same parameters
3. Compare in WandB:
   - `overview/sampling_entropy_normalized` (exploration)
   - `env_curriculum_stats/pool_occupancy_gini` (task distribution)
   - `env_curriculum_stats/pool_lp_gini` (LP distribution)
   - `env_curriculum_stats/mean_lp_score` (average learning progress)

### Testing Curriculum Changes

Before deploying curriculum changes to production:

1. **Test in simulator** (fast, cheap)
   ```bash
   uv run ./tools/run.py experiments.recipes.curriculum_test.task_dependency_simulator.train \
       ema_timescale=0.05 eviction_threshold_percentile=0.2 run=test_new_params
   ```

2. **Verify metrics look good** (check WandB)

3. **Deploy to real training** (confident it will work)

## Summary

âœ… **Option C implemented**: Vectorized environments with CurriculumEnv wrapper
âœ… **Matches real training**: Uses exact same code paths and stats pipeline
âœ… **Task class labels**: Labels show position in dependency chain (taskclass0, taskclass1, ...)
âœ… **Three Gini coefficients**: sampling_gini, pool_occupancy_gini, pool_lp_gini
âœ… **Apples-to-apples metrics**: Format matches real training for direct comparison
âœ… **Vectorization support**: Test with 1 to N parallel environments

The simulator is now a production-grade testing tool for curriculum development! ðŸŽ‰

