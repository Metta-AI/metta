# LP Sweep Config: LP_weight = 0.5 (Moderate Learning Progress)
# This adds moderate learning progress influence to LowRewardCurriculum

defaults:
  - _self_

# Basic run configuration
run: ${oc.env:USER}.lp_sweep_0.5
run_dir: ${data_dir}/${run}

# Training configuration
trainer:
  curriculum: env/mettagrid/curriculum/lp_low_reward
  total_timesteps: 1000000  # 1M timesteps for comparison

  # Fixed hyperparameters for fair comparison
  gamma: 0.99
  gae_lambda: 0.95
  vf_coef: 0.5
  ent_coef: 0.01
  batch_size: 512
  minibatch_size: 256
  bptt_horizon: 8
  learning_rate: 3e-4
  evaluate_interval: 100

# LP weight = 0.5 (moderate learning progress influence)
trainer.curriculum.lp_weight: 0.5

# Environment configuration
trainer.curriculum.env_cfg:
  envs:
    - /env/mettagrid/navigation/training/terrain_from_numpy
    - /env/mettagrid/navigation/training/cylinder_world
    - /env/mettagrid/navigation/training/varied_terrain_sparse
    - /env/mettagrid/navigation/training/varied_terrain_balanced
    - /env/mettagrid/navigation/training/varied_terrain_maze
    - /env/mettagrid/navigation/training/varied_terrain_dense
  num_agents: 24

# LPC-specific parameters (only used when lp_weight > 0)
trainer.curriculum.ema_alpha: 0.001
trainer.curriculum.p_theta: 0.05
trainer.curriculum.num_active_tasks: 16
trainer.curriculum.rand_task_rate: 0.25
trainer.curriculum.sample_threshold: 10
trainer.curriculum.memory: 25
trainer.curriculum.lp_metric: "episode/reward.mean"

# LowRewardCurriculum parameters
trainer.curriculum.low_reward_threshold: 0.1
trainer.curriculum.min_tasks_per_env: 2
trainer.curriculum.max_tasks_per_env: 8

# WandB configuration
wandb:
  enabled: true
  project: metta
  entity: metta-research
  name: ${run}
  tags: ["lp_sweep", "lp_weight_0.5", "low_reward_curriculum", "moderate_lp"]

# Hardware configuration
hardware: macbook
