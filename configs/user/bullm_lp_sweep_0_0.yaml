# LP Sweep Config: LP_weight = 0.0 (Baseline LowRewardCurriculum)
# This is the baseline without any learning progress influence

defaults:
  - _self_

# Basic run configuration
run: ${oc.env:USER}.lp_sweep_0.0
run_dir: ${data_dir}/${run}

# Training configuration
trainer:
  curriculum: env/mettagrid/curriculum/lp_low_reward
  total_timesteps: 1000000  # 1M timesteps for comparison
  gamma: 0.99
  gae_lambda: 0.95
  vf_coef: 0.5
  ent_coef: 0.01
  batch_size: 512
  minibatch_size: 256
  bptt_horizon: 8
  learning_rate: 3e-4
  evaluate_interval: 100
  lp_weight: 0.0
  num_workers: 1
  env_cfg:
    envs:
      - /env/mettagrid/navigation/training/terrain_from_numpy
      - /env/mettagrid/navigation/training/cylinder_world
      - /env/mettagrid/navigation/training/varied_terrain_sparse
      - /env/mettagrid/navigation/training/varied_terrain_balanced
      - /env/mettagrid/navigation/training/varied_terrain_maze
      - /env/mettagrid/navigation/training/varied_terrain_dense
    num_agents: 24
  curriculum:
    lp_weight: 0.0
    ema_alpha: 0.001
    p_theta: 0.05
    num_active_tasks: 16
    rand_task_rate: 0.25
    sample_threshold: 10
    memory:
    lp_metric: "episode/reward.mean"
    low_reward_threshold: 0.1
    min_tasks_per_env: 2
    max_tasks_per_env: 8

# WandB configuration
wandb:
  enabled: false
  project: metta
  entity: metta-research
  name: ${run}
  tags: ["lp_sweep", "lp_weight_0.0", "low_reward_curriculum", "baseline"]

