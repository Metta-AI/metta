{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Interactive Heatmap Widget Example\n",
    "\n",
    "This notebook demonstrates how to use the `HeatmapWidget` - an anywidget-based implementation of the observatory heatmap component for Jupyter notebooks.\n",
    "\n",
    "The widget provides interactive policy evaluation heatmaps with:\n",
    "- Have control over the number of policies displayed\n",
    "- Decide and select metrics to render\n",
    "- Choose which runs to get policies from\n",
    "- Dynamically get data from the Observatory API to render\n",
    "- Display heatmaps for custom metrics if you like\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Import and Basic Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext anywidget\n",
    "\n",
    "print(\"Setup complete! Auto-reload enabled.\")\n",
    "\n",
    "from experiments.notebooks.utils.heatmap_widget.heatmap_widget.HeatmapWidget import HeatmapWidget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Real Data from Metta Database\n",
    "\n",
    "Now let's write code that fetches real evaluation data from metta's databases:\n",
    "\n",
    "First we'll need some API variables.\n",
    "\n",
    "Then we'll make an API client for the metta API.\n",
    "\n",
    "Then we'll use the client to retrieve policy data.\n",
    "\n",
    "Then we'll render it with the HeatmapWidget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metta.common.client.metta_client import MettaAPIClient\n",
    "from experiments.notebooks.utils.heatmap_widget.heatmap_widget.util import fetch_real_heatmap_data\n",
    "\n",
    "client = MettaAPIClient(\"https://api.observatory.softmax-research.net\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example: Finding policies with search text\n",
    "\n",
    "Now let's explore what's available in the database and create a heatmap with real data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, let's try with some common metrics and see what we find:\n",
    "\n",
    "# You can search for policies with a list of search texts\n",
    "search_texts = [\"zfogg.1753775626\", \"daveey.arena.rnd.\"]\n",
    "\n",
    "# You can get training run policies by exact training run names\n",
    "# search_texts = [\n",
    "#     \"daveey.arena.rnd.16x4.2\",\n",
    "#     \"relh.skypilot.fff.j20.666\",\n",
    "#     \"bullm.navigation.low_reward.baseline\",\n",
    "#     \"bullm.navigation.low_reward.baseline.07-17\", \n",
    "#     \"bullm.navigation.low_reward.baseline.07-23\",\n",
    "#     \"relh.multigpu.fff.1\",\n",
    "#     \"relh.skypilot.fff.j21.2\",\n",
    "# ]\n",
    "\n",
    "# Common metrics that are likely to exist:\n",
    "# metrics_to_fetch = [\"reward\", \"heart.get\", \"action.move.success\"]\n",
    "metrics_to_fetch = [\"ore_red.get\"]\n",
    "\n",
    "# We're using search_texts to find training run policies to generate heatmaps for.\n",
    "real_heatmap = await fetch_real_heatmap_data(\n",
    "    search_texts=search_texts,\n",
    "    api_base_url=str(client._http_client.base_url),\n",
    "    metrics=metrics_to_fetch,\n",
    "    max_policies=50  # Limit display to keep it manageable\n",
    ")\n",
    "    \n",
    "real_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example: Finding policies using training run names\n",
    "\n",
    "Here's how to create a heatmap with your own training runs and metrics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a custom heatmap with specific training runs and metrics\n",
    "\n",
    "# Use training run names (recommended - uses smart selection)\n",
    "my_training_runs = [\n",
    "    # Add your training run names here, for example:\n",
    "    \"gregorypylypovych.navigation.ffa_NAV_DEFAULT_vtm_4rooms_of_4_seed0.07-27\",\n",
    "    \"gregorypylypovych.navigation.ffa_DEFAULT_vtm_4rooms_of_4_seed0.07-27\",\n",
    "    \"gregorypylypovych.navigation.ffa_DEFAULT_tfn_4rooms_of_4_seed0.07-27\",\n",
    "    \"gregorypylypovych.navigation.ffa_DEFAULT_vtb_4rooms_of_4_seed0.07-27\",\n",
    "]\n",
    "\n",
    "# Step 2: Define metrics you want to compare\n",
    "my_metrics = [\n",
    "    \"reward\",\n",
    "    \"heart.get\",           # Example game-specific metric\n",
    "    \"action.move.success\", # Example action success rate\n",
    "    # Add more metrics as needed\n",
    "]\n",
    "\n",
    "print(\"ðŸŽ¯ Creating custom heatmap with best policies from training runs...\")\n",
    "# Select best policies from training runs\n",
    "custom_heatmap = await fetch_real_heatmap_data(\n",
    "    api_base_url=str(client._http_client.base_url),\n",
    "    search_texts=my_training_runs,\n",
    "    metrics=my_metrics,\n",
    "    policy_selector=\"best\",\n",
    "    max_policies=20\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š Custom heatmap created! Try:\")\n",
    "print(\"   - Hovering over cells to see detailed values\")\n",
    "print(\"   - Changing metrics with: custom_heatmap.update_metric('heart.get')\")\n",
    "print(\"   - Adjusting policies shown: custom_heatmap.set_num_policies(15)\")\n",
    "\n",
    "custom_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example: Setting the metric\n",
    "\n",
    "Now let's see the `update_metric` functionality working properly! This example shows a heatmap where changing the metric actually changes the displayed values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last one. Scroll up again to see the change.\n",
    "print(\"\\nðŸ”„ Changing metric to 'success_rate'...\")\n",
    "custom_heatmap.update_metric('action.move.success')\n",
    "\n",
    "print(\"Now the heatmap in the cell that ran before this one should have changed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now let's change it to reward.\n",
    "custom_heatmap.update_metric('reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Custom metrics\n",
    "\n",
    "We can really define our cells to have any metric data we want. This is useful because we plan to have all sorts of metrics. Let's look at an example of using any old metric we decide.\n",
    "\n",
    "First, you need to know the data format:\n",
    "\n",
    "### Info: Data Cell Format Reference\n",
    "\n",
    "The heatmap widget expects data in a specific format that matches the\n",
    "observatory dashboard:\n",
    "\n",
    "```python\n",
    "cells = {\n",
    "    'policy_name': {\n",
    "        'eval_name': {\n",
    "            'metrics': {\n",
    "                'reward': 50,\n",
    "                'heart.get': 98,\n",
    "                'action.move.success': 5,\n",
    "                'ore_red.get': 24.2,\n",
    "                # ... more metrics\n",
    "            },\n",
    "            'replayUrl': str,         # URL to replay file\n",
    "            'evalName': str,          # Should match the key\n",
    "        },\n",
    "        # ... more evaluations\n",
    "    },\n",
    "    # ... more policies\n",
    "}\n",
    "```\n",
    "\n",
    "**Important notes:**\n",
    "- Evaluation names with \"/\" will be grouped by category (the part before \"/\")\n",
    "- The heatmap shows policies sorted by average score (worst to best, bottom to top)\n",
    "- Policy names that contain \":v\" will have WandB URLs generated automatically\n",
    "- Replay URLs should be accessible URLs or file paths\n",
    "\n",
    "This widget provides the same interactive functionality as the observatory dashboard but in a python environment, making it perfect for exploratory analysis and sharing results via Jupyter notebooks!\n",
    "\n",
    "\n",
    "### Here's how to create a heatmap with your own data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new heatmap widget\n",
    "from experiments.notebooks.utils.heatmap_widget.heatmap_widget.HeatmapWidget import create_heatmap_widget\n",
    "\n",
    "custom_widget = create_heatmap_widget()\n",
    "\n",
    "# Define your data structure\n",
    "# This should match the format expected by the observatory dashboard\n",
    "cells_data = {\n",
    "    'my_policy_v1': {\n",
    "        'task_a/level1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 85.2,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay1.json', \n",
    "            'evalName': 'task_a/level1'\n",
    "        },\n",
    "        'task_a/level2': {\n",
    "            'metrics': {\n",
    "                'custom_score': 87.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay2.json', \n",
    "            'evalName': 'task_a/level2'\n",
    "        },\n",
    "        'task_b/challenge1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 92.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay3.json', \n",
    "            'evalName': 'task_b/challenge1'\n",
    "        },\n",
    "    },\n",
    "    'my_policy_v2': {\n",
    "        'task_a/level1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 22.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay4.json', \n",
    "            'evalName': 'task_a/level1'\n",
    "        },\n",
    "        'task_a/level2': {\n",
    "            'metrics': {\n",
    "                'custom_score': 42.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay5.json', \n",
    "            'evalName': 'task_a/level2'\n",
    "        },\n",
    "        'task_b/challenge1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 62.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay6.json', \n",
    "            'evalName': 'task_b/challenge1'\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "eval_names = ['task_a/level1', 'task_a/level2', 'task_b/challenge1']\n",
    "policy_names = ['my_policy_v1', 'my_policy_v2']\n",
    "policy_averages = {\n",
    "    'my_policy_v1': 91.6,\n",
    "    'my_policy_v2': 89.6,\n",
    "}\n",
    "\n",
    "# Set the data\n",
    "custom_widget.set_data(\n",
    "    cells=cells_data,\n",
    "    eval_names=eval_names,\n",
    "    policy_names=policy_names,\n",
    "    policy_average_scores=policy_averages,\n",
    "    selected_metric=\"custom_score\"\n",
    ")\n",
    "\n",
    "# Display the widget\n",
    "custom_widget\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
