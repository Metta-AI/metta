"""Curiosity Model Policy Optimization (CMPO) loss."""

from __future__ import annotations

import math
import random
from collections import deque
from typing import Any, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from pydantic import Field
from tensordict import TensorDict
from torch import Tensor
from torchrl.data import Composite, UnboundedContinuous, UnboundedDiscrete

try:  # Prefer gymnasium but fall back to gym if needed.
    from gymnasium import spaces as gym_spaces
except ImportError:  # pragma: no cover - fallback for older envs.
    from gym import spaces as gym_spaces  # type: ignore[no-redef]

from metta.agent.policy import Policy
from metta.rl.advantage import compute_advantage, normalize_advantage_distributed
from metta.rl.loss.loss import Loss, LossConfig
from metta.rl.training import ComponentContext, Experience, TrainingEnvironment
from metta.rl.utils import add_dummy_loss_for_unused_params, forward_policy_for_training
from mettagrid.base_config import Config


def _build_mlp(
    input_dim: int,
    hidden_dims: list[int],
    output_dim: int,
) -> nn.Sequential:
    """Construct a simple feed-forward network."""
    layers: list[nn.Module] = []
    last_dim = input_dim
    for hidden in hidden_dims:
        layers.append(nn.Linear(last_dim, hidden))
        layers.append(nn.LayerNorm(hidden))
        layers.append(nn.ReLU())
        last_dim = hidden
    layers.append(nn.Linear(last_dim, output_dim))
    return nn.Sequential(*layers)


def _stack_trajectories(trajs: list[TensorDict]) -> TensorDict:
    """Stack a list of trajectory tensordicts into a single batch."""
    if not trajs:
        raise ValueError("Cannot stack empty trajectory list")
    keys = list(trajs[0].keys())
    stacked = {key: torch.stack([traj[key] for traj in trajs], dim=0) for key in keys}
    batch_size = (len(trajs),) + trajs[0].batch_size
    return TensorDict(stacked, batch_size=batch_size)


class WorldModelConfig(Config):
    """Hyper-parameters for the dynamics ensemble."""

    ensemble_size: int = Field(default=5, ge=1)
    hidden_dims: list[int] = Field(default_factory=lambda: [512, 512])
    learning_rate: float = Field(default=3e-4, gt=0)
    batch_size: int = Field(default=256, gt=0)
    train_steps: int = Field(default=50, ge=1)
    buffer_size: int = Field(default=50000, gt=1)
    warmup_transitions: int = Field(default=2048, ge=0)
    gradient_clip: float = Field(default=1.0, gt=0)


class CuriosityConfig(Config):
    """Configuration for the curiosity module."""

    hidden_dims: list[int] = Field(default_factory=lambda: [256, 256])
    feature_dim: int = Field(default=128, ge=8)
    learning_rate: float = Field(default=1e-4, gt=0)
    weight_initial: float = Field(default=0.1, ge=0)
    weight_min: float = Field(default=0.01, ge=0)
    weight_max: float = Field(default=1.0, ge=0)
    adjustment_rate: float = Field(default=0.05, ge=0)
    positive_coef: float = Field(default=1.0, ge=0)
    negative_coef: float = Field(default=1.0, ge=0)
    reward_clip: float = Field(default=1.5, ge=0)
    train_steps: int = Field(default=50, ge=1)


class AdaptiveBufferSchedulerConfig(Config):
    """Adjusts the synthetic-vs-real sampling ratio."""

    min_ratio: float = Field(default=0.0, ge=0, le=1)
    max_ratio: float = Field(default=0.5, ge=0, le=1)
    target_model_error: float = Field(default=0.02, ge=0)
    adjustment_rate: float = Field(default=0.05, ge=0, le=1)


class ModelRolloutConfig(Config):
    """Controls synthetic rollouts generated by the world model."""

    rollout_length: int = Field(default=5, ge=1)
    batch_size: int = Field(default=128, ge=1)
    termination_variance_threshold: float = Field(default=0.25, ge=0)


class SyntheticBufferConfig(Config):
    """Buffer capacity for storing synthetic trajectories."""

    capacity: int = Field(default=2048, ge=1)


class CMPOConfig(LossConfig):
    """Full CMPO configuration."""

    # PPO-style hyperparameters
    clip_coef: float = Field(default=0.255736, gt=0, le=1.0)
    ent_coef: float = Field(default=0.027574, ge=0)
    norm_adv: bool = True
    target_kl: float | None = None

    # Value loss settings
    vf_clip_coef: float = Field(default=0.1, ge=0)
    vf_coef: float = Field(default=0.753832, ge=0)
    clip_vloss: bool = True

    world_model: WorldModelConfig = Field(default_factory=WorldModelConfig)
    curiosity: CuriosityConfig = Field(default_factory=CuriosityConfig)
    scheduler: AdaptiveBufferSchedulerConfig = Field(default_factory=AdaptiveBufferSchedulerConfig)
    model_rollouts: ModelRolloutConfig = Field(default_factory=ModelRolloutConfig)
    synthetic_buffer: SyntheticBufferConfig = Field(default_factory=SyntheticBufferConfig)

    def create(
        self,
        policy: Policy,
        trainer_cfg: Any,
        env: TrainingEnvironment,
        device: torch.device,
        instance_name: str,
    ) -> "CMPO":
        return CMPO(
            policy=policy,
            trainer_cfg=trainer_cfg,
            env=env,
            device=device,
            instance_name=instance_name,
            cfg=self,
        )


class FeedForwardDynamics(nn.Module):
    """Single ensemble member predicting state deltas and rewards."""

    def __init__(self, state_dim: int, action_dim: int, hidden_dims: list[int]) -> None:
        super().__init__()
        self.net = _build_mlp(state_dim + action_dim, hidden_dims, 2 * state_dim + 1)
        self.state_dim = state_dim

    def forward(self, state: Tensor, action: Tensor) -> tuple[Tensor, Tensor]:
        x = torch.cat([state, action], dim=-1)
        output = self.net(x)
        delta = output[..., : self.state_dim]
        reward = output[..., -1]
        next_state = state + delta
        return next_state, reward


class WorldModelEnsemble(nn.Module):
    """Ensemble of dynamics models used for MBPO rollouts."""

    def __init__(self, cfg: WorldModelConfig, state_dim: int, action_dim: int) -> None:
        super().__init__()
        self.members = nn.ModuleList(
            FeedForwardDynamics(state_dim=state_dim, action_dim=action_dim, hidden_dims=cfg.hidden_dims)
            for _ in range(cfg.ensemble_size)
        )

    def _predict(self, state: Tensor, action: Tensor) -> tuple[Tensor, Tensor]:
        preds_state: list[Tensor] = []
        preds_reward: list[Tensor] = []
        for member in self.members:
            next_state, reward = member(state, action)
            preds_state.append(next_state)
            preds_reward.append(reward)
        return torch.stack(preds_state, dim=0), torch.stack(preds_reward, dim=0)

    def forward(self, state: Tensor, action: Tensor) -> tuple[Tensor, Tensor]:
        next_state_stack, reward_stack = self._predict(state, action)
        return next_state_stack.mean(dim=0), reward_stack.mean(dim=0)

    def variance(self, state: Tensor, action: Tensor) -> tuple[Tensor, Tensor]:
        next_state_stack, reward_stack = self._predict(state, action)
        return next_state_stack.var(dim=0, unbiased=False), reward_stack.var(dim=0, unbiased=False)


class TransitionBuffer:
    """Stores real environment transitions for model and curiosity updates."""

    def __init__(self, capacity: int) -> None:
        self.capacity = capacity
        self._buffer: deque[dict[str, Tensor]] = deque(maxlen=capacity)

    def add_batch(
        self,
        states: Tensor,
        actions_enc: Tensor,
        actions_raw: Tensor,
        rewards: Tensor,
        next_states: Tensor,
        dones: Tensor,
    ) -> None:
        batch_size = states.shape[0]
        for idx in range(batch_size):
            self._buffer.append(
                {
                    "state": states[idx].detach().cpu(),
                    "action_enc": actions_enc[idx].detach().cpu(),
                    "action_raw": actions_raw[idx].detach().cpu(),
                    "reward": rewards[idx].detach().cpu(),
                    "next_state": next_states[idx].detach().cpu(),
                    "done": dones[idx].detach().cpu(),
                }
            )

    def __len__(self) -> int:
        return len(self._buffer)

    def sample(self, batch_size: int, device: torch.device) -> TensorDict | None:
        if not self._buffer:
            return None
        actual = min(batch_size, len(self._buffer))
        indices = random.sample(range(len(self._buffer)), actual)
        batch = [self._buffer[i] for i in indices]
        stacked = {k: torch.stack([item[k] for item in batch], dim=0).to(device=device) for k in batch[0].keys()}
        return TensorDict(stacked, batch_size=(actual,))


class CuriosityModule(nn.Module):
    """Forward and inverse curiosity models with shared encoder."""

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        cfg: CuriosityConfig,
    ) -> None:
        super().__init__()
        self.encoder = _build_mlp(state_dim, cfg.hidden_dims, cfg.feature_dim)
        forward_input = cfg.feature_dim + action_dim
        self.forward_model = _build_mlp(forward_input, cfg.hidden_dims, cfg.feature_dim)
        inverse_input = cfg.feature_dim * 2
        self.inverse_head = _build_mlp(inverse_input, cfg.hidden_dims, action_dim)

    def encode(self, state: Tensor) -> Tensor:
        return self.encoder(state)

    def predict_feature(self, feature: Tensor, action: Tensor) -> Tensor:
        concat = torch.cat([feature, action], dim=-1)
        return self.forward_model(concat)

    def predict_action(self, feature: Tensor, next_feature: Tensor) -> Tensor:
        concat = torch.cat([feature, next_feature], dim=-1)
        return self.inverse_head(concat)


class AdaptiveBufferScheduler:
    """Tracks appropriate synthetic sampling ratio."""

    def __init__(self, cfg: AdaptiveBufferSchedulerConfig) -> None:
        self.cfg = cfg
        self.current_ratio: float = cfg.min_ratio

    def update(self, model_error: float) -> None:
        if model_error < self.cfg.target_model_error:
            self.current_ratio = min(self.cfg.max_ratio, self.current_ratio + self.cfg.adjustment_rate)
        else:
            self.current_ratio = max(self.cfg.min_ratio, self.current_ratio - self.cfg.adjustment_rate)


class ModelRolloutBuffer:
    """Stores synthetic rollouts to sample during policy updates."""

    def __init__(self, capacity: int) -> None:
        self.capacity = capacity
        self.storage: deque[TensorDict] = deque(maxlen=capacity)

    def add(self, traj: TensorDict) -> None:
        self.storage.append(traj.cpu())

    def __len__(self) -> int:
        return len(self.storage)

    def sample(self, batch_size: int, device: torch.device) -> TensorDict | None:
        if not self.storage:
            return None
        actual = min(batch_size, len(self.storage))
        idx = random.sample(range(len(self.storage)), actual)
        trajs = [self.storage[i] for i in idx]
        stacked = _stack_trajectories(trajs).to(device=device)
        return stacked


class CMPO(Loss):
    """Curiosity Model Policy Optimization loss."""

    def __init__(
        self,
        policy: Policy,
        trainer_cfg: Any,
        env: TrainingEnvironment,
        device: torch.device,
        instance_name: str,
        cfg: CMPOConfig,
    ) -> None:
        super().__init__(policy, trainer_cfg, env, device, instance_name, cfg)
        self.cfg: CMPOConfig = cfg

        if hasattr(self.policy, "burn_in_steps"):
            self.burn_in_steps = self.policy.burn_in_steps
        else:
            self.burn_in_steps = 0
        self.burn_in_steps_iter = 0

        obs_space = env.single_observation_space
        if not hasattr(obs_space, "shape"):
            raise ValueError("Environment observation space must define shape for CMPO.")
        self.obs_shape = tuple(int(dim) for dim in obs_space.shape)
        self.obs_dim = int(torch.tensor(self.obs_shape).prod().item())

        action_space = env.single_action_space
        if isinstance(action_space, gym_spaces.Discrete):
            self.action_dim = int(action_space.n)
            self._encode_action_fn = self._encode_discrete_action
            self._discrete_actions = True
            self._raw_action_dtype = torch.int64
            self._raw_action_shape = (1,)
        elif isinstance(action_space, gym_spaces.Box):
            self.action_dim = int(math.prod(action_space.shape))
            self._encode_action_fn = self._encode_box_action
            self._discrete_actions = False
            self._raw_action_dtype = torch.float32
            self._raw_action_shape = tuple(int(dim) for dim in action_space.shape)
        else:  # pragma: no cover - other spaces currently unsupported.
            raise NotImplementedError(f"Unsupported action space for CMPO: {action_space}")

        self.world_model = WorldModelEnsemble(cfg.world_model, self.obs_dim, self.action_dim).to(device)
        self.world_model_opt = torch.optim.Adam(self.world_model.parameters(), lr=cfg.world_model.learning_rate)

        self.curiosity_module = CuriosityModule(
            state_dim=self.obs_dim,
            action_dim=self.action_dim,
            cfg=cfg.curiosity,
        ).to(device)
        self.curiosity_opt = torch.optim.Adam(self.curiosity_module.parameters(), lr=cfg.curiosity.learning_rate)

        self.transition_buffer = TransitionBuffer(cfg.world_model.buffer_size)
        self.model_buffer = ModelRolloutBuffer(cfg.synthetic_buffer.capacity)
        self.scheduler = AdaptiveBufferScheduler(cfg.scheduler)

        self.curiosity_weight = cfg.curiosity.weight_initial
        self._last_model_error = 1.0

        self._prev_obs: Tensor | None = None
        self._prev_action_enc: Tensor | None = None
        self._prev_action_raw: Tensor | None = None
        self._prev_done: Tensor | None = None
        self._has_prev: Tensor | None = None

        self.register_state_attr(
            "curiosity_weight",
            "_last_model_error",
        )

    # ------------------------------------------------------------------
    # Loss interface overrides
    # ------------------------------------------------------------------
    def attach_replay_buffer(self, experience: Experience) -> None:  # type: ignore[override]
        super().attach_replay_buffer(experience)
        segments = experience.segments
        device = self.device
        self._prev_obs = torch.zeros((segments, self.obs_dim), dtype=torch.float32, device=device)
        self._prev_action_enc = torch.zeros((segments, self.action_dim), dtype=torch.float32, device=device)
        self._prev_action_raw = torch.zeros(
            (segments,) + (self._raw_action_shape or (1,)),
            dtype=self._raw_action_dtype,
            device=device,
        )
        self._prev_done = torch.ones((segments,), dtype=torch.bool, device=device)
        self._has_prev = torch.zeros((segments,), dtype=torch.bool, device=device)

    def get_experience_spec(self) -> Composite:
        action_space = self.env.single_action_space
        if isinstance(action_space, gym_spaces.Discrete):
            action_spec = UnboundedDiscrete(shape=torch.Size([]), dtype=torch.int32)
        else:
            action_spec = UnboundedContinuous(
                shape=torch.Size(tuple(int(dim) for dim in action_space.shape)),
                dtype=torch.float32,
            )
        scalar_f32 = UnboundedContinuous(shape=torch.Size([]), dtype=torch.float32)
        return Composite(
            actions=action_spec,
            values=scalar_f32,
            rewards=scalar_f32,
            dones=scalar_f32,
            truncateds=scalar_f32,
            act_log_prob=scalar_f32,
            intrinsic_rewards=scalar_f32,
            extrinsic_rewards=scalar_f32,
        )

    # ------------------------------------------------------------------
    # Helper utilities
    # ------------------------------------------------------------------
    def _flatten_obs(self, obs: Tensor) -> Tensor:
        obs_f = obs.to(dtype=torch.float32)
        if obs_f.max() > 1.0:
            obs_f = obs_f / 255.0
        return obs_f.view(obs.shape[0], -1)

    def _unflatten_obs(self, obs_flat: Tensor) -> Tensor:
        obs = obs_flat.view(obs_flat.shape[0], *self.obs_shape)
        obs = (obs * 255.0).clamp(0, 255)
        return obs.to(dtype=torch.uint8)

    def _encode_discrete_action(self, action: Tensor) -> Tensor:
        action = action.view(-1).long()
        return F.one_hot(action, num_classes=self.action_dim).to(dtype=torch.float32)

    def _encode_box_action(self, action: Tensor) -> Tensor:
        return action.view(action.shape[0], -1).to(dtype=torch.float32)

    def _encode_action(self, action: Tensor) -> Tensor:
        return self._encode_action_fn(action)

    def _world_model_predict(self, state: Tensor, action: Tensor) -> tuple[Tensor, Tensor, Tensor, Tensor]:
        next_state_mean, reward_mean = self.world_model(state, action)
        next_state_var, reward_var = self.world_model.variance(state, action)
        return next_state_mean, reward_mean, next_state_var.mean(dim=-1), reward_var

    def _compute_intrinsic_reward(
        self,
        state: Tensor,
        action: Tensor,
        next_state: Tensor,
        reward: Tensor,
        pred_next: Tensor,
        pred_reward: Tensor,
    ) -> Tensor:
        cfg = self.cfg.curiosity
        with torch.no_grad():
            feature = self.curiosity_module.encode(state)
            next_feature = self.curiosity_module.encode(next_state)
            pred_feature = self.curiosity_module.predict_feature(feature, action)
            feature_error = F.mse_loss(pred_feature, next_feature, reduction="none").mean(dim=-1)

            reward_diff = reward - pred_reward
            positive_mask = (reward_diff >= 0).float()
            negative_mask = (reward_diff < 0).float()

            positive_curiosity = feature_error * positive_mask
            negative_curiosity = feature_error * negative_mask

            intrinsic = self.curiosity_weight * (
                cfg.positive_coef * positive_curiosity - cfg.negative_coef * negative_curiosity
            )
            intrinsic = intrinsic.clamp(-cfg.reward_clip, cfg.reward_clip)
        return intrinsic

    def _update_curiosity_weight(self, model_error: float) -> None:
        cfg = self.cfg.curiosity
        if model_error < self.cfg.scheduler.target_model_error:
            self.curiosity_weight = min(cfg.weight_max, self.curiosity_weight + cfg.adjustment_rate)
        else:
            self.curiosity_weight = max(cfg.weight_min, self.curiosity_weight - cfg.adjustment_rate)

    # ------------------------------------------------------------------
    # Training helpers
    # ------------------------------------------------------------------
    def _train_world_model(self) -> float:
        cfg = self.cfg.world_model
        if len(self.transition_buffer) < cfg.warmup_transitions:
            return self._last_model_error

        total_loss = 0.0
        steps = 0
        for _ in range(cfg.train_steps):
            batch = self.transition_buffer.sample(cfg.batch_size, self.device)
            if batch is None:
                break
            states = batch["state"]
            actions = batch["action_enc"]
            rewards = batch["reward"]
            targets = batch["next_state"]

            self.world_model_opt.zero_grad()
            pred_next, pred_reward = self.world_model(states, actions)
            loss_state = F.mse_loss(pred_next, targets)
            loss_reward = F.mse_loss(pred_reward, rewards)
            loss = loss_state + loss_reward
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.world_model.parameters(), cfg.gradient_clip)
            self.world_model_opt.step()

            total_loss += float(loss.item())
            steps += 1

        if steps == 0:
            return self._last_model_error
        avg_loss = total_loss / steps
        self._last_model_error = avg_loss
        self.loss_tracker["world_model_loss"].append(avg_loss)
        return avg_loss

    def _train_curiosity_model(self) -> None:
        cfg = self.cfg.curiosity
        if len(self.transition_buffer) < self.cfg.world_model.warmup_transitions:
            return

        for _ in range(cfg.train_steps):
            batch = self.transition_buffer.sample(cfg.feature_dim * 2, self.device)
            if batch is None:
                break
            states = batch["state"]
            actions_enc = batch["action_enc"]
            actions_raw = batch["action_raw"]
            next_states = batch["next_state"]

            self.curiosity_opt.zero_grad()
            feature = self.curiosity_module.encode(states)
            next_feature = self.curiosity_module.encode(next_states)
            pred_next_feature = self.curiosity_module.predict_feature(feature, actions_enc)
            forward_loss = F.mse_loss(pred_next_feature, next_feature)

            action_pred = self.curiosity_module.predict_action(feature, next_feature)
            if self._discrete_actions:
                target = actions_raw.view(-1).long()
                action_loss = F.cross_entropy(action_pred, target)
            else:
                action_loss = F.mse_loss(action_pred, actions_enc)

            loss = forward_loss + action_loss
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.curiosity_module.parameters(), 1.0)
            self.curiosity_opt.step()

            self.loss_tracker["curiosity_forward_loss"].append(float(forward_loss.item()))
            self.loss_tracker["curiosity_inverse_loss"].append(float(action_loss.item()))

    def _generate_model_rollouts(self) -> None:
        cfg = self.cfg.model_rollouts
        if len(self.transition_buffer) < self.cfg.world_model.warmup_transitions:
            return

        starts = self.transition_buffer.sample(cfg.batch_size, self.device)
        if starts is None:
            return

        start_states = starts["next_state"]
        start_obs = self._unflatten_obs(start_states)
        trajectories: list[TensorDict] = []
        for idx in range(start_states.shape[0]):
            traj = self._simulate_rollout(
                state=start_states[idx : idx + 1],
                obs=start_obs[idx : idx + 1],
                max_length=cfg.rollout_length,
            )
            if traj is not None:
                trajectories.append(traj)

        if trajectories:
            for traj in trajectories:
                self.model_buffer.add(traj)

    def _simulate_rollout(self, state: Tensor, obs: Tensor, max_length: int) -> TensorDict | None:
        device = state.device
        self.policy.reset_memory()
        values: list[Tensor] = []
        rewards: list[Tensor] = []
        extrinsic: list[Tensor] = []
        intrinsic: list[Tensor] = []
        actions_list: list[Tensor] = []
        logprob_list: list[Tensor] = []
        entropy_list: list[Tensor] = []
        dones_list: list[Tensor] = []
        obs_list: list[Tensor] = []

        current_state = state
        current_obs = obs
        done_flag = torch.zeros((1,), dtype=torch.float32, device=device)

        for _ in range(max_length):
            obs_list.append(current_obs.squeeze(0))
            policy_input = TensorDict(
                {
                    "env_obs": current_obs.to(device=device, dtype=torch.uint8),
                    "batch": torch.ones(1, dtype=torch.long, device=device),
                    "bptt": torch.ones(1, dtype=torch.long, device=device),
                },
                batch_size=(1,),
            )
            with torch.no_grad():
                policy_td = self.policy.forward(policy_input)
            action = policy_td["actions"]
            actions_list.append(action.squeeze(0))
            logprob_list.append(policy_td["act_log_prob"].squeeze(0))
            entropy_list.append(policy_td.get("entropy", torch.zeros_like(policy_td["act_log_prob"])).squeeze(0))
            values.append(policy_td["values"].squeeze(0))

            action_enc = self._encode_action(action)
            next_state, pred_reward, state_var, reward_var = self._world_model_predict(current_state, action_enc)
            next_obs = self._unflatten_obs(next_state)

            state_uncertainty = state_var.mean(dim=-1)
            reward_uncertainty = reward_var.squeeze(-1)
            intrinsic_reward = self.curiosity_weight * (
                self.cfg.curiosity.positive_coef * state_uncertainty
                - self.cfg.curiosity.negative_coef * reward_uncertainty
            )
            intrinsic_reward = intrinsic_reward.clamp(-self.cfg.curiosity.reward_clip, self.cfg.curiosity.reward_clip)
            total_reward = pred_reward + intrinsic_reward
            rewards.append(total_reward.squeeze(0))
            extrinsic.append(pred_reward.squeeze(0))
            intrinsic.append(intrinsic_reward.squeeze(0))
            dones_list.append(done_flag.squeeze(0))

            if reward_var.mean().item() > self.cfg.model_rollouts.termination_variance_threshold:
                break

            current_state = next_state
            current_obs = next_obs

        if not rewards:
            return None

        traj_len = len(rewards)
        traj = TensorDict(
            {
                "env_obs": torch.stack(obs_list, dim=0),
                "actions": torch.stack(actions_list, dim=0),
                "act_log_prob": torch.stack(logprob_list, dim=0),
                "entropy": torch.stack(entropy_list, dim=0),
                "values": torch.stack(values, dim=0),
                "rewards": torch.stack(rewards, dim=0),
                "extrinsic_rewards": torch.stack(extrinsic, dim=0),
                "intrinsic_rewards": torch.stack(intrinsic, dim=0),
                "dones": torch.stack(dones_list, dim=0),
                "truncateds": torch.zeros(traj_len, dtype=torch.float32, device=device),
            },
            batch_size=(traj_len,),
        )

        advantages = torch.zeros_like(traj["values"])
        adv_cfg = self.trainer_cfg.advantage
        advantages = compute_advantage(
            traj["values"],
            traj["rewards"],
            traj["dones"],
            torch.ones_like(traj["values"]),
            advantages,
            adv_cfg.gamma,
            adv_cfg.gae_lambda,
            self.device,
            adv_cfg.vtrace_rho_clip,
            adv_cfg.vtrace_c_clip,
        )
        traj.set("advantages", advantages)
        traj.set("returns", advantages + traj["values"])
        return traj.detach()

    def _sample_model_batch(self, segments: int, device: torch.device) -> TensorDict | None:
        if segments <= 0:
            return None
        model_batch = self.model_buffer.sample(segments, device=device)
        if model_batch is None:
            return None

        policy_td = forward_policy_for_training(self.policy, model_batch, self.policy_experience_spec)
        model_batch.set("policy_td", policy_td)
        return model_batch

    def _importance_ratio(self, new_logprob: Tensor, old_logprob: Tensor) -> Tensor:
        logratio = torch.clamp(new_logprob - old_logprob, -10, 10)
        return logratio.exp()

    def _compute_ppo_losses(
        self,
        *,
        new_logprob: Tensor,
        old_logprob: Tensor,
        entropy: Tensor,
        new_values: Tensor,
        old_values: Tensor,
        returns: Tensor,
        advantages: Tensor,
        ratio: Tensor,
    ) -> tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
        pg_loss1 = -advantages * ratio
        pg_loss2 = -advantages * torch.clamp(ratio, 1 - self.cfg.clip_coef, 1 + self.cfg.clip_coef)
        pg_loss = torch.max(pg_loss1, pg_loss2).mean()

        entropy_loss = entropy.mean()

        if self.cfg.clip_vloss:
            v_loss_unclipped = (new_values - returns) ** 2
            vf_clip_coef = self.cfg.vf_clip_coef
            v_clipped = old_values + torch.clamp(new_values - old_values, -vf_clip_coef, vf_clip_coef)
            v_loss_clipped = (v_clipped - returns) ** 2
            v_loss = 0.5 * torch.max(v_loss_unclipped, v_loss_clipped).mean()
        else:
            v_loss = 0.5 * ((new_values - returns) ** 2).mean()

        with torch.no_grad():
            logratio = new_logprob - old_logprob
            approx_kl = ((ratio - 1) - logratio).mean()
            clipfrac = ((ratio - 1.0).abs() > self.cfg.clip_coef).float().mean()

        return pg_loss, v_loss, entropy_loss, approx_kl, clipfrac

    def _track(self, key: str, value: Tensor) -> None:
        self.loss_tracker[key].append(float(value.item()))

    def _policy_loss(
        self,
        minibatch: TensorDict,
        policy_td: TensorDict,
        advantages: Tensor,
        *,
        prio_weights: Optional[Tensor] = None,
        update_indices: Optional[Tensor] = None,
        prefix: str = "",
        track_importance: bool = True,
    ) -> Tensor:
        old_logprob = minibatch["act_log_prob"]
        new_logprob = policy_td["act_log_prob"].reshape(old_logprob.shape)
        entropy = policy_td["entropy"]

        old_values = minibatch["values"]
        new_values = policy_td["values"].view(old_values.shape)
        values_for_adv = old_values
        new_values_for_loss = new_values
        if hasattr(self.policy, "critic_quantiles"):
            values_for_adv = old_values.mean(dim=-1)
            new_values_for_loss = new_values.mean(dim=-1)

        ratio = self._importance_ratio(new_logprob, old_logprob)

        adv_cfg = self.trainer_cfg.advantage
        vtrace_adv = compute_advantage(
            values_for_adv,
            minibatch["rewards"],
            minibatch["dones"],
            ratio,
            torch.zeros_like(values_for_adv),
            adv_cfg.gamma,
            adv_cfg.gae_lambda,
            self.device,
            adv_cfg.vtrace_rho_clip,
            adv_cfg.vtrace_c_clip,
        )
        vtrace_adv = normalize_advantage_distributed(vtrace_adv, self.cfg.norm_adv)
        if prio_weights is not None:
            vtrace_adv = prio_weights * vtrace_adv

        returns = advantages + values_for_adv

        pg_loss, v_loss, entropy_loss, approx_kl, clipfrac = self._compute_ppo_losses(
            new_logprob=new_logprob,
            old_logprob=old_logprob,
            entropy=entropy,
            new_values=new_values_for_loss,
            old_values=values_for_adv,
            returns=returns,
            advantages=vtrace_adv,
            ratio=ratio,
        )
        v_loss = v_loss * self.cfg.vf_coef

        if update_indices is not None:
            update_td = TensorDict(
                {
                    "ratio": ratio.detach(),
                    "values": new_values.view(old_values.shape).detach(),
                },
                batch_size=minibatch.batch_size,
            )
            self.replay.update(update_indices, update_td)

        metrics: dict[str, Tensor] = {
            "policy_loss": pg_loss,
            "value_loss": v_loss,
            "entropy": entropy_loss,
            "approx_kl": approx_kl,
            "clipfrac": clipfrac,
        }
        if track_importance:
            metrics["importance"] = ratio.mean()
            metrics["current_logprobs"] = new_logprob.mean()
        for key, value in metrics.items():
            name = f"{prefix}{key}" if prefix else key
            self._track(name, value)

        loss = pg_loss - self.cfg.ent_coef * entropy_loss + v_loss
        return add_dummy_loss_for_unused_params(
            loss,
            td=policy_td,
            used_keys=["act_log_prob", "entropy", "values"],
        )

    def _process_minibatch_update(
        self,
        minibatch: TensorDict,
        policy_td: TensorDict,
        indices: Tensor,
        prio_weights: Tensor,
        advantages: Tensor,
    ) -> Tensor:
        return self._policy_loss(
            minibatch,
            policy_td,
            advantages,
            prio_weights=prio_weights,
            update_indices=indices,
        )

    def _process_model_batch(self, model_batch: TensorDict) -> Tensor:
        return self._policy_loss(
            model_batch,
            model_batch["policy_td"],
            model_batch["advantages"],
            prefix="model_",
            track_importance=False,
        )

    # ------------------------------------------------------------------
    # Core rollout/training loop overrides
    # ------------------------------------------------------------------
    def run_rollout(self, td: TensorDict, context: ComponentContext) -> None:
        with torch.no_grad():
            if "actions" in td.keys():
                self.policy.forward(td, action=td["actions"])
            else:
                self.policy.forward(td)

        env_slice = context.training_env_id
        if env_slice is None:
            raise RuntimeError("ComponentContext.training_env_id is required for CMPO rollout")

        rewards = td["rewards"].to(dtype=torch.float32)
        dones = td["dones"].to(dtype=torch.float32)
        truncateds = td["truncateds"].to(dtype=torch.float32)
        terminals = torch.logical_or(dones > 0.5, truncateds > 0.5)

        obs = td["env_obs"]
        obs_flat = self._flatten_obs(obs)
        actions = td["actions"]
        actions_enc = self._encode_action(actions)
        actions_raw = actions.to(dtype=self._raw_action_dtype).view(actions.shape[0], *self._raw_action_shape)

        intrinsic_rewards = torch.zeros_like(rewards)
        extrinsic_rewards = rewards.clone()

        if self._has_prev is not None and self._has_prev[env_slice].any():
            mask = self._has_prev[env_slice]
            prev_states = self._prev_obs[env_slice][mask]
            prev_actions_enc = self._prev_action_enc[env_slice][mask]
            prev_actions_raw = self._prev_action_raw[env_slice][mask]
            prev_dones = self._prev_done[env_slice][mask].float()

            current_states = obs_flat[mask]
            current_rewards = rewards[mask]

            pred_next, pred_reward, _, _ = self._world_model_predict(prev_states, prev_actions_enc)
            intrinsic = self._compute_intrinsic_reward(
                prev_states,
                prev_actions_enc,
                current_states,
                current_rewards,
                pred_next,
                pred_reward,
            )
            intrinsic_rewards[mask] = intrinsic
            total_rewards = current_rewards + intrinsic
            td["rewards"][mask] = total_rewards

            self.transition_buffer.add_batch(
                states=prev_states.detach(),
                actions_enc=prev_actions_enc.detach(),
                actions_raw=prev_actions_raw.detach(),
                rewards=current_rewards.detach(),
                next_states=current_states.detach(),
                dones=prev_dones.detach(),
            )

        td["intrinsic_rewards"] = intrinsic_rewards
        td["extrinsic_rewards"] = extrinsic_rewards

        if self.burn_in_steps_iter < self.burn_in_steps:
            self.burn_in_steps_iter += 1
        else:
            self.replay.store(data_td=td, env_id=env_slice)

        if self._prev_obs is None or self._prev_action_enc is None or self._prev_action_raw is None:
            raise RuntimeError("CMPO replay buffers not initialized before rollout")

        self._prev_obs[env_slice] = obs_flat.detach()
        self._prev_action_enc[env_slice] = actions_enc.detach()
        self._prev_action_raw[env_slice] = actions_raw.detach()
        done_flags = terminals
        self._prev_done[env_slice] = done_flags
        self._has_prev[env_slice] = ~done_flags.bool()

    def run_train(
        self,
        shared_loss_data: TensorDict,
        context: ComponentContext,
        mb_idx: int,
    ) -> tuple[Tensor, TensorDict, bool]:
        stop_update_epoch = False
        if mb_idx > 0 and self.cfg.target_kl is not None:
            if self.loss_tracker["approx_kl"]:
                avg_kl = sum(self.loss_tracker["approx_kl"]) / len(self.loss_tracker["approx_kl"])
            else:
                avg_kl = 0.0
            if avg_kl > self.cfg.target_kl:
                stop_update_epoch = True

        if mb_idx == 0:
            model_error = self._train_world_model()
            self._train_curiosity_model()
            self._update_curiosity_weight(model_error)
            self.scheduler.update(model_error)
            self._generate_model_rollouts()

        minibatch = shared_loss_data["sampled_mb"]
        if minibatch.batch_size.numel() == 0:  # early exit if minibatch is empty
            return self._zero(), shared_loss_data, stop_update_epoch

        policy_td = shared_loss_data["policy_td"]
        indices = shared_loss_data["indices"][:, 0]
        prio_weights = shared_loss_data["prio_weights"]
        advantages = shared_loss_data["advantages"]

        real_loss = self._process_minibatch_update(
            minibatch=minibatch,
            policy_td=policy_td,
            indices=indices,
            prio_weights=prio_weights,
            advantages=advantages,
        )

        model_ratio = self.scheduler.current_ratio
        model_loss = torch.tensor(0.0, device=self.device)
        if model_ratio > 0:
            synthetic_segments = max(1, int(self.replay.minibatch_segments * model_ratio))
            model_batch = self._sample_model_batch(synthetic_segments, self.device)
            if model_batch is not None:
                model_loss = self._process_model_batch(model_batch)

        total_loss = (1 - model_ratio) * real_loss + model_ratio * model_loss
        return total_loss, shared_loss_data, stop_update_epoch
