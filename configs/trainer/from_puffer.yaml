_target_: metta.rl.pufferlib.trainer.PufferTrainer

defaults:
  - trainer
  - _self_

resume: true
use_e3b: false

# From [train] section in metta.ini
total_timesteps: 1_000_000_000_000
learning_rate: 0.0013848535655657842
gamma: 0.9959746852829785
gae_lambda: 0.9283720217357007
ent_coef: 0.0008901028045115906
max_grad_norm: 1.2453426220454547
vf_coef: 1.0103141121889738
anneal_lr: false
batch_size: 524288
minibatch_size: 32768

# Adam optimizer settings from metta.ini
optimizer:
  type: adam
  beta1: 0.8597471085735918
  beta2: 0.9998113998134229
  eps: 0.000249501214984291
  learning_rate: 0.0013848535655657842
  weight_decay: 0

# Other settings
clip_coef: 0.1
vf_clip_coef: 0.1
l2_reg_loss_coef: 0
l2_init_loss_coef: 0

norm_adv: true
clip_vloss: true
target_kl: null

zero_copy: true
require_contiguous_env_ids: false
verbose: true

bptt_horizon: 64
update_epochs: 1

cpu_offload: false
compile: false
compile_mode: reduce-overhead
profiler_interval_epochs: 10000

forward_pass_minibatch_target_size: 2048
async_factor: 2

lr_scheduler:
  enabled: false

stats:
  overview:
    episode/reward.mean: episode_reward
  step: train/agent_step

kickstart:
  teacher_uri: null
  action_loss_coef: 1
  value_loss_coef: 1
  kickstart_steps: 50_000_000
  additional_teachers: []
