# Sweep configuration for comparing Learning Progress weights
# This sweep compares LP_weight = 0, 0.3, 0.5, 1.0 for LowRewardCurriculum

trainer:
  curriculum: env/mettagrid/curriculum/lp_low_reward
  total_timesteps: 1000000  # 1M timesteps for comparison

  # Fixed hyperparameters for fair comparison
  gamma: 0.99
  gae_lambda: 0.95
  vf_coef: 0.5
  ent_coef: 0.01
  batch_size: 512
  minibatch_size: 256
  bptt_horizon: 8
  learning_rate: 3e-4

# LP weight comparison - this will create 4 different runs
# 0 = no learning progress (baseline LowRewardCurriculum)
# 0.3 = light learning progress influence
# 0.5 = moderate learning progress influence
# 1.0 = full learning progress influence
trainer.curriculum.lp_weight: ${ss:choice, 0.0, 0.3, 0.5, 1.0}

# Environment configuration
trainer.curriculum.env_cfg:
  envs:
    - /env/mettagrid/navigation/training/terrain_from_numpy
    - /env/mettagrid/navigation/training/cylinder_world
    - /env/mettagrid/navigation/training/varied_terrain_sparse
    - /env/mettagrid/navigation/training/varied_terrain_balanced
    - /env/mettagrid/navigation/training/varied_terrain_maze
    - /env/mettagrid/navigation/training/varied_terrain_dense
  num_agents: 24

# LPC-specific parameters (only used when lp_weight > 0)
trainer.curriculum.ema_alpha: 0.001
trainer.curriculum.p_theta: 0.05
trainer.curriculum.num_active_tasks: 16
trainer.curriculum.rand_task_rate: 0.25
trainer.curriculum.sample_threshold: 10
trainer.curriculum.memory: 25
trainer.curriculum.lp_metric: "episode/reward.mean"

# LowRewardCurriculum parameters
trainer.curriculum.low_reward_threshold: 0.1
trainer.curriculum.min_tasks_per_env: 2
trainer.curriculum.max_tasks_per_env: 8
