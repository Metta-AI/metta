# SkyPilot Launch Configuration for Learning Progress Training
# Run with: sky launch configs/sweep/learning_progress_launch.yaml

name: learning-progress-training

resources:
  # Single node with multiple GPUs
  accelerators: L4:2  # 2 L4 GPUs on one node
  # Alternative configurations:
  # accelerators: L4:4  # 4 L4 GPUs
  # accelerators: A100:2  # 2 A100 GPUs
  # accelerators: {L4:1, A100:1}  # Mixed GPU types

setup: |
  # Setup commands
  echo "Setting up learning progress training environment..."

  # Install dependencies
  pip install -r requirements.txt

  # Setup distributed environment variables
  export WORLD_SIZE=2  # Number of GPUs
  export MASTER_ADDR=localhost
  export MASTER_PORT=29500

run: |
  # Main training script
  echo "Starting learning progress training with $WORLD_SIZE GPUs..."

  # Run distributed training with PyTorch DDP
  python -m torch.distributed.launch \
    --nproc_per_node=$WORLD_SIZE \
    --master_port=$MASTER_PORT \
    experiments/user/arena_lp_test.py \
    --distributed \
    --num_episodes=2000 \
    --learning_progress=True \
    --wandb_project=metta \
    --wandb_run_name=msb_lpdehyd_multi_gpu
