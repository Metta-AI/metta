resources:
  cloud: aws
  any_of:
    - region: us-east-1
      use_spot: true
      accelerators: "L4:1"
    - region: us-east-1
      use_spot: true
      accelerators: "A10G:1"
    - region: us-east-1
      accelerators: "L4:1"
    - region: us-east-1
      accelerators: "A10G:1"
    - region: us-west-2
      use_spot: true
      accelerators: "L4:1"
    - region: us-west-2
      use_spot: true
      accelerators: "A10G:1"
    - region: us-west-2
      accelerators: "L4:1"
    - region: us-west-2
      accelerators: "A10G:1"
  cpus: 8+
  image_id: docker:metta:latest
  job_recovery:
    strategy: EAGER_NEXT_REGION
    max_restarts_on_errors: 20

config:
  docker:
    run_options:
      - --cap-add=IPC_LOCK
      - --ulimit memlock=-1:-1
      - --shm-size=4g

file_mounts:
  /mnt/s3/softmax-public:
    source: s3://softmax-public
    mode: MOUNT_CACHED
  /mnt/s3/train_dir:
    source: s3://softmax-train-dir
    mode: MOUNT_CACHED

setup: |
  set -e
  cd /workspace/metta

  # the docker image may start already in a venv!

  # Activate the metta venv
  if [ -n "$VIRTUAL_ENV" ]; then
      deactivate 2>/dev/null || true
  fi
  . .venv/bin/activate

  git fetch --depth=1000 origin "$METTA_GIT_REF" || git fetch origin
  git checkout "$METTA_GIT_REF"
  echo "Checked out: $(git rev-parse HEAD)"

  # Python environment setup
  echo "Setting up Python environment..."
  uv sync

  # Create required directories
  mkdir -p "$WANDB_DIR"

  echo "METTA_CMD_ARGS = $METTA_CMD_ARGS"
  echo "METTA_RUN_ID = $METTA_RUN_ID"
  echo "METTA_CMD = $METTA_CMD"
  echo "METTA_GIT_REF = $METTA_GIT_REF"
  echo "HEARTBEAT_TIMEOUT = $HEARTBEAT_TIMEOUT"

  # Note that different sets of skypilot environment variables are available in "run" vs "setup"
  # see https://docs.skypilot.co/en/latest/running-jobs/environment-variables.html

run: |
  set -euo pipefail
  cd /workspace/metta

  # Activate the metta venv
  if [ -n "$VIRTUAL_ENV" ]; then
      deactivate 2>/dev/null || true
  fi
  . .venv/bin/activate

  bash ./devops/skypilot/config/configure_environment.sh

  echo "METTA_RUN_ID: $METTA_RUN_ID"
  echo "SKYPILOT_TASK_ID: $SKYPILOT_TASK_ID"

  if [ -f common/src/metta/common/util/skypilot_latency.py ]; then
    echo "Collecting skypilot latency..."
    LATENCY_OUTPUT=$(uv run python common/src/metta/common/util/skypilot_latency.py 2>&1) || true
    echo "$LATENCY_OUTPUT"
  else
    echo "Latency script is missing!"
  fi

  METTA_ENV_FILE="$(uv run ./common/src/metta/common/util/constants.py METTA_ENV_FILE)"

  if [ -f common/src/metta/common/util/cost_monitor.py ]; then
    echo "Collecting instance cost..."
    if uv run python common/src/metta/common/util/cost_monitor.py; then
      echo "wrote METTA_HOURLY_COST to $METTA_ENV_FILE"
    else
      echo "Cost monitor script failed to run."
    fi
  else
    echo "Cost monitor script is missing!"
  fi

  # load environment vars written by child scripts

  source "$METTA_ENV_FILE"

  echo "Cluster configuration:"
  echo "  NUM_GPUS=$NUM_GPUS"
  echo "  NUM_NODES=$NUM_NODES"
  echo "  MASTER_ADDR=$MASTER_ADDR"
  echo "  NODE_INDEX=$NODE_INDEX"

  echo "METTA_CMD_ARGS = $METTA_CMD_ARGS"
  echo "METTA_RUN_ID = $METTA_RUN_ID"
  echo "METTA_CMD = $METTA_CMD"
  echo "METTA_GIT_REF = $METTA_GIT_REF"
  echo "HEARTBEAT_TIMEOUT = $HEARTBEAT_TIMEOUT"

  # Rendezvous (torch c10d store) — one fixed port
  export MASTER_PORT="${MASTER_PORT:-29501}"

  # NCCL data sockets — a small, explicit range
  export NCCL_PORT_RANGE="${NCCL_PORT_RANGE:-43000-43063}"

  # Make sockets unambiguous
  export NCCL_SOCKET_IFNAME="${NCCL_SOCKET_IFNAME:-enp39s0}"
  export NCCL_SOCKET_FAMILY="${NCCL_SOCKET_FAMILY:-AF_INET}"

  # Quiet by default; perf on; debug toggle via METTA_NCCL_DEBUG=1
  export TORCH_NCCL_ASYNC_ERROR_HANDLING="${TORCH_NCCL_ASYNC_ERROR_HANDLING:-1}"
  if [ "${METTA_NCCL_DEBUG:-0}" = "1" ]; then
    export NCCL_DEBUG="${NCCL_DEBUG:-INFO}"
    export NCCL_DEBUG_SUBSYS="${NCCL_DEBUG_SUBSYS:-ALL}"
    export CUDA_LAUNCH_BLOCKING=1
  else
    export NCCL_DEBUG="${NCCL_DEBUG:-VERSION}"
    unset NCCL_DEBUG_SUBSYS
    unset CUDA_LAUNCH_BLOCKING
  fi

  # Enable fast paths (still no IB)
  export NCCL_P2P_DISABLE="${NCCL_P2P_DISABLE:-0}"
  export NCCL_SHM_DISABLE="${NCCL_SHM_DISABLE:-0}"
  export NCCL_IB_DISABLE="${NCCL_IB_DISABLE:-1}"

  # Light socket parallelism + a few channels
  export NCCL_MIN_NCHANNELS="${NCCL_MIN_NCHANNELS:-4}"
  export NCCL_MAX_NCHANNELS="${NCCL_MAX_NCHANNELS:-8}"
  export NCCL_SOCKET_NTHREADS="${NCCL_SOCKET_NTHREADS:-2}"
  export NCCL_NSOCKS_PERTHREAD="${NCCL_NSOCKS_PERTHREAD:-4}"

  echo "Rendezvous: $MASTER_ADDR:$MASTER_PORT | IFACE=$NCCL_SOCKET_IFNAME AF=$NCCL_SOCKET_FAMILY PORT_RANGE=$NCCL_PORT_RANGE"
  echo "NCCL: DEBUG=${NCCL_DEBUG}${NCCL_DEBUG_SUBSYS:+/$NCCL_DEBUG_SUBSYS} P2P=$((1- ${NCCL_P2P_DISABLE:-0})) SHM=$((1- ${NCCL_SHM_DISABLE:-0})) IB=$((1- ${NCCL_IB_DISABLE:-1})) CH=${NCCL_MIN_NCHANNELS}-${NCCL_MAX_NCHANNELS}"

  ulimit -l
  mount | grep /dev/shm

  ip -o route get $MASTER_ADDR
  ip -o addr show enp39s0

  # Run comprehensive GPU diagnostics and NCCL tests
  echo "Running GPU diagnostics and NCCL tests..."
  if [ "$NUM_NODES" -gt 1 ] || [ "$NUM_GPUS" -gt 1 ]; then
      # For distributed training
      if ! uv run torchrun \
        --nproc_per_node=$NUM_GPUS \
        --nnodes=$NUM_NODES \
        --node_rank=$NODE_INDEX \
        --master_addr=$MASTER_ADDR \
        --master_port="$MASTER_PORT" \
        ./devops/skypilot/test_nccl.py; then
          echo "NCCL pre-flight check failed!"
          exit 1
      fi
  else
      # For single GPU
      if ! uv run python ./devops/skypilot/test_nccl.py; then
          echo "GPU pre-flight check failed!"
          # exit 1
      fi
  fi




  HEARTBEAT_FILE=${HEARTBEAT_FILE:-$WANDB_DIR/heartbeat.txt}

  # Start train.sh in background
  ./devops/"$METTA_CMD".sh run="$METTA_RUN_ID" $METTA_CMD_ARGS &
  TRAIN_PID=$!

  # Start heartbeat *watching the train PID*
  if [[ "${HEARTBEAT_TIMEOUT}" != "0" ]]; then
    echo "[INFO] Starting heartbeat monitor ${HEARTBEAT_TIMEOUT}s on $HEARTBEAT_FILE (pid=$TRAIN_PID)"
    python -m metta.common.util.heartbeat monitor "$HEARTBEAT_FILE" \
      --pid "$TRAIN_PID" --timeout "$HEARTBEAT_TIMEOUT" \
      >/dev/null 2>&1 &
    HEARTBEAT_PID=$!
  fi

  # Wait for training to finish, capture exit, then kill heartbeat cleanly
  wait "$TRAIN_PID"
  CMD_EXIT=$?

  if [[ -n "${HEARTBEAT_PID:-}" ]]; then
    kill "$HEARTBEAT_PID" 2>/dev/null || true
    wait "$HEARTBEAT_PID" 2>/dev/null || true
  fi

  # if a github token is provided, report training status back to github
  COMMIT_SHA=$(git rev-parse HEAD)
  GITHUB_REPOSITORY="$(uv run ./common/src/metta/common/util/constants.py METTA_GITHUB_REPO)"

  # Just export the variables and run the script
  if [ -n "${GITHUB_TOKEN:-}" ]; then
      echo "Posting status to GitHub…"

      # The script reads CMD_EXIT, COMMIT_SHA, and SKYPILOT_TASK_ID from environment
      uv run python ./devops/skypilot/post_commit_status.py
  else
      echo "GITHUB_TOKEN not set; skipping GitHub status update"
  fi

  echo "$METTA_CMD job complete."
  exit $CMD_EXIT

  echo "$METTA_CMD script exit code: $CMD_EXIT"

  echo "$METTA_CMD job complete."
  exit $CMD_EXIT

envs:
  METTA_RUN_ID: ""
  METTA_CMD: train
  METTA_CMD_ARGS: ""
  METTA_GIT_REF: main
  WANDB_DIR: ./wandb
  HEARTBEAT_TIMEOUT: 600

  # if a github token is provided, we will report training status back to the branch
  GITHUB_TOKEN: ""

  # s3 mount slows down uv, so we put DATA_DIR outside of /workspace/metta
  DATA_DIR: /mnt/s3/train_dir
  SKYPILOT_DOCKER_USERNAME: ""
  SKYPILOT_DOCKER_PASSWORD: ""
  SKYPILOT_DOCKER_SERVER: 751442549699.dkr.ecr.us-east-1.amazonaws.com

secrets:
  # configured by launch script based on local credentials
  WANDB_PASSWORD: ""
  OBSERVATORY_TOKEN: ""
