# PPO Algorithm Configuration

# Advantage estimation
gamma: 0.977  # Discount factor
gae_lambda: 0.916  # GAE lambda for advantage estimation

# Clipping parameters
clip_coef: 0.1  # PPO clip coefficient for policy loss
vf_clip_coef: 0.1  # Value function clipping coefficient
clip_vloss: true  # Whether to clip value loss

# Loss coefficients
ent_coef: 0.0021  # Entropy coefficient
vf_coef: 0.44  # Value function coefficient
l2_reg_loss_coef: 0  # L2 regularization coefficient
l2_init_loss_coef: 0  # L2 init loss coefficient

# Training parameters
update_epochs: 1  # Number of epochs to update the policy
norm_adv: true  # Normalize advantages
target_kl: null  # Target KL divergence for early stopping

# Gradient clipping
max_grad_norm: 0.5  # Maximum gradient norm for clipping

# V-trace parameters (for off-policy corrections)
vtrace:
  vtrace_rho_clip: 1.0  # Default: on-policy (no off-policy correction)
  vtrace_c_clip: 1.0    # Default: on-policy bootstrapping

# Prioritized experience replay parameters
prioritized_experience_replay:
  prio_alpha: 0.0  # Default to uniform sampling (0.0 = uniform, >0 = prioritized)
  prio_beta0: 0.6  # Initial importance sampling correction
