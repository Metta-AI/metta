resources:
  cloud: aws
  any_of:
    - region: us-east-1
      use_spot: true
      accelerators: "L4:1"
    - region: us-east-1
      use_spot: true
      accelerators: "A10G:1"
    - region: us-east-1
      accelerators: "L4:1"
    - region: us-east-1
      accelerators: "A10G:1"
    - region: us-west-2
      use_spot: true
      accelerators: "L4:1"
    - region: us-west-2
      use_spot: true
      accelerators: "A10G:1"
    - region: us-west-2
      accelerators: "L4:1"
    - region: us-west-2
      accelerators: "A10G:1"
  cpus: 8+
  image_id: docker:metta:latest
  job_recovery:
    strategy: EAGER_NEXT_REGION
    max_restarts_on_errors: 20

file_mounts:
  /mnt/s3/softmax-public:
    source: s3://softmax-public
    mode: MOUNT_CACHED
  /mnt/s3/train_dir:
    source: s3://softmax-train-dir
    mode: MOUNT_CACHED

setup: |
  set -e
  cd /workspace/metta

  # the docker image may start already in a venv!

  # Activate the metta venv
  if [ -n "$VIRTUAL_ENV" ]; then
      deactivate 2>/dev/null || true
  fi
  . .venv/bin/activate

  git fetch --depth=1000 origin "$METTA_GIT_REF" || git fetch origin
  git checkout "$METTA_GIT_REF"
  echo "Checked out: $(git rev-parse HEAD)"

  # Python environment setup
  echo "Setting up Python environment..."
  uv sync

  # Create required directories
  mkdir -p "$WANDB_DIR"

  echo "METTA_CMD_ARGS = $METTA_CMD_ARGS"
  echo "METTA_RUN_ID = $METTA_RUN_ID"
  echo "METTA_CMD = $METTA_CMD"
  echo "METTA_GIT_REF = $METTA_GIT_REF"
  echo "HEARTBEAT_TIMEOUT = $HEARTBEAT_TIMEOUT"

  # Note that different sets of skypilot environment variables are available in "run" vs "setup"
  # see https://docs.skypilot.co/en/latest/running-jobs/environment-variables.html

run: |
  set -euo pipefail
  cd /workspace/metta

  # Activate the metta venv
  if [ -n "$VIRTUAL_ENV" ]; then
      deactivate 2>/dev/null || true
  fi
  . .venv/bin/activate

  bash ./devops/skypilot/config/configure_environment.sh

  echo "METTA_RUN_ID: $METTA_RUN_ID"
  echo "SKYPILOT_TASK_ID: $SKYPILOT_TASK_ID"

  if [ -f common/src/metta/common/util/skypilot_latency.py ]; then
    echo "Collecting skypilot latency..."
    LATENCY_OUTPUT=$(uv run python common/src/metta/common/util/skypilot_latency.py 2>&1) || true
    echo "$LATENCY_OUTPUT"
  else
    echo "Latency script is missing!"
  fi

  METTA_ENV_FILE="$(uv run ./common/src/metta/common/util/constants.py METTA_ENV_FILE)"

  if [ -f common/src/metta/common/util/cost_monitor.py ]; then
    echo "Collecting instance cost..."
    if uv run python common/src/metta/common/util/cost_monitor.py; then
      echo "wrote METTA_HOURLY_COST to $METTA_ENV_FILE"
    else
      echo "Cost monitor script failed to run."
    fi
  else
    echo "Cost monitor script is missing!"
  fi

  # load environment vars written by child scripts

  source "$METTA_ENV_FILE"

  echo "Cluster configuration:"
  echo "  NUM_GPUS=$NUM_GPUS"
  echo "  NUM_NODES=$NUM_NODES"
  echo "  MASTER_ADDR=$MASTER_ADDR"
  echo "  NODE_INDEX=$NODE_INDEX"

  echo "METTA_CMD_ARGS = $METTA_CMD_ARGS"
  echo "METTA_RUN_ID = $METTA_RUN_ID"
  echo "METTA_CMD = $METTA_CMD"
  echo "METTA_GIT_REF = $METTA_GIT_REF"
  echo "HEARTBEAT_TIMEOUT = $HEARTBEAT_TIMEOUT"

  # Run comprehensive GPU diagnostics and NCCL tests
  echo "Running GPU diagnostics and NCCL tests..."
  if [ "$NUM_NODES" -gt 1 ] || [ "$NUM_GPUS" -gt 1 ]; then
      # For distributed training
      if ! uv run torchrun --nproc_per_node=$NUM_GPUS --nnodes=$NUM_NODES --node_rank=$NODE_INDEX --master_addr=$MASTER_ADDR ./devops/skypilot/test_nccl.py; then
          echo "NCCL pre-flight check failed!"
          exit 1
      fi
  else
      # For single GPU
      if ! uv run python ./devops/skypilot/test_nccl.py; then
          echo "GPU pre-flight check failed!"
          # exit 1
      fi
  fi

  echo "Running $METTA_CMD script for run=$METTA_RUN_ID with args $METTA_CMD_ARGS"

  if ./devops/"$METTA_CMD".sh run="$METTA_RUN_ID" $METTA_CMD_ARGS; then
    CMD_EXIT=0
  else
    CMD_EXIT=$?
  fi

  # if a github token is provided, report training status back to github
  COMMIT_SHA=$(git rev-parse HEAD)
  GITHUB_REPOSITORY="$(uv run ./common/src/metta/common/util/constants.py METTA_GITHUB_REPO)"

  # Just export the variables and run the script
  if [ -n "${GITHUB_TOKEN:-}" ]; then
      echo "Posting status to GitHubâ€¦"

      # The script reads CMD_EXIT, COMMIT_SHA, and SKYPILOT_TASK_ID from environment
      uv run python ./devops/skypilot/post_commit_status.py
  else
      echo "GITHUB_TOKEN not set; skipping GitHub status update"
  fi

  echo "$METTA_CMD job complete."
  exit $CMD_EXIT

  echo "$METTA_CMD script exit code: $CMD_EXIT"

  echo "$METTA_CMD job complete."
  exit $CMD_EXIT

envs:
  METTA_RUN_ID: ""
  METTA_CMD: train
  METTA_CMD_ARGS: ""
  METTA_GIT_REF: main
  WANDB_DIR: ./wandb
  HEARTBEAT_TIMEOUT: 600

  # if a github token is provided, we will report training status back to the branch
  GITHUB_TOKEN: ""

  # s3 mount slows down uv, so we put DATA_DIR outside of /workspace/metta
  DATA_DIR: /mnt/s3/train_dir
  SKYPILOT_DOCKER_USERNAME: ""
  SKYPILOT_DOCKER_PASSWORD: ""
  SKYPILOT_DOCKER_SERVER: 751442549699.dkr.ecr.us-east-1.amazonaws.com

secrets:
  # configured by launch script based on local credentials
  WANDB_PASSWORD: ""
  OBSERVATORY_TOKEN: ""
