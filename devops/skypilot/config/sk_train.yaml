resources:
  cloud: aws
  any_of:
    - region: us-east-1
      use_spot: true
      accelerators: "L4:1"
    - region: us-east-1
      use_spot: true
      accelerators: "A10G:1"
    - region: us-east-1
      accelerators: "L4:1"
    - region: us-east-1
      accelerators: "A10G:1"
    - region: us-west-2
      use_spot: true
      accelerators: "L4:1"
    - region: us-west-2
      use_spot: true
      accelerators: "A10G:1"
    - region: us-west-2
      accelerators: "L4:1"
    - region: us-west-2
      accelerators: "A10G:1"
  cpus: 8+
  image_id: docker:metta:latest
  job_recovery:
    strategy: EAGER_NEXT_REGION
    max_restarts_on_errors: 20

file_mounts:
  /mnt/s3/softmax-public:
    source: s3://softmax-public
    mode: MOUNT_CACHED
  /mnt/s3/train_dir:
    source: s3://softmax-train-dir
    mode: MOUNT_CACHED

setup: |
  set -e
  cd /workspace/metta

  echo "[SETUP] Fetching latest from origin..."
  git fetch origin "$METTA_GIT_REF" || git fetch --depth=1000 origin

  echo "[SETUP] Checking out ref: $METTA_GIT_REF"
  git checkout $METTA_GIT_REF

  uv sync
  mkdir -p $WANDB_DIR

  echo "[SETUP] Creating job secrets..."
  ./devops/skypilot/create_job_secrets.py \
    --wandb-password "$WANDB_PASSWORD" \
    --observatory-token "$OBSERVATORY_TOKEN"

  echo "[SETUP] Setup complete."

run: |
  set -euo pipefail

  cd /workspace/metta

  # Drop any preloaded venv; activate your own
  if [ -n "${VIRTUAL_ENV:-}" ]; then
    deactivate 2>/dev/null || true
  fi
  . .venv/bin/activate

  echo "[CONFIG] Run Configuration:"
  echo "  - METTA_RUN_ID: ${METTA_RUN_ID:-}"
  echo "  - SKYPILOT_TASK_ID: ${SKYPILOT_TASK_ID:-}"
  echo "  - HEARTBEAT_TIMEOUT: ${HEARTBEAT_TIMEOUT:-'NOT SET'}"
  echo "  - MAX_RUNTIME_HOURS: ${MAX_RUNTIME_HOURS:-'NOT SET'}"
  echo "  - METTA_CMD: ${METTA_CMD:-'NOT SET'}"
  echo "  - METTA_CMD_ARGS: ${METTA_CMD_ARGS:-'NOT SET'}"

  if [ -f common/src/metta/common/util/skypilot_latency.py ]; then
    echo "[RUN] Collecting skypilot latency..."
    uv run python common/src/metta/common/util/skypilot_latency.py || true
  else
    echo "[RUN] Latency script is missing!"
  fi

  export NUM_GPUS="${SKYPILOT_NUM_GPUS_PER_NODE:-1}"
  export NUM_NODES="${SKYPILOT_NUM_NODES:-1}"
  export MASTER_ADDR="$(echo "${SKYPILOT_NODE_IPS:-}" | head -n1)"
  export MASTER_PORT=8008
  export NODE_INDEX="${SKYPILOT_NODE_RANK:-0}"
  export NCCL_SHM_DISABLE=1

  METTA_ENV_FILE="$(uv run ./common/src/metta/common/util/constants.py METTA_ENV_FILE)"

  if [ -f common/src/metta/common/util/cost_monitor.py ]; then
    echo "[RUN] Collecting instance cost..."
    METTA_HOURLY_COST="$(uv run python common/src/metta/common/util/cost_monitor.py 2>/dev/null | tail -1 || true)"
    if [ -n "${METTA_HOURLY_COST:-}" ]; then
      echo "[RUN] METTA_HOURLY_COST set to: $METTA_HOURLY_COST in $METTA_ENV_FILE"
    else
      echo "[RUN] Cost monitor script failed to run or returned no value."
    fi
  else
    echo "[RUN] Cost monitor script is missing!"
  fi

  if [ -f "$METTA_ENV_FILE" ]; then
    source "$METTA_ENV_FILE"
  else
    echo "Warning: $METTA_ENV_FILE does not exist. Creating empty file."
    touch "$METTA_ENV_FILE"
  fi

  if [ -n "${GITHUB_PAT:-}" ] && [ -n "${GITHUB_REPOSITORY:-}" ] && [ -n "${METTA_GIT_REF:-}" ]; then
    ENABLE_GITHUB_STATUS=true
    echo "[RUN] GitHub status reporting is enabled"
    echo "[RUN] GITHUB_PAT: ${GITHUB_PAT:+<set>}"  # Don't print the actual token
    echo "[RUN] GITHUB_REPOSITORY: ${GITHUB_REPOSITORY:-<not set>}"
    echo "[RUN] METTA_GIT_REF: ${METTA_GIT_REF:-<not set>}"
    echo "[RUN] METTA_RUN_ID: ${METTA_RUN_ID:-<not set>}"

    GITHUB_STATUS_STATE=pending GITHUB_STATUS_DESCRIPTION="Queued on SkyPilotâ€¦" \
      uv run devops/skypilot/set_github_status.py || echo "[WARN] Pending status post failed; continuing"
  else
    ENABLE_GITHUB_STATUS=false
    echo "[RUN] GitHub status reporting is disabled (missing required credentials)"
  fi

  export HEARTBEAT_FILE=${HEARTBEAT_FILE:-$WANDB_DIR/heartbeat.txt}
  export TERMINATION_REASON=""  # Track how the job ended

  # Configurable intervals
  export TIMEOUT_CHECK_INTERVAL=${TIMEOUT_CHECK_INTERVAL:-30}
  export HEARTBEAT_CHECK_INTERVAL=${HEARTBEAT_CHECK_INTERVAL:-10}

  # Create a temp directory for IPC files
  export IPC_DIR="/tmp/metta_job_$$"
  mkdir -p "$IPC_DIR"
  export TERMINATION_REASON_FILE="$IPC_DIR/termination_reason"

  terminate_watchdogs() {
    if [[ -n "${HEARTBEAT_MONITOR_PID:-}" ]]; then
      kill "$HEARTBEAT_MONITOR_PID" 2>/dev/null || true
      wait "$HEARTBEAT_MONITOR_PID" 2>/dev/null || true
      echo "[INFO] Terminating heartbeat monitor"
    fi

    if [[ -n "${TIMEOUT_MONITOR_PID:-}" ]]; then
      kill "$TIMEOUT_MONITOR_PID" 2>/dev/null || true
      wait "$TIMEOUT_MONITOR_PID" 2>/dev/null || true
      echo "[INFO] Terminating timeout monitor"
    fi
  }

  terminate_process() {
    local pid=$1
    local reason=$2

    if kill -0 "$pid" 2>/dev/null; then
      echo "[INFO] Terminating process group $pid (reason: $reason)"
      echo "$reason" > "$TERMINATION_REASON_FILE"
      terminate_watchdogs

      # Kill the entire process group (negative PID)
      kill -TERM -"$pid" 2>/dev/null || true

      # Give process time to clean up
      local count=0
      while kill -0 "$pid" 2>/dev/null && [ $count -lt 5 ]; do
        sleep 1
        count=$((count + 1))
      done

      # Force kill if still running
      if kill -0 "$pid" 2>/dev/null; then
        echo "[WARN] Process group $pid didn't terminate gracefully, forcing kill"
        kill -KILL -"$pid" 2>/dev/null || true
      fi
    fi
  }

  run_cmd() {
    local START_TIME=$(date +%s)

    # Start training in its own process group with job control to create process groups
    set -m
    ./devops/"${METTA_CMD:?missing METTA_CMD}".sh run="${METTA_RUN_ID:?missing METTA_RUN_ID}" ${METTA_CMD_ARGS:-} &
    local CMD_PID=$!
    set +m

    echo "[INFO] Started training process group with PID: $CMD_PID"

    # Start timeout monitor if MAX_RUNTIME_HOURS is set
    if [[ -n "${MAX_RUNTIME_HOURS:-}" ]] && [[ "${MAX_RUNTIME_HOURS}" != "None" ]]; then
      (
        exec 2>&1
        max_seconds=$(awk "BEGIN {print ${MAX_RUNTIME_HOURS} * 3600}")
        echo "[INFO] Timeout monitor started - max runtime: ${MAX_RUNTIME_HOURS} hours (${max_seconds} seconds)"
        echo "[INFO] Checking every ${TIMEOUT_CHECK_INTERVAL} seconds"

        while true; do
          sleep "$TIMEOUT_CHECK_INTERVAL"

          # Check if main process is still alive
          if ! kill -0 "$CMD_PID" 2>/dev/null; then
            echo "[INFO] Timeout monitor: main process no longer running, exiting"
            break
          fi

          elapsed=$(($(date +%s) - START_TIME))
          remaining=$((max_seconds - elapsed))

          if [ $remaining -gt 0 ]; then
            elapsed_min=$((elapsed / 60))
            remaining_min=$((remaining / 60))
            echo "[INFO] Timeout Status: ${elapsed_min} minutes elapsed, ${remaining_min} minutes remaining (max: ${MAX_RUNTIME_HOURS}h)"
          else
            echo "[INFO] Timeout limit reached - terminating training process"
            # Use shared terminate function
            terminate_process "$CMD_PID" "max_runtime_reached"
            echo "[INFO] Timeout monitor exiting - process terminated"
            break
          fi
        done
      ) &
      TIMEOUT_MONITOR_PID=$!
      echo "[INFO] Started timeout monitor with PID: $TIMEOUT_MONITOR_PID"
    fi

    # Start heartbeat monitor if enabled
    if [[ "${HEARTBEAT_TIMEOUT}" != "0" ]]; then
      echo "[INFO] Starting heartbeat monitor ${HEARTBEAT_TIMEOUT}s on $HEARTBEAT_FILE"
      echo "[INFO] Checking every ${HEARTBEAT_CHECK_INTERVAL} seconds"

      (
        LAST_HEARTBEAT_TIME=$(date +%s)
        HEARTBEAT_COUNT=0

        while kill -0 "$CMD_PID" 2>/dev/null; do
          if [ -f "$HEARTBEAT_FILE" ]; then
            CURRENT_MTIME=$(stat -c %Y "$HEARTBEAT_FILE" 2>/dev/null || stat -f %m "$HEARTBEAT_FILE" 2>/dev/null || echo 0)
            CURRENT_TIME=$(date +%s)

            if [ "$CURRENT_MTIME" -gt "$LAST_HEARTBEAT_TIME" ]; then
              HEARTBEAT_COUNT=$((HEARTBEAT_COUNT + 1))
              LAST_HEARTBEAT_TIME=$CURRENT_MTIME

              # Print status every 10 heartbeats
              if [ $((HEARTBEAT_COUNT % 10)) -eq 0 ]; then
                echo "[INFO] Heartbeat received! (Total: $HEARTBEAT_COUNT heartbeats)"
              fi
            fi

            # Check if timeout exceeded
            if [ $((CURRENT_TIME - LAST_HEARTBEAT_TIME)) -gt "$HEARTBEAT_TIMEOUT" ]; then
              echo "[ERROR] Heartbeat timeout! No heartbeat for $HEARTBEAT_TIMEOUT seconds"
              # Use shared terminate function
              terminate_process "$CMD_PID" "heartbeat_timeout"
              break
            fi
          fi

          sleep "$HEARTBEAT_CHECK_INTERVAL"
        done

        echo "[INFO] Heartbeat monitor exiting"
      ) &
      HEARTBEAT_MONITOR_PID=$!
      echo "[INFO] Started heartbeat monitor with PID: $HEARTBEAT_MONITOR_PID"
    fi

    # Wait for training to finish
    wait "$CMD_PID"
    local CMD_EXIT=$?

    # Read termination reason from file if it exists
    if [ -f "$TERMINATION_REASON_FILE" ]; then
      TERMINATION_REASON=$(cat "$TERMINATION_REASON_FILE")
      echo "[INFO] Termination reason from monitor: $TERMINATION_REASON"
    fi

    terminate_watchdogs

    # Calculate total runtime
    local END_TIME=$(date +%s)
    local DURATION=$((END_TIME - START_TIME))
    echo "[SUMMARY] Total runtime: $DURATION seconds ($((DURATION / 60)) minutes)"

    return $CMD_EXIT
  }

  # Export the terminate function so it's available in subshells
  export TIMEOUT_MONITOR_PID=""
  export HEARTBEAT_MONITOR_PID=""
  export -f terminate_process
  export -f terminate_watchdogs

  # Run training without the timeout wrapper
  echo "[INFO] Starting training with watchdog monitors"
  run_cmd
  CMD_EXIT=$?

  # Check termination reason
  if [[ "${TERMINATION_REASON}" == "heartbeat_timeout" ]]; then
    echo "[ERROR] Job terminated due to heartbeat timeout"
    export GITHUB_STATUS_DESCRIPTION="Job failed - no heartbeat for ${HEARTBEAT_TIMEOUT} seconds"
    # Keep failure exit code
  elif [[ "${TERMINATION_REASON}" == "max_runtime_reached" ]]; then
    echo "[INFO] Job terminated due to max runtime limit"
    export GITHUB_STATUS_DESCRIPTION="Job ran successfully for ${MAX_RUNTIME_HOURS:-unknown} hours"
    # Map to success
    CMD_EXIT=0
  elif [[ $CMD_EXIT -eq 0 ]]; then
    echo "[SUCCESS] Job completed successfully"
    export GITHUB_STATUS_DESCRIPTION="Job completed successfully"
    export TERMINATION_REASON="completed"
  else
    echo "[ERROR] Job failed with exit code $CMD_EXIT"
    export GITHUB_STATUS_DESCRIPTION="Job failed with exit code $CMD_EXIT"
    export TERMINATION_REASON="exit_code_${CMD_EXIT}"
  fi

  # Final summary
  echo "[SUMMARY] ===== Job Summary ====="
  echo "[SUMMARY] Job ID: ${METTA_RUN_ID}"
  echo "[SUMMARY] Exit code: ${CMD_EXIT}"
  echo "[SUMMARY] Termination reason: ${TERMINATION_REASON:-unknown}"
  echo "[SUMMARY] ======================"

  if [ "$ENABLE_GITHUB_STATUS" = "true" ]; then
    # The set_github_status.py script uses CMD_EXIT to determine status
    export CMD_EXIT
    uv run devops/skypilot/set_github_status.py || echo "[WARN] Final status post failed"
  fi

  echo "[RUN] Job complete with exit code: $CMD_EXIT (reason: ${TERMINATION_REASON:-unknown})"

  # Exit with 0 for planned terminations to prevent SkyPilot restarts
  if [[ "${TERMINATION_REASON}" == "max_runtime_reached" ]] || [[ "${TERMINATION_REASON}" == "completed" ]]; then
    echo "[INFO] Exiting with code 0 to prevent SkyPilot restart"
    exit 0
  else
    echo "[INFO] Exiting with actual exit code: $CMD_EXIT"
    exit "$CMD_EXIT"
  fi

envs:
  GITHUB_REPOSITORY: Metta-AI/metta
  GITHUB_PAT: ""

  METTA_RUN_ID: ""
  METTA_CMD: train
  METTA_CMD_ARGS: ""
  METTA_GIT_REF: main
  WANDB_DIR: ./wandb
  HEARTBEAT_TIMEOUT: 600

  # s3 mount slows down uv, so we put DATA_DIR outside of /workspace/metta
  DATA_DIR: /mnt/s3/train_dir
  SKYPILOT_DOCKER_USERNAME: ""
  SKYPILOT_DOCKER_PASSWORD: ""
  SKYPILOT_DOCKER_SERVER: 751442549699.dkr.ecr.us-east-1.amazonaws.com

secrets:
  # configured by launch script based on local credentials
  WANDB_PASSWORD: ""
  OBSERVATORY_TOKEN: ""
