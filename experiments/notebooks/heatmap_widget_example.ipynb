{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Interactive Heatmap Widget Example\n",
    "\n",
    "This notebook demonstrates how to use the `HeatmapWidget` - an anywidget-based implementation of the observatory heatmap component for Jupyter notebooks.\n",
    "\n",
    "The widget provides interactive policy evaluation heatmaps with:\n",
    "- Hover effects showing detailed information\n",
    "- Double-click to open replay URLs\n",
    "- Dynamic control over number of policies displayed\n",
    "- Automatic organization by evaluation categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Installation\n",
    "\n",
    "First, make sure you have the required dependencies:\n",
    "`pip install anywidget traitlets`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Import and Basic Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from experiments.notebooks.utils.metrics import fetch_metrics\n",
    "from experiments.notebooks.utils.monitoring import monitor_training_statuses\n",
    "from experiments.notebooks.utils.replays import show_replay\n",
    "from experiments.notebooks.utils.training import launch_training\n",
    "from experiments.notebooks.utils.metrics import find_training_jobs\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"default\")\n",
    "\n",
    "# Add utils directory to path\n",
    "sys.path.append(os.path.join(os.getcwd(), 'utils'))\n",
    "\n",
    "%load_ext anywidget\n",
    "\n",
    "print(\"Setup complete! Auto-reload enabled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example 1: Demo Heatmap with Sample Data\n",
    "\n",
    "Let's start with a simple demo that includes sample data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.notebooks.utils.heatmap_widget import HeatmapWidget, create_demo_heatmap, create_heatmap_widget\n",
    "\n",
    "# Create a demo heatmap with sample data\n",
    "demo_widget = create_demo_heatmap()\n",
    "\n",
    "# Display the widget\n",
    "demo_widget\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Real Data from Metta Database\n",
    "\n",
    "Now let's create a function that fetches real evaluation data from metta's databases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_base_url = \"https://api.observatory.softmax-research.net\"\n",
    "auth_token = \"WTGipvHPU5RtZsN3S033H4QnWLKkOq3LtPmhes6iFQk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Testing the fixed API client...\n",
      "ğŸ§ª Testing fixed API client...\n",
      "\n",
      "ğŸ“‹ Testing /heatmap/policies endpoint...\n",
      "ğŸ” Making POST request to: https://api.observatory.softmax-research.net/heatmap/policies\n",
      "ğŸ”‘ Headers: {'Content-Type': 'application/json', 'X-Auth-Token': 'WTGipvHPU5RtZsN3S033H4QnWLKkOq3LtPmhes6iFQk'}\n",
      "ğŸ“¦ Payload: {'search_text': None, 'pagination': {'page': 1, 'page_size': 5}}\n",
      "ğŸ“¨ Response status: 200\n",
      "âœ… Success! Got response: <class 'dict'>\n",
      "ğŸ“Š Found 5 policies\n",
      "ğŸ” Sample policy: {'id': 'a23fae54-0001-499a-8c86-a9083ad48c35', 'type': 'training_run', 'name': 'bullm.navigation.low_reward.with_context.07-24', 'user_id': 'matthew@stem.ai', 'created_at': '2025-07-25 07:01:19.783193', 'tags': ['user:unknown']}\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ Fixed API Client Implementation\n",
    "import asyncio\n",
    "import httpx\n",
    "from typing import Dict, List, Optional, Any\n",
    "import logging\n",
    "\n",
    "class MettaAPIClientFixed:\n",
    "    \"\"\"Fixed client that properly handles authentication and response parsing.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, auth_token: Optional[str] = None):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "        if auth_token:\n",
    "            # Use X-Auth-Token header format like extract_training_rewards.py\n",
    "            self.headers[\"X-Auth-Token\"] = auth_token\n",
    "\n",
    "    async def _make_request(self, method: str, endpoint: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Make an HTTP request to the API.\"\"\"\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        print(f\"ğŸ” Making {method} request to: {url}\")\n",
    "        print(f\"ğŸ”‘ Headers: {self.headers}\")\n",
    "        if 'json' in kwargs:\n",
    "            print(f\"ğŸ“¦ Payload: {kwargs['json']}\")\n",
    "            \n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.request(method, url, headers=self.headers, timeout=30.0, **kwargs)\n",
    "            print(f\"ğŸ“¨ Response status: {response.status_code}\")\n",
    "            if response.status_code >= 400:\n",
    "                print(f\"âŒ Response body: {response.text}\")\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "    \n",
    "    async def get_policies(self, search_text: Optional[str] = None, page_size: int = 50):\n",
    "        \"\"\"Get available policies and training runs.\"\"\"\n",
    "        url = \"/heatmap/policies\"\n",
    "        payload = {\n",
    "            \"search_text\": search_text,  # Use None instead of empty string\n",
    "            \"pagination\": {\"page\": 1, \"page_size\": page_size}\n",
    "        }\n",
    "        return await self._make_request(\"POST\", url, json=payload)\n",
    "    \n",
    "    async def get_eval_names(self, training_run_ids: List[str], run_free_policy_ids: List[str] = []):\n",
    "        \"\"\"Get evaluation names for selected policies.\"\"\"\n",
    "        url = \"/heatmap/evals\"\n",
    "        payload = {\n",
    "            \"training_run_ids\": training_run_ids,\n",
    "            \"run_free_policy_ids\": run_free_policy_ids\n",
    "        }\n",
    "        return await self._make_request(\"POST\", url, json=payload)\n",
    "    \n",
    "    async def get_available_metrics(self, training_run_ids: List[str], run_free_policy_ids: List[str], eval_names: List[str]):\n",
    "        \"\"\"Get available metrics for selected policies and evaluations.\"\"\"\n",
    "        url = \"/heatmap/metrics\"\n",
    "        payload = {\n",
    "            \"training_run_ids\": training_run_ids,\n",
    "            \"run_free_policy_ids\": run_free_policy_ids,\n",
    "            \"eval_names\": eval_names\n",
    "        }\n",
    "        return await self._make_request(\"POST\", url, json=payload)\n",
    "    \n",
    "    async def generate_heatmap(self, training_run_ids: List[str], run_free_policy_ids: List[str], \n",
    "                        eval_names: List[str], metric: str, policy_selector: str = \"best\"):\n",
    "        \"\"\"Generate heatmap data.\"\"\"\n",
    "        url = \"/heatmap/heatmap\"\n",
    "        payload = {\n",
    "            \"training_run_ids\": training_run_ids,\n",
    "            \"run_free_policy_ids\": run_free_policy_ids,\n",
    "            \"eval_names\": eval_names,\n",
    "            \"metric\": metric,\n",
    "            \"training_run_policy_selector\": policy_selector\n",
    "        }\n",
    "        return await self._make_request(\"POST\", url, json=payload)\n",
    "\n",
    "# Test the fixed client with better debugging\n",
    "async def test_api_connection_fixed(api_base_url: str, auth_token: str):\n",
    "    \"\"\"Test API connection with the fixed client.\"\"\"\n",
    "    print(f\"ğŸ§ª Testing fixed API client...\")\n",
    "    client = MettaAPIClientFixed(api_base_url, auth_token)\n",
    "    \n",
    "    try:\n",
    "        # Test getting policies\n",
    "        print(\"\\nğŸ“‹ Testing /heatmap/policies endpoint...\")\n",
    "        policies_response = await client.get_policies(page_size=5)\n",
    "        print(f\"âœ… Success! Got response: {type(policies_response)}\")\n",
    "        if isinstance(policies_response, dict) and 'policies' in policies_response:\n",
    "            print(f\"ğŸ“Š Found {len(policies_response['policies'])} policies\")\n",
    "            if policies_response['policies']:\n",
    "                print(f\"ğŸ” Sample policy: {policies_response['policies'][0]}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  Unexpected response structure: {policies_response}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "print(\"ğŸš€ Testing the fixed API client...\")\n",
    "test_result = await test_api_connection_fixed(\n",
    "    api_base_url=api_base_url,\n",
    "    auth_token=auth_token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Metta HTTP API client loaded!\n",
      "ğŸ“‹ Available functions:\n",
      "   - fetch_real_heatmap_data(training_run_names, metrics, policy_selector)\n",
      "   - get_available_policies(api_base_url)\n",
      "   - get_available_eval_names(training_run_ids, api_base_url)\n",
      "   - get_available_metrics(training_run_ids, eval_names, api_base_url)\n",
      "\n",
      "ğŸ’¡ This uses HTTP API calls exactly like repo.ts!\n",
      "ğŸ”— Requires app_backend server running on http://localhost:8000\n",
      "ğŸš€ Start with: cd app_backend && uv run python server.py\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from typing import Dict, List, Optional\n",
    "import logging\n",
    "import httpx\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "from experiments.notebooks.utils.heatmap_widget import HeatmapWidget, create_heatmap_widget\n",
    "\n",
    "# Simple HTTP client approach - mimics repo.ts exactly\n",
    "class MettaAPIClient:\n",
    "    \"\"\"Simple HTTP client that mimics the TypeScript repo.ts functionality.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, auth_token: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the API client.\n",
    "\n",
    "        Args:\n",
    "            base_url: Base URL of the API server\n",
    "            auth_token: Optional authentication token\n",
    "        \"\"\"\n",
    "        if not base_url:\n",
    "            base_url = \"http://localhost:8000\"\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        if not auth_token:\n",
    "            raise ValueError(\"auth_token is required\")\n",
    "        self.headers = {}\n",
    "        self.headers[\"X-Auth-Token\"] = auth_token\n",
    "\n",
    "    async def _make_request(self, method: str, endpoint: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Make an HTTP request to the API.\"\"\"\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.request(method, url, headers=self.headers, timeout=30.0, **kwargs)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "    \n",
    "    async def get_policies(self, search_text: str = \"\", page_size: int = 50):\n",
    "        \"\"\"Get available policies and training runs.\"\"\"\n",
    "        url = \"/heatmap/policies\"\n",
    "        payload = {\n",
    "            \"search_text\": search_text,\n",
    "            \"pagination\": {\"page\": 1, \"page_size\": page_size}\n",
    "        }\n",
    "        data = await self._make_request(\"POST\", url, json=payload)\n",
    "        return data\n",
    "    \n",
    "    async def get_eval_names(self, training_run_ids: List[str], run_free_policy_ids: List[str] = []):\n",
    "        \"\"\"Get evaluation names for selected policies.\"\"\"\n",
    "        url = \"/heatmap/evals\"\n",
    "        payload = {\n",
    "            \"training_run_ids\": training_run_ids,\n",
    "            \"run_free_policy_ids\": run_free_policy_ids\n",
    "        }\n",
    "        data = await self._make_request(\"POST\", url, json=payload)\n",
    "        return data\n",
    "    \n",
    "    async def get_available_metrics(self, training_run_ids: List[str], run_free_policy_ids: List[str], eval_names: List[str]):\n",
    "        \"\"\"Get available metrics for selected policies and evaluations.\"\"\"\n",
    "        url = \"/heatmap/metrics\"\n",
    "        payload = {\n",
    "            \"training_run_ids\": training_run_ids,\n",
    "            \"run_free_policy_ids\": run_free_policy_ids,\n",
    "            \"eval_names\": eval_names\n",
    "        }\n",
    "        data = await self._make_request(\"POST\", url, json=payload)\n",
    "        return data.get(\"metrics\", [])\n",
    "    \n",
    "    async def generate_heatmap(self, training_run_ids: List[str], run_free_policy_ids: List[str], \n",
    "                        eval_names: List[str], metric: str, policy_selector: str = \"best\"):\n",
    "        \"\"\"Generate heatmap data.\"\"\"\n",
    "        url = \"/heatmap/heatmap\"\n",
    "        payload = {\n",
    "            \"training_run_ids\": training_run_ids,\n",
    "            \"run_free_policy_ids\": run_free_policy_ids,\n",
    "            \"eval_names\": eval_names,\n",
    "            \"metric\": metric,\n",
    "            \"training_run_policy_selector\": policy_selector\n",
    "        }\n",
    "        data = await self._make_request(\"POST\", url, json=payload)\n",
    "        return data.get(\"heatmap\", {})\n",
    "\n",
    "async def fetch_real_heatmap_data(\n",
    "    training_run_names: List[str],\n",
    "    metrics: List[str],\n",
    "    policy_selector: str = \"best\",\n",
    "    api_base_url: str = \"http://localhost:8000\",\n",
    "    auth_token: Optional[str] = None,\n",
    "    max_policies: int = 20\n",
    ") -> HeatmapWidget:\n",
    "    \"\"\"\n",
    "    Fetch real evaluation data using the metta HTTP API (same as repo.ts).\n",
    "    \n",
    "    Args:\n",
    "        training_run_names: List of training run names (e.g., [\"daveey.arena.rnd.16x4.2\"])\n",
    "        metrics: List of metrics to include (e.g., [\"reward\", \"heart.get\"])\n",
    "        policy_selector: \"best\" or \"latest\" policy selection strategy\n",
    "        api_base_url: Base URL for the metta API\n",
    "        max_policies: Maximum number of policies to display\n",
    "        \n",
    "    Returns:\n",
    "        HeatmapWidget with real data\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” Fetching real heatmap data using HTTP API: {api_base_url}\")\n",
    "    print(f\"ğŸ“‹ Training runs: {training_run_names}\")\n",
    "    print(f\"ğŸ“Š Metrics: {metrics}\")\n",
    "    print(f\"ğŸ¯ Policy selector: {policy_selector}\")\n",
    "    \n",
    "    try:\n",
    "        client = MettaAPIClient(api_base_url, auth_token)\n",
    "        \n",
    "        # Step 1: Get available policies to find training run IDs\n",
    "        print(\"ğŸ” Getting available policies...\")\n",
    "        policies_data = await client.get_policies(page_size=1000)\n",
    "        \n",
    "        # Find training run IDs that match our training run names\n",
    "        training_run_ids = []\n",
    "        for policy in policies_data[\"policies\"]:\n",
    "            if policy[\"type\"] == \"training_run\" and any(run_name in policy[\"name\"] for run_name in training_run_names):\n",
    "                training_run_ids.append(policy[\"id\"])\n",
    "        \n",
    "        if not training_run_ids:\n",
    "            print(f\"âŒ No training runs found matching: {training_run_names}\")\n",
    "            return create_heatmap_widget()\n",
    "        \n",
    "        print(f\"âœ… Found {len(training_run_ids)} matching training runs\")\n",
    "        \n",
    "        # Step 2: Get available evaluations for these training runs\n",
    "        print(\"ğŸ” Getting evaluation names...\")\n",
    "        eval_names = await client.get_eval_names(training_run_ids, [])\n",
    "        if not eval_names:\n",
    "            print(\"âŒ No evaluations found for selected training runs\")\n",
    "            return create_heatmap_widget()\n",
    "        \n",
    "        print(f\"âœ… Found {len(eval_names)} evaluations\")\n",
    "        \n",
    "        # Step 3: Get available metrics \n",
    "        print(\"ğŸ” Getting available metrics...\")\n",
    "        available_metrics = await client.get_available_metrics(training_run_ids, [], eval_names)\n",
    "        \n",
    "        # Filter to requested metrics that actually exist\n",
    "        valid_metrics = [m for m in metrics if m in available_metrics]\n",
    "        if not valid_metrics:\n",
    "            print(f\"âŒ None of the requested metrics {metrics} are available\")\n",
    "            print(f\"ğŸ’¡ Available metrics: {available_metrics[:10]}...\")\n",
    "            return create_heatmap_widget()\n",
    "        \n",
    "        print(f\"âœ… Using metrics: {valid_metrics}\")\n",
    "        \n",
    "        # Step 4: Generate heatmap for the first metric\n",
    "        primary_metric = valid_metrics[0]\n",
    "        print(f\"ğŸ” Generating heatmap for metric: {primary_metric}\")\n",
    "        keys = list(eval_names.keys())\n",
    "        heatmap_data = await client.generate_heatmap(\n",
    "            training_run_ids, [], keys, primary_metric, policy_selector\n",
    "        )\n",
    "        \n",
    "        if not heatmap_data[\"policyNames\"]:\n",
    "            print(\"âŒ No heatmap data generated\")\n",
    "            return create_heatmap_widget()\n",
    "        \n",
    "        # Limit policies if requested\n",
    "        policy_names = heatmap_data[\"policyNames\"]\n",
    "        if len(policy_names) > max_policies:\n",
    "            print(f\"ğŸ”¢ Limiting to {max_policies} policies (found {len(policy_names)})\")\n",
    "            # Sort by average score and take top N\n",
    "            avg_scores = heatmap_data[\"policyAverageScores\"]\n",
    "            top_policies = sorted(avg_scores.keys(), key=lambda p: avg_scores[p], reverse=True)[:max_policies]\n",
    "            \n",
    "            # Filter the data\n",
    "            filtered_cells = {p: heatmap_data[\"cells\"][p] for p in top_policies if p in heatmap_data[\"cells\"]}\n",
    "            heatmap_data[\"policyNames\"] = top_policies\n",
    "            heatmap_data[\"cells\"] = filtered_cells\n",
    "            heatmap_data[\"policyAverageScores\"] = {p: avg_scores[p] for p in top_policies if p in avg_scores}\n",
    "        \n",
    "        print(f\"ğŸ“Š Final dataset: {len(heatmap_data['policyNames'])} policies Ã— {len(heatmap_data['evalNames'])} evaluations\")\n",
    "        \n",
    "        # Step 5: Convert to widget format\n",
    "        cells = {}\n",
    "        for policy_name in heatmap_data[\"policyNames\"]:\n",
    "            cells[policy_name] = {}\n",
    "            for eval_name in heatmap_data[\"evalNames\"]:\n",
    "                cell = heatmap_data[\"cells\"].get(policy_name, {}).get(eval_name, {})\n",
    "                cells[policy_name][eval_name] = {\n",
    "                    'metrics': {primary_metric: cell.get(\"value\", 0.0)},\n",
    "                    'replayUrl': cell.get(\"replayUrl\"),\n",
    "                    'evalName': eval_name\n",
    "                }\n",
    "        \n",
    "        # Create widget\n",
    "        widget = create_heatmap_widget()\n",
    "        widget.set_multi_metric_data(\n",
    "            cells=cells,\n",
    "            eval_names=heatmap_data[\"evalNames\"], \n",
    "            policy_names=heatmap_data[\"policyNames\"],\n",
    "            metrics=[primary_metric],\n",
    "            selected_metric=primary_metric\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Successfully created heatmap widget with real data!\")\n",
    "        return widget\n",
    "        \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âŒ Could not connect to metta API server\")\n",
    "        print(\"ğŸ’¡ Make sure the app_backend server is running on http://localhost:8000\")\n",
    "        print(\"ğŸ’¡ You can start it with: cd app_backend && uv run python server.py\")\n",
    "        return create_heatmap_widget()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching real data: {e}\")\n",
    "        return create_heatmap_widget()\n",
    "\n",
    "async def get_available_policies(api_base_url: str = \"http://localhost:8000\", limit: int = 50):\n",
    "    \"\"\"Get available policies and training runs.\"\"\"\n",
    "    try:\n",
    "        client = MettaAPIClient(api_base_url, auth_token)\n",
    "        return await client.get_policies(page_size=limit)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching policies: {e}\")\n",
    "        return {\"policies\": []}\n",
    "\n",
    "async def get_available_eval_names(training_run_ids: List[str], api_base_url: str = \"http://localhost:8000\"):\n",
    "    \"\"\"Get available evaluation names.\"\"\"\n",
    "    try:\n",
    "        client = MettaAPIClient(api_base_url, auth_token)\n",
    "        return await client.get_eval_names(training_run_ids, [])\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching eval names: {e}\")\n",
    "        return []\n",
    "\n",
    "async def get_available_metrics(training_run_ids: List[str], eval_names: List[str], api_base_url: str = \"http://localhost:8000\"):\n",
    "    \"\"\"Get available metrics.\"\"\"\n",
    "    try:\n",
    "        client = MettaAPIClient(api_base_url, auth_token)\n",
    "        return await client.get_available_metrics(training_run_ids, [], eval_names)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching metrics: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"ğŸš€ Metta HTTP API client loaded!\")\n",
    "print(\"ğŸ“‹ Available functions:\")\n",
    "print(\"   - fetch_real_heatmap_data(training_run_names, metrics, policy_selector)\")\n",
    "print(\"   - get_available_policies(api_base_url)\")  \n",
    "print(\"   - get_available_eval_names(training_run_ids, api_base_url)\")\n",
    "print(\"   - get_available_metrics(training_run_ids, eval_names, api_base_url)\")\n",
    "print(\"\")\n",
    "print(\"ğŸ’¡ This uses HTTP API calls exactly like repo.ts!\")\n",
    "print(\"ğŸ”— Requires app_backend server running on http://localhost:8000\")\n",
    "print(\"ğŸš€ Start with: cd app_backend && uv run python server.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example: Using Real Data\n",
    "\n",
    "Now let's explore what's available in the database and create a heatmap with real data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Creating heatmap with real data...\n",
      "ğŸ” Fetching real heatmap data using HTTP API: https://api.observatory.softmax-research.net\n",
      "ğŸ“‹ Training runs: ['daveey.arena.rnd.16x4.2', 'relh.skypilot.fff.j20.666', 'bullm.navigation.low_reward.baseline', 'bullm.navigation.low_reward.baseline.07-17', 'bullm.navigation.low_reward.baseline.07-23', 'relh.multigpu.fff.1', 'relh.skypilot.fff.j21.2']\n",
      "ğŸ“Š Metrics: ['reward', 'heart.get', 'ore_red.get', 'action.move.success']\n",
      "ğŸ¯ Policy selector: best\n",
      "ğŸ” Getting available policies...\n",
      "âŒ Error fetching real data: Client error '422 Unprocessable Entity' for url 'https://api.observatory.softmax-research.net/heatmap/policies'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422\n",
      "ğŸš€ HeatmapWidget initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# For now, let's try with some common metrics and see what we find:\n",
    "try:\n",
    "    # specific_runs = [\n",
    "    #     \"daveey.arena.rnd.16x4.2\",\n",
    "    #     \"relh.skypilot.fff.j20.666\",\n",
    "    #     \"bullm.navigation.low_reward.baseline\",\n",
    "    #     \"bullm.navigation.low_reward.baseline.07-17\", \n",
    "    #     \"bullm.navigation.low_reward.baseline.07-23\",\n",
    "    #     \"relh.multigpu.fff.1\",\n",
    "    #     \"relh.skypilot.fff.j21.2\",\n",
    "    # ]\n",
    "    \n",
    "    # Common metrics that are likely to exist:\n",
    "    metrics_to_fetch = [\"reward\", \"heart.get\", \"ore_red.get\", \"action.move.success\"]\n",
    "    \n",
    "    print(\"ğŸ¯ Creating heatmap with real data...\")\n",
    "    real_heatmap = await fetch_real_heatmap_data(\n",
    "        api_base_url=api_base_url,\n",
    "        auth_token=auth_token,\n",
    "        training_run_names=[\n",
    "            \"daveey.arena.rnd.16x4.2\",\n",
    "            \"relh.skypilot.fff.j20.666\",\n",
    "            \"bullm.navigation.low_reward.baseline\",\n",
    "            \"bullm.navigation.low_reward.baseline.07-17\", \n",
    "            \"bullm.navigation.low_reward.baseline.07-23\",\n",
    "            \"relh.multigpu.fff.1\",\n",
    "            \"relh.skypilot.fff.j21.2\",\n",
    "        ],\n",
    "        metrics=metrics_to_fetch,\n",
    "        max_policies=15  # Limit display to keep it manageable\n",
    "    )\n",
    "    \n",
    "    # Display the widget\n",
    "    real_heatmap\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error fetching real data: {e}\")\n",
    "    print(\"ğŸ’¡ This might happen if:\")\n",
    "    print(\"   - The database URI is incorrect\")\n",
    "    print(\"   - You're not authenticated with wandb\")\n",
    "    print(\"   - The specified metrics don't exist in the database\")\n",
    "    print(\"   - You don't have access to the database\")\n",
    "    print(\"\\nğŸ”„ Falling back to demo data...\")\n",
    "    \n",
    "    # Fall back to demo data if real data fails\n",
    "    from experiments.notebooks.utils.heatmap_widget import create_demo_heatmap\n",
    "    demo_fallback = create_demo_heatmap()\n",
    "    demo_fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Comparing Policy Selection Strategies\n",
    "\n",
    "Let's see the difference between \"best\" and \"latest\" policy selection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare \"best\" vs \"latest\" policy selection for the same runs\n",
    "sample_runs = [\n",
    "    \"daveey.arena.rnd.16x4.2\",\n",
    "    \"bullm.navigation.low_reward.baseline\",\n",
    "    \"relh.skypilot.fff.j20.666\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ” Comparing 'best' vs 'latest' policy selection strategies:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Select using \"latest\" strategy\n",
    "    print(\"\\\\nğŸ“ˆ LATEST strategy (highest version/epoch):\")\n",
    "    latest_policies = select_best_policies_from_runs(\n",
    "        training_runs=sample_runs,\n",
    "        selector=\"latest\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\nğŸ† BEST strategy (highest average reward):\")\n",
    "    best_policies = select_best_policies_from_runs(\n",
    "        training_runs=sample_runs, \n",
    "        metric=\"reward\",\n",
    "        selector=\"best\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\nğŸ“Š COMPARISON:\")\n",
    "    print(f\"{'Run':<35} {'Latest':<25} {'Best':<25}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    # Create lookup dictionaries\n",
    "    latest_lookup = {}\n",
    "    best_lookup = {}\n",
    "    \n",
    "    for policy in latest_policies:\n",
    "        for run in sample_runs:\n",
    "            if policy.startswith(run):\n",
    "                latest_lookup[run] = policy\n",
    "                break\n",
    "                \n",
    "    for policy in best_policies:\n",
    "        for run in sample_runs:\n",
    "            if policy.startswith(run):\n",
    "                best_lookup[run] = policy\n",
    "                break\n",
    "    \n",
    "    for run in sample_runs:\n",
    "        latest = latest_lookup.get(run, \"None\")\n",
    "        best = best_lookup.get(run, \"None\")\n",
    "        same = \"âœ…\" if latest == best else \"âŒ\"\n",
    "        print(f\"{run:<35} {latest:<25} {best:<25} {same}\")\n",
    "        \n",
    "    print(\"\\\\nğŸ’¡ Key differences:\")\n",
    "    print(\"   - 'Latest' picks the most recent version (highest epoch/version number)\")\n",
    "    print(\"   - 'Best' picks the version with highest average performance across evaluations\")\n",
    "    print(\"   - They may differ when a later version performs worse than an earlier one\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error comparing strategies: {e}\")\n",
    "    print(\"ğŸ’¡ Make sure you have access to the evaluation database\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Advanced Usage: Custom Training Runs and Metrics\n",
    "\n",
    "Here's how to create a heatmap with your own training runs and metrics using the smart policy selection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a custom heatmap with specific training runs and metrics\n",
    "\n",
    "# Option 1: Use training run names (recommended - uses smart selection)\n",
    "my_training_runs = [\n",
    "    # Add your training run names here, for example:\n",
    "    # \"my_experiment_batch_1\",\n",
    "    # \"my_experiment_batch_2\", \n",
    "    # \"baseline_run_v1\"\n",
    "]\n",
    "\n",
    "# Option 2: Use exact policy URIs (if you know exactly which ones you want)\n",
    "my_specific_policies = [\n",
    "    # Add exact policy URIs here, for example:\n",
    "    # \"my_policy_name:v123\",\n",
    "    # \"baseline_experiment:v456\",\n",
    "    # \"new_approach:v789\"\n",
    "]\n",
    "\n",
    "# Step 2: Define metrics you want to compare\n",
    "my_metrics = [\n",
    "    \"reward\",\n",
    "    \"heart.get\",           # Example game-specific metric\n",
    "    \"action.move.success\", # Example action success rate\n",
    "    # Add more metrics as needed\n",
    "]\n",
    "\n",
    "# Step 3: Optional - filter to specific evaluations\n",
    "# eval_filter = \"sim_env LIKE '%maze%'\"  # Only maze environments\n",
    "# eval_filter = \"sim_env LIKE '%combat%'\"  # Only combat environments  \n",
    "eval_filter = None  # No filter - include all evaluations\n",
    "\n",
    "# Step 4: Create the heatmap\n",
    "if my_training_runs:  # Use smart policy selection from training runs\n",
    "    print(\"ğŸ¯ Creating custom heatmap with best policies from training runs...\")\n",
    "    \n",
    "    # Select best policies from training runs\n",
    "    selected_policies = select_best_policies_from_runs(\n",
    "        training_runs=my_training_runs,\n",
    "        eval_db_uri=\"wandb://stats/navigation_db\",\n",
    "        metric=\"reward\",  # Metric to optimize for when selecting \"best\"\n",
    "        selector=\"best\"   # or \"latest\"\n",
    "    )\n",
    "    \n",
    "    custom_heatmap = fetch_real_heatmap_data(\n",
    "        policy_names=selected_policies,\n",
    "        metrics=my_metrics,\n",
    "        eval_db_uri=\"wandb://stats/navigation_db\",\n",
    "        eval_filter=eval_filter,\n",
    "        max_policies=20\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ“Š Custom heatmap created! Try:\")\n",
    "    print(\"   - Hovering over cells to see detailed values\")\n",
    "    print(\"   - Changing metrics with: custom_heatmap.update_metric('heart.get')\")\n",
    "    print(\"   - Adjusting policies shown: custom_heatmap.set_num_policies(15)\")\n",
    "    \n",
    "    custom_heatmap\n",
    "    \n",
    "elif my_specific_policies:  # Use exact policy URIs\n",
    "    print(\"ğŸ¯ Creating custom heatmap with specific policies...\")\n",
    "    custom_heatmap = fetch_real_heatmap_data(\n",
    "        policy_names=my_specific_policies,\n",
    "        metrics=my_metrics,\n",
    "        eval_db_uri=\"wandb://stats/navigation_db\",\n",
    "        eval_filter=eval_filter,\n",
    "        max_policies=20\n",
    "    )\n",
    "    \n",
    "    custom_heatmap\n",
    "    \n",
    "else:\n",
    "    print(\"ğŸ“ To use this example:\")\n",
    "    print(\"\\\\nğŸš€ RECOMMENDED: Use training run names (Option 1)\")\n",
    "    print(\"1. Add your training run names to 'my_training_runs' list above\")\n",
    "    print(\"2. The system will automatically select the best policy from each run\")\n",
    "    print(\"3. Customize the 'my_metrics' list with metrics you're interested in\")\n",
    "    print(\"4. Run this cell again\")\n",
    "    print(\"\\\\nğŸ’¡ Example training run names:\")\n",
    "    print(\"   - 'my_experiment_batch_1'\")\n",
    "    print(\"   - 'baseline_run_v2'\")\n",
    "    print(\"   - 'new_approach_test'\")\n",
    "    print(\"\\\\nâš™ï¸  ALTERNATIVE: Use exact policy URIs (Option 2)\")\n",
    "    print(\"1. Add exact policy URIs to 'my_specific_policies' list\")\n",
    "    print(\"2. Example: 'my_policy_name:v123', 'baseline:v456'\")\n",
    "    print(\"\\\\nğŸ” TIP: Run the exploration code in previous cells to see available options\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Available Evaluation Database URIs\n",
    "\n",
    "You can choose from several different evaluation databases depending on what type of data you want to analyze:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ—ƒï¸ AVAILABLE EVALUATION DATABASES\n",
    "\n",
    "# Domain-specific databases (most commonly used)\n",
    "available_databases = {\n",
    "    \"navigation_db\": \"wandb://stats/navigation_db\",      # Navigation tasks\n",
    "    \"memory_db\": \"wandb://stats/memory_db\",              # Memory tasks  \n",
    "    \"objectuse_db\": \"wandb://stats/objectuse_db\",        # Object use tasks\n",
    "    \"nav_sequence_db\": \"wandb://stats/nav_sequence_db\",  # Navigation sequence tasks\n",
    "    # User-specific databases\n",
    "    \"jack_db\": \"wandb://stats/jack_db\",                  # Jack's personal database\n",
    "}\n",
    "\n",
    "print(\"ğŸ—„ï¸  Available Evaluation Databases:\")\n",
    "print(\"=\" * 50)\n",
    "for name, uri in available_databases.items():\n",
    "    print(f\"ğŸ“Š {name:<20} â†’ {uri}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Usage examples:\")\n",
    "print(\"   # For navigation analysis:\")\n",
    "print(\"   fetch_real_heatmap_data(..., eval_db_uri='wandb://stats/navigation_db')\")\n",
    "print(\"   # For memory analysis:\")  \n",
    "print(\"   fetch_real_heatmap_data(..., eval_db_uri='wandb://stats/memory_db')\")\n",
    "\n",
    "print(\"\\nğŸ” You can also use:\")\n",
    "print(\"   â€¢ Local files: './path/to/my_stats.db'\")\n",
    "print(\"   â€¢ S3 buckets: 's3://bucket/path/stats.db'\")\n",
    "\n",
    "# Quick function to check what's in each database\n",
    "def explore_database(db_name: str, db_uri: str, limit: int = 5):\n",
    "    \"\"\"Quickly explore what's available in a database\"\"\"\n",
    "    print(f\"\\nğŸ” Exploring {db_name} ({db_uri}):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Get a small sample of data\n",
    "        policies = get_available_policy_names(eval_db_uri=db_uri, limit=limit)\n",
    "        metrics = get_available_metrics(eval_db_uri=db_uri, limit=10)\n",
    "        evals = get_available_evaluations(eval_db_uri=db_uri, limit=10)\n",
    "        \n",
    "        print(f\"ğŸ“‹ Sample policies ({len(policies)}): {policies[:3]}...\")\n",
    "        print(f\"ğŸ“Š Sample metrics ({len(metrics)}): {metrics[:5]}...\")  \n",
    "        print(f\"ğŸƒ Sample evaluations ({len(evals)}): {evals[:5]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error accessing {db_name}: {e}\")\n",
    "        if \"wandb\" in str(e).lower():\n",
    "            print(\"ğŸ’¡ You may need to authenticate with wandb or check permissions\")\n",
    "\n",
    "# Uncomment to explore different databases:\n",
    "# explore_database(\"Navigation DB\", \"wandb://stats/navigation_db\")\n",
    "# explore_database(\"Memory DB\", \"wandb://stats/memory_db\")  \n",
    "# explore_database(\"Object Use DB\", \"wandb://stats/objectuse_db\")\n",
    "\n",
    "print(\"\\nğŸ“ To explore a database, uncomment the explore_database() calls above!\")\n",
    "print(\"\\nğŸš€ Quick start: Most users will want 'wandb://stats/navigation_db'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Example: Comparing Policies Across Different Task Categories\n",
    "\n",
    "# Here's how to create heatmaps from different evaluation databases:\n",
    "\n",
    "# Example 1: Navigation tasks\n",
    "print(\"ğŸ§­ Creating navigation heatmap...\")\n",
    "try:\n",
    "    navigation_runs = [\"daveey.arena.rnd.16x4.2\", \"bullm.navigation.low_reward.baseline\"]\n",
    "    \n",
    "    nav_policies = select_best_policies_from_runs(\n",
    "        training_runs=navigation_runs,\n",
    "        eval_db_uri=\"wandb://stats/navigation_db\",  # Navigation database\n",
    "        selector=\"best\",\n",
    "        metric=\"reward\"\n",
    "    )\n",
    "    \n",
    "    nav_heatmap = fetch_real_heatmap_data(\n",
    "        policy_names=nav_policies,\n",
    "        metrics=[\"reward\", \"heart.get\", \"action.move.success\"],\n",
    "        eval_db_uri=\"wandb://stats/navigation_db\",\n",
    "        max_policies=5\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Navigation heatmap created successfully!\")\n",
    "    # nav_heatmap  # Uncomment to display\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Navigation heatmap failed: {e}\")\n",
    "\n",
    "# Example 2: Memory tasks (if available)\n",
    "print(\"\\nğŸ§  Memory tasks would use:\")\n",
    "print(\"   eval_db_uri='wandb://stats/memory_db'\")\n",
    "print(\"   # Likely different metrics like memory.recall, sequence.accuracy, etc.\")\n",
    "\n",
    "# Example 3: Object use tasks (if available)  \n",
    "print(\"\\nğŸ”§ Object use tasks would use:\")\n",
    "print(\"   eval_db_uri='wandb://stats/objectuse_db'\")\n",
    "print(\"   # Likely different metrics like tool.use.success, manipulation.accuracy, etc.\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Pro tip: Each database specializes in different task types:\")\n",
    "print(\"   ğŸ§­ navigation_db    â†’ spatial reasoning, pathfinding\")\n",
    "print(\"   ğŸ§  memory_db        â†’ recall, sequence learning\") \n",
    "print(\"   ğŸ”§ objectuse_db     â†’ manipulation, tool use\")\n",
    "print(\"   ğŸ“š nav_sequence_db  â†’ sequential navigation tasks\")\n",
    "\n",
    "print(\"\\nğŸ“Š To switch databases, just change the eval_db_uri parameter!\")\n",
    "print(\"   Example: eval_db_uri='wandb://stats/memory_db'\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Advanced Usage: Custom Policies and Metrics\n",
    "\n",
    "Here's how to create a heatmap with specific policies and metrics of your choice:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a custom heatmap with specific policies and metrics\n",
    "\n",
    "# Step 1: Define your policies of interest\n",
    "my_policies = [\n",
    "    # Add your policy names here, for example:\n",
    "    # \"my_experiment_1:v100\",\n",
    "    # \"my_experiment_2:v200\", \n",
    "    # \"baseline_policy:v50\"\n",
    "]\n",
    "\n",
    "# Step 2: Define metrics you want to compare\n",
    "my_metrics = [\n",
    "    \"reward\",\n",
    "    \"heart.get\",           # Example game-specific metric\n",
    "    \"action.move.success\", # Example action success rate\n",
    "    # Add more metrics as needed\n",
    "]\n",
    "\n",
    "# Step 3: Optional - filter to specific evaluations\n",
    "# eval_filter = \"sim_env LIKE '%maze%'\"  # Only maze environments\n",
    "# eval_filter = \"sim_env LIKE '%combat%'\"  # Only combat environments  \n",
    "eval_filter = None  # No filter - include all evaluations\n",
    "\n",
    "# Step 4: Create the heatmap\n",
    "if my_policies:  # Only run if you've specified policies\n",
    "    print(\"ğŸ¯ Creating custom heatmap...\")\n",
    "    custom_heatmap = fetch_real_heatmap_data(\n",
    "        policy_names=my_policies,\n",
    "        metrics=my_metrics,\n",
    "        eval_db_uri=\"wandb://stats/navigation_db\",  # Adjust as needed\n",
    "        eval_filter=eval_filter,\n",
    "        max_policies=20\n",
    "    )\n",
    "    \n",
    "    # Step 5: Display and interact\n",
    "    print(\"ğŸ“Š Custom heatmap created! Try:\")\n",
    "    print(\"   - Hovering over cells to see detailed values\")\n",
    "    print(\"   - Changing metrics with: custom_heatmap.update_metric('heart.get')\")\n",
    "    print(\"   - Adjusting policies shown: custom_heatmap.set_num_policies(15)\")\n",
    "    \n",
    "    custom_heatmap\n",
    "else:\n",
    "    print(\"ğŸ“ To use this example:\")\n",
    "    print(\"1. Uncomment the exploration code in the previous cell to see available policies\")\n",
    "    print(\"2. Add your policy names to the 'my_policies' list above\") \n",
    "    print(\"3. Customize the 'my_metrics' list with metrics you're interested in\")\n",
    "    print(\"4. Run this cell again\")\n",
    "    print(\"\\nğŸ’¡ Example policy names might look like:\")\n",
    "    print(\"   - 'my_policy_name:v123'\")\n",
    "    print(\"   - 'baseline_experiment:v456'\") \n",
    "    print(\"   - 'new_approach:v789'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from experiments.notebooks.utils.heatmap_widget import HeatmapWidget, create_demo_heatmap, create_heatmap_widget\n",
    "\n",
    "# Create a demo heatmap with sample data\n",
    "demo_widget = create_demo_heatmap()\n",
    "display(demo_widget)\n",
    "\n",
    "w = widgets.Button(description=\"Click me\", style=dict(width=\"200px\", height=\"50px\"))\n",
    "display(w)\n",
    "# Display the widget\n",
    "print(demo_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Try interacting with the heatmap above:**\n",
    "- Hover over cells to see detailed information\n",
    "- Click on a row's left policy title label to \"open\" that policy's Wandb URL in a new tab\n",
    "- Adjust the \"Policies to show\" input to change how many policies are displayed\n",
    "- Click on policy names to open WandB links (in demo, these won't work)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example 2: Creating Your Own Heatmap Data\n",
    "\n",
    "Here's how to create a heatmap with your own data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example 4: Multiple Metrics with Working selectedMetric\n",
    "\n",
    "Now let's see the `selectedMetric` functionality working properly! This example shows a heatmap where changing the metric actually changes the displayed values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-metric heatmap widget\n",
    "from experiments.notebooks.utils.heatmap_widget import create_multi_metric_demo\n",
    "\n",
    "multi_metric_widget = create_multi_metric_demo()\n",
    "\n",
    "# Display the widget\n",
    "multi_metric_widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try changing the metric to see the values actually change!\n",
    "print(\"ğŸ”„ Changing metric to 'episode_length'...\")\n",
    "multi_metric_widget.update_metric('episode_length')\n",
    "\n",
    "# NOTE: Notice how the values in the heatmap widget change as you switch\n",
    "# metrics?  Do not display the widget again and try to change that. That ends up\n",
    "# creating a seperate copy of the widget in a new output cell.  Instead just\n",
    "# reference the one you originally rendered, call its functions, and watch it\n",
    "# change in its Juypter notebook cell. Like we just did. Let's do it again in\n",
    "# the next cell too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more time. Run this cell then scroll back up again to see the change.\n",
    "print(\"\\nğŸ”„ Changing metric to 'success_rate'...\")\n",
    "multi_metric_widget.update_metric('success_rate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last one. Scroll up again to see the change.\n",
    "print(\"\\nğŸ”„ Changing metric to 'success_rate'...\")\n",
    "multi_metric_widget.update_metric('success_rate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom metrics\n",
    "\n",
    "We can really define our cells to have any metric data we want. This is useful because we plan to have all sorts of metrics. Let's look at an example of using any old metric we decide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new heatmap widget\n",
    "custom_widget = create_heatmap_widget()\n",
    "\n",
    "# Define your data structure\n",
    "# This should match the format expected by the observatory dashboard\n",
    "cells_data = {\n",
    "    'my_policy_v1': {\n",
    "        'task_a/level1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 85.2,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay1.json', \n",
    "            'evalName': 'task_a/level1'\n",
    "        },\n",
    "        'task_a/level2': {\n",
    "            'metrics': {\n",
    "                'custom_score': 87.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay2.json', \n",
    "            'evalName': 'task_a/level2'\n",
    "        },\n",
    "        'task_b/challenge1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 92.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay3.json', \n",
    "            'evalName': 'task_b/challenge1'\n",
    "        },\n",
    "    },\n",
    "    'my_policy_v2': {\n",
    "        'task_a/level1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 22.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay4.json', \n",
    "            'evalName': 'task_a/level1'\n",
    "        },\n",
    "        'task_a/level2': {\n",
    "            'metrics': {\n",
    "                'custom_score': 42.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay5.json', \n",
    "            'evalName': 'task_a/level2'\n",
    "        },\n",
    "        'task_b/challenge1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 62.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay6.json', \n",
    "            'evalName': 'task_b/challenge1'\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "eval_names = ['task_a/level1', 'task_a/level2', 'task_b/challenge1']\n",
    "policy_names = ['my_policy_v1', 'my_policy_v2']\n",
    "policy_averages = {\n",
    "    'my_policy_v1': 91.6,\n",
    "    'my_policy_v2': 89.6,\n",
    "}\n",
    "\n",
    "# Set the data\n",
    "custom_widget.set_data(\n",
    "    cells=cells_data,\n",
    "    eval_names=eval_names,\n",
    "    policy_names=policy_names,\n",
    "    policy_average_scores=policy_averages,\n",
    "    selected_metric=\"custom_score\"\n",
    ")\n",
    "\n",
    "# Display the widget\n",
    "custom_widget\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example 3: Adding Callbacks for Interactivity\n",
    "\n",
    "You can add Python callbacks to respond to user interactions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: these callbacks do not work with print(), and that's really just how\n",
    "# Jupyter widgets work.  Once the Jupyter python cell finishes running and\n",
    "# outputs a widget, that widget won't be able to affect the output of the cell\n",
    "# anymore. The only way to to print() from a python widget callback is to write\n",
    "# to a file (or use a thread maybe). I give an example below.\n",
    "\n",
    "# Create another widget for callback demonstration\n",
    "callback_widget = create_heatmap_widget()\n",
    "\n",
    "# Set up the same data as before\n",
    "callback_widget.set_data(\n",
    "    cells=cells_data,\n",
    "    eval_names=eval_names,\n",
    "    policy_names=policy_names,\n",
    "    policy_average_scores=policy_averages,\n",
    "    selected_metric=\"Interactive Score (%)\"\n",
    ")\n",
    "\n",
    "# Define callback functions\n",
    "def handle_cell_selection(cell_info):\n",
    "    \"\"\"Called when user hovers over a cell (not 'overall' column).\"\"\"\n",
    "    with open(\"output_cell_selection.txt\", \"w\") as f:\n",
    "        f.write(f\"ğŸ“ Cell selected: {cell_info['policyUri']} on evaluation '{cell_info['evalName']}'\")\n",
    "\n",
    "def handle_replay_opened(replay_info):\n",
    "    \"\"\"Called when user clicks to open a replay.\"\"\"\n",
    "    with open(\"output_replay_opened.txt\", \"w\") as f:\n",
    "        f.write(f\"ğŸ¬ Replay opened: {replay_info['replayUrl']}\")\n",
    "        f.write(f\"   Policy: {replay_info['policyUri']}\")\n",
    "        f.write(f\"   Evaluation: {replay_info['evalName']}\")\n",
    "\n",
    "# Register the callbacks\n",
    "callback_widget.on_cell_selected(handle_cell_selection)\n",
    "callback_widget.on_replay_opened(handle_replay_opened)\n",
    "\n",
    "# Display the widget\n",
    "callback_widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the files created by the callbacks in the previous cell, if they exist\n",
    "import os\n",
    "\n",
    "for fname in [\"output_cell_selection.txt\", \"output_replay_opened.txt\"]:\n",
    "    try:\n",
    "        with open(fname, \"r\") as f:\n",
    "            print(f.read())\n",
    "        os.remove(fname)\n",
    "        print(f\"File {fname} deleted\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Try interacting with the heatmap above to see the callback messages printed to\n",
    "*output files!**\n",
    "\n",
    "## Data Format Reference\n",
    "\n",
    "The heatmap widget expects data in a specific format that matches the\n",
    "observatory dashboard:\n",
    "\n",
    "```python\n",
    "cells = {\n",
    "    'policy_name': {\n",
    "        'eval_name': {\n",
    "            'metrics': {\n",
    "                'reward': 50,\n",
    "                'heart.get': 98,\n",
    "                'action.move.success': 5,\n",
    "                'ore_red.get': 24.2,\n",
    "                # ... more metrics\n",
    "            },\n",
    "            'replayUrl': str,         # URL to replay file\n",
    "            'evalName': str,          # Should match the key\n",
    "        },\n",
    "        # ... more evaluations\n",
    "    },\n",
    "    # ... more policies\n",
    "}\n",
    "```\n",
    "\n",
    "**Important notes:**\n",
    "- Evaluation names with \"/\" will be grouped by category (the part before \"/\")\n",
    "- The heatmap shows policies sorted by average score (worst to best, bottom to top)\n",
    "- Policy names that contain \":v\" will have WandB URLs generated automatically\n",
    "- Replay URLs should be accessible URLs or file paths\n",
    "\n",
    "This widget provides the same interactive functionality as the observatory dashboard but in a python environment, making it perfect for exploratory analysis and sharing results via Jupyter notebooks!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
