{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mettabook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete! Auto-reload enabled.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Enable auto-reload of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from mettabook_widgets import (\n",
    "    JobLauncher,\n",
    "    JobStatusMonitor,\n",
    "    MetricsFetcher,\n",
    "    ReplayViewer,\n",
    "    TrainingConfigurator,\n",
    "    WandBConnector,\n",
    ")\n",
    "from plotly.subplots import make_subplots\n",
    "from run_store import RunStore\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"default\")\n",
    "\n",
    "print(\"Setup complete! Auto-reload enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components initialized with shared RunStore!\n"
     ]
    }
   ],
   "source": [
    "# Initialize a shared RunStore using singleton pattern\n",
    "from run_store import get_runstore\n",
    "\n",
    "# Get the singleton RunStore instance\n",
    "run_store = get_runstore()\n",
    "\n",
    "# Initialize components with the shared RunStore\n",
    "config = TrainingConfigurator(run_store=run_store)\n",
    "launcher = JobLauncher(config, run_store=run_store)\n",
    "monitor = JobStatusMonitor(launcher, run_store=run_store)\n",
    "wandb_conn = WandBConnector(run_store=run_store)\n",
    "fetcher = MetricsFetcher(wandb_conn)\n",
    "replay_viewer = ReplayViewer(wandb_conn)\n",
    "\n",
    "# Store monitor reference globally for JavaScript access\n",
    "_metta_monitor = monitor\n",
    "if \"_metta_monitors\" not in globals():\n",
    "    _metta_monitors = {}\n",
    "_metta_monitors[id(monitor)] = monitor\n",
    "\n",
    "print(\"Components initialized with shared RunStore!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Confirm Credential Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Confirm Credential Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Run Store Overview\n",
    "\n",
    "The RunStore provides unified tracking of all your training runs across SkyPilot and W&B. It persists data locally and provides a single view of all runs.\n",
    "\n",
    "**Important**: If you've made changes to the code, restart the kernel (Kernel → Restart) and re-run cells 1-5 to reload the modules.\n",
    "\n",
    "View all tracked runs in the table below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5298b0587440beae54bf64d4b5e274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HBox(children=(Text(value='', layout=Layout(width='250px'), placeholder='Search …"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display RunStore table with integrated buttons\n",
    "from run_store import get_runstore\n",
    "\n",
    "# Get the singleton RunStore instance\n",
    "rs = get_runstore()\n",
    "\n",
    "# Helper functions for manual testing\n",
    "def add_test_run(run_id):\n",
    "    rs.add_run(run_id)\n",
    "    print(f\"Added {run_id}\")\n",
    "\n",
    "def refresh_test_run(run_id):\n",
    "    updated = rs.refresh_run(run_id)\n",
    "    print(f\"Refreshed {run_id}: Updated={updated}\")\n",
    "\n",
    "def refresh_all_test():\n",
    "    runs = rs.refresh_all()\n",
    "    print(f\"Refreshed {len(runs)} runs\")\n",
    "\n",
    "# Display the integrated table using widgets\n",
    "rs.to_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<run_store.RunStore at 0x110f27450>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training\n",
    "\n",
    "This section allows you to launch and monitor a training run. You can skip to the \"Analyze a Run\" section if you have an existing run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Specify and Launch Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(config.display())\n",
    "display(launcher.display())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Monitor Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the job status monitor with integrated RunStore table\n",
    "# Store monitor reference for JavaScript integration\n",
    "_metta_monitor = monitor\n",
    "if \"_metta_monitors\" not in globals():\n",
    "    _metta_monitors = {}\n",
    "_metta_monitors[id(monitor)] = monitor\n",
    "_metta_monitor_id = id(monitor)\n",
    "\n",
    "display(monitor.display())\n",
    "monitor.start_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze a Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pick which run to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(wandb_conn.display())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fetch metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fetcher.display())\n",
    "if wandb_conn.run:\n",
    "    fetcher.auto_fetch()\n",
    "else:\n",
    "    print(\"Run section 3.1 first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Analyze\n",
    "\n",
    "`fetcher.metrics_df` contains a dataframe with the sampled metrics from above\n",
    "\n",
    "You can analyze them in any way you like. Below is some boilerplate code that shows `overview/*` metrics over agent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = fetcher.metrics_df\n",
    "if metrics_df is None or len(metrics_df) == 0:\n",
    "    print(\"No metrics data available. Please fetch metrics first.\")\n",
    "else:\n",
    "    assert metrics_df is not None\n",
    "    include_prefixes = [\"overview/\"]\n",
    "    plot_cols = []\n",
    "\n",
    "    for col in metrics_df.columns:\n",
    "        # Skip non-numeric columns\n",
    "        if not pd.api.types.is_numeric_dtype(metrics_df[col]):\n",
    "            continue\n",
    "        # Skip columns with no variation\n",
    "        if metrics_df[col].nunique() <= 1:\n",
    "            continue\n",
    "        if not any(col.startswith(prefix) for prefix in include_prefixes):\n",
    "            continue\n",
    "        plot_cols.append(col)\n",
    "\n",
    "    if not plot_cols:\n",
    "        print(\"No plottable metrics found\")\n",
    "    else:\n",
    "        # Calculate grid dimensions\n",
    "        n_metrics = len(plot_cols)\n",
    "        n_cols = min(3, n_metrics)  # Max 3 columns\n",
    "        n_rows = (n_metrics + n_cols - 1) // n_cols\n",
    "\n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=n_rows,\n",
    "            cols=n_cols,\n",
    "            subplot_titles=[col.replace(\"overview/\", \"\").replace(\"_\", \" \") for col in plot_cols],\n",
    "            vertical_spacing=0.08,\n",
    "            horizontal_spacing=0.1,\n",
    "        )\n",
    "\n",
    "        # Color palette\n",
    "        colors = [\"blue\", \"red\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n",
    "\n",
    "        # Add traces for each metric\n",
    "        for idx, col in enumerate(plot_cols):\n",
    "            row = (idx // n_cols) + 1\n",
    "            col_idx = (idx % n_cols) + 1\n",
    "            color = colors[idx % len(colors)]\n",
    "\n",
    "            if \"_step\" in metrics_df.columns:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=metrics_df[\"_step\"],\n",
    "                        y=metrics_df[col],\n",
    "                        mode=\"lines\",\n",
    "                        name=col.replace(\"overview/\", \"\"),\n",
    "                        line=dict(color=color, width=2),\n",
    "                        showlegend=False,\n",
    "                    ),\n",
    "                    row=row,\n",
    "                    col=col_idx,\n",
    "                )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(height=250 * n_rows, title_text=\"Overview Metric / Agent Step\", showlegend=False)\n",
    "\n",
    "        # Update x-axes labels for bottom row\n",
    "        for col_idx in range(1, min(n_cols, n_metrics) + 1):\n",
    "            fig.update_xaxes(title_text=\"Steps\", row=n_rows, col=col_idx)\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Replay Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(replay_viewer.display())\n",
    "\n",
    "if wandb_conn.run:\n",
    "    print(\"Fetching replays...\")\n",
    "    replay_viewer.auto_fetch()\n",
    "else:\n",
    "    print(\"Select a run first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_viewer.display_iframe(width=1000, height=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
