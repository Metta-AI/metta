# Muesli Implementation

This directory contains a complete implementation of **Muesli** (Model-based Offline Policy Optimization), a state-of-the-art reinforcement learning algorithm that combines model-free policy gradients with model-based planning.

## Overview

Muesli achieves **562% median human-normalized score** on Atari games, significantly outperforming PPO (~300%) while maintaining computational efficiency comparable to model-free methods. The algorithm's key innovation is combining:

- **CMPO** (Clipped Maximum a Posteriori Policy Optimization) for stable policy updates
- **Model learning** through multi-step dynamics prediction
- **Retrace** for off-policy correction
- **Categorical representations** for values and rewards

## Key Components

### 1. Agent Architecture (`agent.py`)

The Muesli agent consists of four main networks:

- **Representation Network**: Encodes observations into hidden states
- **Dynamics Network**: Predicts next hidden state and reward given action
- **Prediction Networks**: Outputs policy and value from hidden states
- **Target Network**: EMA copy for stable value targets

```python
from metta.rl.muesli.agent import MuesliAgent
from metta.rl.muesli.config import MuesliConfig

config = MuesliConfig()
agent = MuesliAgent(
    obs_shape=(84, 84, 4),  # Atari frame stack
    action_space=env.action_space,
    config=config,
    device=torch.device('cuda')
)
```

### 2. CMPO Policy Optimization (`losses.py`)

CMPO clips advantages directly instead of probability ratios:

```python
œÄ_CMPO(a|s) ‚àù œÄ_prior(a|s) * exp(clip(advantage(s,a), -c, c))
```

This provides exact control over policy changes with maximum TV distance: `max D_TV = tanh(c/2)`

### 3. Replay Buffer (`replay_buffer.py`)

The replay buffer supports:
- Multi-step sequence storage for model learning
- Retrace target computation
- Priority-based sampling
- Mixed on-policy/off-policy data

```python
from metta.rl.muesli.replay_buffer import MuesliReplayBuffer

replay_buffer = MuesliReplayBuffer(
    capacity=100000,
    unroll_length=5,
    gamma=0.99,
    device=device
)
```

### 4. Training Loop (`trainer.py`)

The training loop implements:
- Mixed replay (75%) and fresh data (25%) training
- Multi-step model unrolling
- Target network updates
- Variance normalization for advantages

## Configuration

Key hyperparameters in `configs/trainer/muesli.yaml`:

```yaml
muesli:
  cmpo:
    clip_bound: 1.0  # Advantage clipping (max TV = tanh(0.5) ‚âà 0.46)
    cmpo_weight: 1.0  # CMPO regularization weight
    
  model_learning:
    unroll_steps: 5  # Model supervision depth
    
  categorical:
    support_size: 601  # Bins for value/reward representation
    value_min: -300.0
    value_max: 300.0
    
  replay:
    capacity: 100000
    replay_fraction: 0.75  # 75% replay, 25% fresh
```

## Usage

### Training with Muesli

To train using Muesli instead of PPO:

```bash
# Use the Muesli trainer configuration
python train.py trainer=muesli env=mettagrid/arena/tag

# Or override algorithm in existing config
python train.py trainer.algorithm=muesli trainer.muesli.cmpo.clip_bound=1.0
```

### Testing the Implementation

Run the test script to verify all components:

```bash
python -m metta.rl.muesli.test_muesli
```

This will test:
1. Agent creation and forward pass
2. Categorical value representations
3. Replay buffer operations
4. Loss computation
5. Target network updates
6. End-to-end training loop

## Performance Expectations

With correct implementation, expect:

- **Simple environments** (CartPole, LunarLander): Solved in 1-2 hours
- **Atari games**: 500-600% median human-normalized score
- **Sample efficiency**: 2-3x improvement over PPO
- **Computation time**: ~20% overhead vs PPO due to model components

## Algorithm Details

### CMPO vs PPO Clipping

PPO clips probability ratios:
```python
L_PPO = min(r(Œ∏)A, clip(r(Œ∏), 1-Œµ, 1+Œµ)A)
```

Muesli clips advantages directly:
```python
œÄ_CMPO ‚àù œÄ_prior * exp(clip(A, -c, c))
```

### Model Learning

The model learns through multi-step unrolling:
1. Encode observation ‚Üí hidden state
2. For each step:
   - Predict next hidden state and reward
   - Supervise with real transitions
   - Match CMPO policy at future states

### Retrace for Value Targets

Retrace provides off-policy corrected value targets:
```python
V_retrace(s_t) = V(s_t) + Œ£_œÑ (Œ†_s c_s) * Œ¥_œÑ
```

Where `c_s = min(1, œÄ_target/œÄ_behavior)` clips importance ratios.

## Implementation Status

‚úÖ **Completed:**
- Core Muesli agent with all networks
- CMPO policy computation
- Categorical value/reward representations  
- Multi-step replay buffer
- Retrace implementation
- Model learning objectives
- Training loop with mixed data
- Integration with existing trainer
- Configuration files
- Test suite

üîÑ **Future Improvements:**
- Continuous action support
- Distributed training
- Advanced priority schemes
- Curiosity-driven exploration
- Benchmark on full Atari suite

## References

- [Muesli Paper](https://arxiv.org/abs/2104.06159): Hessel et al., "Muesli: Combining Improvements in Policy Optimization"
- [MuZero Paper](https://arxiv.org/abs/1911.08265): Schrittwieser et al., "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model"
- [Retrace Paper](https://arxiv.org/abs/1606.02647): Munos et al., "Safe and Efficient Off-Policy Reinforcement Learning"