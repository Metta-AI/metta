# @package __global__

defaults:
  # - override /agent: simple
  - _self_

trainer:
  env: /env/mettagrid/walkaround
  evaluate_interval: 2

# policy: wandb://run/b.daveey.train.maze.sm.dr.warm.0
# baselines: wandb://run/b.daveey.train.maze.sm.11x11.0

# policy_uri: wandb://run/b.daveey.sm.train.er.new.0
# policy_uri: wandb://run/daveey.ar.cards.1
# policy_uri: wandb://run/b.daveey.t.32.instant
#policy_uri: ${trained_policy_uri}
policy_uri: wandb://run/b.georgedeane.terrain_extra_hard
# b.daphne.terrain_varied_cyl_lab_pretrained:v0
# policy_uri: wandb://run/terrain_training_multienv_april18
# policy_uri: wandb://run/b.daphne.terrain_multienv_april18


# Infinite_cooldown models:
# navigation_infinite_cooldown_sweep_2g_.r.0 - ok
# navigation_infinite_cooldown_sweep_2g.r.1 - 9 reward after 400 timesteps
# navigation_infinite_cooldown_high_ent_no_initial_heart - ~9, current best
# navigation_poisson_train_sampling5 breaks in infinite_cooldown, works great in poisson with short distance
# navigation_poisson_train_sampling5
# npc_policy_uri: ${trained_policy_uri}

analyzer:
  # eval_stats_uri: ${run_dir}/eval_stats
  policy_uri: ${..policy_uri}
  analysis:
    metrics:
      - metric: episode_reward
  output_path: evalresults/memory_db.html

sim:
  num_envs: 2
  num_episodes: 2
  max_time_s: 600

  # policy_uri: ${..policy_uri}
  # npc_policy_uri: ${..npc_policy_uri}
  env: /env/mettagrid/navigation/training/empty_world


run_id: 103
run: ${oc.env:USER}.local.${run_id}
trained_policy_uri: ${run_dir}/checkpoints

sweep_params: "sweep/fast"
sweep_name: "${oc.env:USER}.local.sweep.${run_id}"
seed: null
