# @package __global__

# User config for dual-policy training with checkpoint NPC
defaults:
  - /common
  - /agent/fast
  - /trainer/trainer
  - /sim/all@evals
  - _self_

# Basic run configuration
run: dual_policy_checkpoint_experiment
cmd: train

data_dir: ${oc.env:DATA_DIR,./train_dir}
run_dir: ${data_dir}/${run}

# Trainer configuration overrides
trainer:
  curriculum: /env/mettagrid/curriculum/arena/learning_progress
  num_workers: 8  # Increased for GPU training
  total_timesteps: 10_000_000_000

  # Dual-policy configuration
  dual_policy:
    enabled: true
    policy_a_percentage: 0.5  # 50% of agents use the main policy

    # Configure checkpoint NPC behavior
    npc_type: "checkpoint"
    checkpoint_npc:
      # Use the specific WandB policy as the NPC
      checkpoint_path: "wandb://metta-research/dual_policy_training/model/bullm_dual_policy_against_roomba_v9:v2"

      # Alternative examples - other actual runs:
      # checkpoint_path: "wandb://metta-research/metta/model/yudhister.recipes.arena.4x4.efficiency_baseline.07-24-00-17:latest"
      # checkpoint_path: "wandb://metta-research/metta/model/yudhister.recipes.arena.2x4.efficiency_baseline.07-24-00-17:latest"

      # For local testing, you can still use local paths:
      # checkpoint_path: "./train_dir/previous_training_run/checkpoints/model_0000.pt"

# GPU configuration
device: cuda  # Use CUDA GPU
compile: true  # Enable PyTorch compilation for faster training
compile_mode: reduce-overhead

# WandB configuration
wandb:
  entity: metta-research
  project: dual_policy_training
  enabled: true

# System monitoring for cost tracking
system_monitor:
  enabled: true
  sampling_interval_sec: 1.0
  history_size: 100
  auto_start: true
