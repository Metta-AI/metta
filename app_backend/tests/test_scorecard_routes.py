import datetime
import uuid

import pytest
from fastapi.testclient import TestClient

from metta.app_backend.clients.stats_client import StatsClient
from metta.app_backend.leaderboard_updater import LeaderboardUpdater
from metta.app_backend.metta_repo import MettaRepo


@pytest.mark.slow
class TestPolicyScorecardRoutes:
    """Integration tests for policy-based scorecard routes."""

    def test_get_policies_basic(self, test_client: TestClient, create_test_data, record_episodes) -> None:
        """Test basic functionality of getting policies and training runs."""
        # Create test data with both training run and run-free policies
        test_data = create_test_data("get_policies_basic", num_policies=2, create_run_free_policies=1)

        # Record episodes so policies appear in wide_episodes view
        record_episodes(
            test_data,
            eval_category="navigation",
            env_names=["test_env"],
            metric_values={"policy_0_test_env": 75.0, "policy_1_test_env": 85.0, "policy_2_test_env": 65.0},
        )

        # Get all policies without search
        response = test_client.get("/heatmap/policies")
        assert response.status_code == 200
        result = response.json()

        # Verify response structure
        assert "policies" in result

        # Should have 2 total policies (1 training run + 1 run-free policy)
        assert len(result["policies"]) >= 2

        # Find training run and policy in unified list
        training_run = next(p for p in result["policies"] if p["type"] == "training_run")
        policy = next(p for p in result["policies"] if p["type"] == "policy")

        # Verify training run structure
        assert "id" in training_run
        assert "type" in training_run
        assert "name" in training_run
        assert "user_id" in training_run
        assert "created_at" in training_run
        assert "tags" in training_run
        assert training_run["type"] == "training_run"
        assert isinstance(training_run["tags"], list)
        assert training_run["tags"] == ["test_tag", "scorecard_test"]

        # Verify run-free policy structure
        assert "id" in policy
        assert "type" in policy
        assert "name" in policy
        assert "user_id" in policy
        assert "created_at" in policy
        assert "tags" in policy
        assert policy["type"] == "policy"
        assert isinstance(policy["tags"], list)
        assert policy["tags"] == []  # Run-free policies have empty tags

    def test_get_eval_categories(
        self,
        test_client: TestClient,
        create_test_data,
        record_episodes,
    ) -> None:
        """Test getting evaluation categories for selected policies."""
        test_data = create_test_data("eval_categories_test", num_policies=2)

        # Record episodes with different categories and environments
        record_episodes(
            test_data,
            "navigation",
            ["maze1", "maze2"],
            {"policy_0_maze1": 80.0, "policy_0_maze2": 75.0, "policy_1_maze1": 85.0, "policy_1_maze2": 90.0},
        )

        record_episodes(test_data, "combat", ["arena1"], {"policy_0_arena1": 70.0, "policy_1_arena1": 88.0})

        # Get eval names for this training run
        training_run_id = str(test_data["training_run"].id)
        response = test_client.post(
            "/heatmap/evals",
            json={"training_run_ids": [training_run_id], "run_free_policy_ids": []},
        )
        assert response.status_code == 200
        eval_names = response.json()

        # Should have eval names for navigation and combat
        expected_eval_names = {"navigation/maze1", "navigation/maze2", "combat/arena1"}
        assert set(eval_names) == expected_eval_names

    def test_get_eval_categories_empty_policies(self, test_client: TestClient) -> None:
        """Test getting eval names with empty policy and training run lists."""
        response = test_client.post("/heatmap/evals", json={"training_run_ids": [], "run_free_policy_ids": []})
        assert response.status_code == 200
        assert response.json() == []

    def test_get_available_metrics(
        self,
        test_client: TestClient,
        stats_client: StatsClient,
        create_test_data,
    ) -> None:
        """Test getting available metrics for selected policies and evaluations."""
        test_data = create_test_data("metrics_test", num_policies=1)

        # Record episodes with different metrics
        policy = test_data["policies"][0]
        epoch = test_data["epochs"][0]

        # Record episode with multiple metrics
        stats_client.record_episode(
            agent_policies={0: policy.id},
            agent_metrics={0: {"reward": 80.0, "survival_time": 120.0, "score": 95.0}},
            primary_policy_id=policy.id,
            sim_suite="metrics_suite",
            env_name="test_env",
            stats_epoch=epoch.id,
            replay_url="https://example.com/replay/test",
        )

        # Get available metrics for the training run
        training_run_id = str(test_data["training_run"].id)
        response = test_client.post(
            "/heatmap/metrics",
            json={
                "training_run_ids": [training_run_id],
                "run_free_policy_ids": [],
                "eval_names": ["metrics_suite/test_env"],
            },
        )
        assert response.status_code == 200
        metrics = response.json()

        # Should include all recorded metrics
        assert "reward" in metrics
        assert "survival_time" in metrics
        assert "score" in metrics

    def test_get_available_metrics_empty_inputs(self, test_client: TestClient) -> None:
        """Test getting available metrics with empty inputs."""
        # Empty training run IDs and policy IDs
        response = test_client.post(
            "/heatmap/metrics",
            json={
                "training_run_ids": [],
                "run_free_policy_ids": [],
                "eval_names": ["test/env1"],
            },
        )
        assert response.status_code == 200
        assert response.json() == []

        # Empty eval selections
        response = test_client.post(
            "/heatmap/metrics",
            json={"training_run_ids": ["some-id"], "run_free_policy_ids": [], "eval_names": []},
        )
        assert response.status_code == 200
        assert response.json() == []

    def test_generate_policy_scorecard_latest_selector(
        self, test_client: TestClient, create_test_data, record_episodes
    ) -> None:
        """Test generating scorecard with latest policy selector."""
        # Create two training runs with multiple policies each
        test_data1 = create_test_data("heatmap_latest_1", num_policies=2)
        test_data2 = create_test_data("heatmap_latest_2", num_policies=2)

        # Record episodes for both runs
        metrics1 = {"policy_0_env1": 70.0, "policy_0_env2": 75.0, "policy_1_env1": 85.0, "policy_1_env2": 90.0}
        metrics2 = {"policy_0_env1": 80.0, "policy_0_env2": 85.0, "policy_1_env1": 90.0, "policy_1_env2": 95.0}

        record_episodes(test_data1, "test_suite", ["env1", "env2"], metrics1)
        record_episodes(test_data2, "test_suite", ["env1", "env2"], metrics2)

        # Get training run IDs
        training_run_ids = [str(test_data1["training_run"].id), str(test_data2["training_run"].id)]

        # Generate scorecard with latest selector
        response = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": training_run_ids,
                "run_free_policy_ids": [],
                "eval_names": ["test_suite/env1", "test_suite/env2"],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        if response.status_code != 200:
            print(f"Response status: {response.status_code}")
            print(f"Response body: {response.text}")
        assert response.status_code == 200
        heatmap = response.json()

        # Should have 2 policies (latest from each run = policy_1 from each)
        assert len(heatmap["policyNames"]) == 2
        latest_policy_names = {test_data1["policy_names"][1], test_data2["policy_names"][1]}
        assert set(heatmap["policyNames"]) == latest_policy_names

        # Verify eval names structure
        assert set(heatmap["evalNames"]) == {"test_suite/env1", "test_suite/env2"}

        # Verify cells structure
        assert "cells" in heatmap
        for policy_name in heatmap["policyNames"]:
            assert policy_name in heatmap["cells"]
            for eval_name in heatmap["evalNames"]:
                assert eval_name in heatmap["cells"][policy_name]
                assert "value" in heatmap["cells"][policy_name][eval_name]
                assert "replayUrl" in heatmap["cells"][policy_name][eval_name]

    def test_generate_policy_scorecard_best_selector(
        self, test_client: TestClient, create_test_data, record_episodes
    ) -> None:
        """Test generating scorecard with best policy selector."""
        test_data = create_test_data("heatmap_best", num_policies=3)

        # Record episodes where policy performance varies
        # Policy 0: average = (60 + 70) / 2 = 65
        # Policy 1: average = (80 + 90) / 2 = 85 (best)
        # Policy 2: average = (50 + 60) / 2 = 55
        metrics = {
            "policy_0_env1": 60.0,
            "policy_0_env2": 70.0,
            "policy_1_env1": 80.0,
            "policy_1_env2": 90.0,  # Best policy
            "policy_2_env1": 50.0,
            "policy_2_env2": 60.0,
        }

        record_episodes(test_data, "best_suite", ["env1", "env2"], metrics)

        # Generate scorecard with best selector
        response = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(test_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": ["best_suite/env1", "best_suite/env2"],
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response.status_code == 200
        heatmap = response.json()

        # Should only have the best policy (policy_1)
        assert len(heatmap["policyNames"]) == 1
        assert heatmap["policyNames"][0].startswith("policy_heatmap_best_1")

        # Verify it has the expected values
        best_policy_name = heatmap["policyNames"][0]
        assert heatmap["cells"][best_policy_name]["best_suite/env1"]["value"] == 80.0
        assert heatmap["cells"][best_policy_name]["best_suite/env2"]["value"] == 90.0

        # Verify average score
        assert abs(heatmap["policyAverageScores"][best_policy_name] - 85.0) < 0.01

    def test_generate_policy_scorecard_with_run_free_policies(
        self, test_client: TestClient, create_test_data, record_episodes
    ) -> None:
        """Test scorecard generation includes run-free policies correctly."""
        # Create mix of training run and run-free policies
        test_data = create_test_data("mixed_policies", num_policies=1, create_run_free_policies=2)

        # Record episodes for all policies
        metrics = {
            "policy_0_env1": 70.0,  # Training run policy
            "policy_1_env1": 85.0,  # Run-free policy 1
            "policy_2_env1": 90.0,  # Run-free policy 2
        }
        record_episodes(test_data, "mixed_suite", ["env1"], metrics)

        # Run-free policies are the last 2 policies (since create_run_free_policies=2)
        run_free_policy_ids = [str(p.id) for p in test_data["policies"][1:]]  # Skip first policy (training run policy)

        # Test with latest selector (should include all: 1 from run + 2 run-free)
        response = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(test_data["training_run"].id)],
                "run_free_policy_ids": run_free_policy_ids,
                "eval_names": ["mixed_suite/env1"],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response.status_code == 200
        heatmap = response.json()

        # Should have all 3 policies (1 from training run + 2 run-free)
        assert len(heatmap["policyNames"]) == 3
        expected_names = set(test_data["policy_names"])
        assert set(heatmap["policyNames"]) == expected_names

    def test_generate_policy_heatmap_missing_parameters(self, test_client: TestClient) -> None:
        """Test heatmap generation with missing required parameters."""
        # Missing training run IDs and policy IDs
        response = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [],
                "run_free_policy_ids": [],
                "eval_names": ["test/env1"],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response.status_code == 400

        # Missing eval_names
        response = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": ["some-id"],
                "run_free_policy_ids": [],
                "eval_names": [],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response.status_code == 400

        # Missing metric
        response = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": ["some-id"],
                "run_free_policy_ids": [],
                "eval_names": ["test/env1"],
                "training_run_policy_selector": "latest",
                "metric": "",
            },
        )
        assert response.status_code == 400

    def test_generate_policy_heatmap_empty_result(
        self,
        test_client: TestClient,
        create_test_data,
    ) -> None:
        """Test heatmap generation when no matching data exists."""
        test_data = create_test_data("empty_result", num_policies=1)

        # Don't record any episodes, so no data should be found

        response = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(test_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": ["nonexistent/env1"],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response.status_code == 200
        heatmap = response.json()

        # Should return empty heatmap structure
        assert heatmap["evalNames"] == []
        assert heatmap["policyNames"] == []
        assert heatmap["cells"] == {}
        assert heatmap["policyAverageScores"] == {}
        assert heatmap["evalAverageScores"] == {}
        assert heatmap["evalMaxScores"] == {}

    def test_generate_policy_heatmap_multiple_categories(
        self, test_client: TestClient, create_test_data, record_episodes
    ) -> None:
        """Test heatmap generation with multiple evaluation categories."""
        test_data = create_test_data("multi_category", num_policies=1)

        # Record episodes across multiple categories
        record_episodes(test_data, "navigation", ["maze1", "maze2"], {"policy_0_maze1": 80.0, "policy_0_maze2": 85.0})
        record_episodes(test_data, "combat", ["arena1"], {"policy_0_arena1": 90.0})
        record_episodes(test_data, "cooperation", ["team1", "team2"], {"policy_0_team1": 75.0, "policy_0_team2": 88.0})

        response = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(test_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": [
                    "navigation/maze1",
                    "navigation/maze2",
                    "combat/arena1",
                    "cooperation/team1",  # Only select team1
                ],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response.status_code == 200
        heatmap = response.json()

        # Should have eval names from all selected categories and environments
        expected_evals = {
            "navigation/maze1",
            "navigation/maze2",
            "combat/arena1",
            "cooperation/team1",  # team2 not selected
        }
        assert set(heatmap["evalNames"]) == expected_evals

        # Verify all values are present
        policy_name = test_data["policy_names"][0]
        assert heatmap["cells"][policy_name]["navigation/maze1"]["value"] == 80.0
        assert heatmap["cells"][policy_name]["combat/arena1"]["value"] == 90.0
        assert heatmap["cells"][policy_name]["cooperation/team1"]["value"] == 75.0

    def test_generate_policy_heatmap_aggregation(
        self,
        test_client: TestClient,
        stats_client: StatsClient,
        create_test_data,
    ) -> None:
        """Test that multiple episodes are properly aggregated."""
        test_data = create_test_data("aggregation", num_policies=1)
        policy = test_data["policies"][0]
        epoch = test_data["epochs"][0]

        # Record multiple episodes for the same policy/eval combination
        for i, reward_value in enumerate([80.0, 90.0, 100.0]):
            stats_client.record_episode(
                agent_policies={0: policy.id},
                agent_metrics={0: {"reward": reward_value}},
                primary_policy_id=policy.id,
                stats_epoch=epoch.id,
                sim_suite="agg_suite",
                env_name="test_env",
                replay_url=f"https://example.com/replay/episode_{i}",
            )

        response = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(test_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": ["agg_suite/test_env"],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response.status_code == 200
        heatmap = response.json()

        # Should aggregate the values: (80 + 90 + 100) / 3 = 90.0
        policy_name = test_data["policy_names"][0]
        assert heatmap["cells"][policy_name]["agg_suite/test_env"]["value"] == 90.0

    def test_invalid_policy_selector_value(self, test_client: TestClient) -> None:
        """Test that invalid training_run_policy_selector values are rejected."""
        response = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [],
                "run_free_policy_ids": ["some-id"],
                "eval_names": ["test/env1"],
                "training_run_policy_selector": "invalid",  # Should only accept 'latest' or 'best'
                "metric": "reward",
            },
        )
        assert response.status_code == 422  # Validation error

    def test_policy_search_ordering(self, test_client: TestClient, create_test_data, record_episodes) -> None:
        """Test that policies are returned in descending date order."""
        # Create policies with different timestamps by creating them sequentially
        base_name = "ordering_test"

        # Create policies in sequence to ensure different timestamps
        policy_names = []
        for i in range(3):
            test_data = create_test_data(f"{base_name}_{i}", num_policies=1)
            policy_names.extend(test_data["policy_names"])

            # Record episodes so policies appear in wide_episodes view
            record_episodes(
                test_data,
                eval_category="navigation",
                env_names=["test_env"],
                metric_values={"policy_0_test_env": 75.0 + i * 5},  # Different values for each
            )

        # Get all policies
        response = test_client.get("/heatmap/policies")
        assert response.status_code == 200
        policies_data = response.json()

        # Get all policies from unified response to check ordering
        all_items = policies_data["policies"]
        matching_items = [p for p in all_items if base_name in p["name"]]
        assert len(matching_items) >= 3

        # Verify they are sorted by created_at in descending order (newest first)
        timestamps = [p["created_at"] for p in matching_items]
        sorted_timestamps = sorted(timestamps, reverse=True)
        assert timestamps == sorted_timestamps

    def test_complex_heatmap_aggregation_scenarios(
        self, test_client: TestClient, stats_client: StatsClient, create_test_data, record_episodes
    ) -> None:
        """Test complex heatmap generation with multiple policies, runs, and aggregations."""
        # Create three separate training runs with different characteristics
        test_data1 = create_test_data("complex_heatmap_run1", num_policies=3)
        test_data2 = create_test_data("complex_heatmap_run2", num_policies=2)
        test_data3 = create_test_data("complex_heatmap_run3", num_policies=2)

        # Also create some run-free policies
        run_free_data = create_test_data("run_free_policies", num_policies=0, create_run_free_policies=2)

        # Different evaluation categories and environments for comprehensive testing
        navigation_envs = ["maze1", "maze2", "maze3"]
        combat_envs = ["arena1", "arena2"]
        cooperation_envs = ["team1"]

        # Record initial episodes for run1 - policy performance varies significantly by environment
        run1_navigation_metrics = {
            "policy_0_maze1": 60.0,
            "policy_0_maze2": 65.0,
            "policy_0_maze3": 55.0,  # Avg: 60.0
            "policy_1_maze1": 85.0,
            "policy_1_maze2": 90.0,
            "policy_1_maze3": 80.0,  # Avg: 85.0
            "policy_2_maze1": 75.0,
            "policy_2_maze2": 70.0,
            "policy_2_maze3": 80.0,  # Avg: 75.0
        }
        run1_combat_metrics = {
            "policy_0_arena1": 70.0,
            "policy_0_arena2": 80.0,  # Avg: 75.0
            "policy_1_arena1": 60.0,
            "policy_1_arena2": 70.0,  # Avg: 65.0
            "policy_2_arena1": 95.0,
            "policy_2_arena2": 85.0,  # Avg: 90.0
        }
        run1_cooperation_metrics = {
            "policy_0_team1": 40.0,  # Policy 0 poor at cooperation
            "policy_1_team1": 95.0,  # Policy 1 excellent at cooperation
            "policy_2_team1": 70.0,  # Policy 2 decent at cooperation
        }

        record_episodes(test_data1, "navigation", navigation_envs, run1_navigation_metrics)
        record_episodes(test_data1, "combat", combat_envs, run1_combat_metrics)
        record_episodes(test_data1, "cooperation", cooperation_envs, run1_cooperation_metrics)

        # Record episodes for run2 - different performance profile
        run2_navigation_metrics = {
            "policy_0_maze1": 95.0,
            "policy_0_maze2": 90.0,
            "policy_0_maze3": 85.0,  # Avg: 90.0
            "policy_1_maze1": 70.0,
            "policy_1_maze2": 75.0,
            "policy_1_maze3": 65.0,  # Avg: 70.0
        }
        run2_combat_metrics = {
            "policy_0_arena1": 50.0,
            "policy_0_arena2": 60.0,  # Avg: 55.0
            "policy_1_arena1": 100.0,
            "policy_1_arena2": 95.0,  # Avg: 97.5
        }
        run2_cooperation_metrics = {
            "policy_0_team1": 80.0,  # Decent cooperation
            "policy_1_team1": 85.0,  # Good cooperation
        }

        record_episodes(test_data2, "navigation", navigation_envs, run2_navigation_metrics)
        record_episodes(test_data2, "combat", combat_envs, run2_combat_metrics)
        record_episodes(test_data2, "cooperation", cooperation_envs, run2_cooperation_metrics)

        # Record episodes for run3 - balanced performance
        run3_metrics = {
            "policy_0_maze1": 78.0,
            "policy_0_maze2": 82.0,
            "policy_0_maze3": 80.0,  # Avg: 80.0
            "policy_1_maze1": 76.0,
            "policy_1_maze2": 84.0,
            "policy_1_maze3": 78.0,  # Avg: 79.33
            "policy_0_arena1": 79.0,
            "policy_0_arena2": 81.0,  # Avg: 80.0
            "policy_1_arena1": 77.0,
            "policy_1_arena2": 83.0,  # Avg: 80.0
            "policy_0_team1": 82.0,  # Good cooperation
            "policy_1_team1": 78.0,  # Good cooperation
        }
        record_episodes(test_data3, "navigation", navigation_envs, run3_metrics)
        record_episodes(test_data3, "combat", combat_envs, run3_metrics)
        record_episodes(test_data3, "cooperation", cooperation_envs, run3_metrics)

        # Record episodes for run-free policies - high performance standalone policies
        run_free_metrics = {
            "policy_0_maze1": 100.0,
            "policy_0_maze2": 95.0,
            "policy_0_maze3": 98.0,  # Avg: 97.67
            "policy_1_maze1": 88.0,
            "policy_1_maze2": 92.0,
            "policy_1_maze3": 90.0,  # Avg: 90.0
            "policy_0_arena1": 92.0,
            "policy_0_arena2": 98.0,  # Avg: 95.0
            "policy_1_arena1": 85.0,
            "policy_1_arena2": 87.0,  # Avg: 86.0
            "policy_0_team1": 96.0,  # Excellent cooperation
            "policy_1_team1": 89.0,  # Good cooperation
        }
        record_episodes(run_free_data, "navigation", navigation_envs, run_free_metrics)
        record_episodes(run_free_data, "combat", combat_envs, run_free_metrics)
        record_episodes(run_free_data, "cooperation", cooperation_envs, run_free_metrics)

        # Get all policy IDs
        training_run_ids = [
            str(test_data1["training_run"].id),
            str(test_data2["training_run"].id),
            str(test_data3["training_run"].id),
        ]
        run_free_policy_ids = [str(p.id) for p in run_free_data["policies"]]

        # Test 1: Latest selector with all categories
        response_latest_all = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": training_run_ids,
                "run_free_policy_ids": run_free_policy_ids,
                "eval_names": [f"navigation/{env}" for env in navigation_envs]
                + [f"combat/{env}" for env in combat_envs]
                + [f"cooperation/{env}" for env in cooperation_envs],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_latest_all.status_code == 200
        heatmap_latest_all = response_latest_all.json()

        # Should have 5 policies: latest from each of 3 training runs (policy_1, policy_1, policy_1) + 2 run-free
        # Note: policy_1 is selected from run1 because both policy_1 and policy_2 have same epoch,
        # but policy_1 comes first alphabetically
        assert len(heatmap_latest_all["policyNames"]) == 5
        expected_latest_policies = {
            test_data1["policy_names"][1],  # Latest from run1 (epoch2, first alphabetically)
            test_data2["policy_names"][1],  # Latest from run2 (epoch2)
            test_data3["policy_names"][1],  # Latest from run3 (epoch2)
            run_free_data["policy_names"][0],  # Run-free policy 0
            run_free_data["policy_names"][1],  # Run-free policy 1
        }
        assert set(heatmap_latest_all["policyNames"]) == expected_latest_policies

        # Should have all evaluations: 3 navigation + 2 combat + 1 cooperation = 6 total
        expected_eval_names = {
            "navigation/maze1",
            "navigation/maze2",
            "navigation/maze3",
            "combat/arena1",
            "combat/arena2",
            "cooperation/team1",
        }
        assert set(heatmap_latest_all["evalNames"]) == expected_eval_names

        # Test 2: Best selector with all categories
        response_best_all = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": training_run_ids,
                "run_free_policy_ids": run_free_policy_ids,
                "eval_names": [f"navigation/{env}" for env in navigation_envs]
                + [f"combat/{env}" for env in combat_envs]
                + [f"cooperation/{env}" for env in cooperation_envs],
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response_best_all.status_code == 200
        heatmap_best_all = response_best_all.json()

        # Calculate expected best policies per training run:
        # Run1: Policy0=(60+75+40)/3=58.33, Policy1=(85+65+95)/3=81.67, Policy2=(75+90+70)/3=78.33 -> Policy1 best
        # Run2: Policy0=(90+55+80)/3=75.0, Policy1=(70+97.5+85)/3=84.17 -> Policy1 best
        # Run3: Policy0=(80+80+82)/3=80.67, Policy1=(79.33+80+78)/3=79.11 -> Policy0 best
        expected_best_policies = {
            test_data1["policy_names"][1],  # Best from run1
            test_data2["policy_names"][1],  # Best from run2
            test_data3["policy_names"][0],  # Best from run3
            run_free_data["policy_names"][0],  # Run-free policy 0 (highest avg)
            run_free_data["policy_names"][1],  # Run-free policy 1
        }
        assert set(heatmap_best_all["policyNames"]) == expected_best_policies

        # Test 3: Navigation only evaluation
        response_nav_only = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": training_run_ids,
                "run_free_policy_ids": run_free_policy_ids,
                "eval_names": [
                    "navigation/maze1",
                    "navigation/maze3",  # Only 2 of 3 envs
                ],
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response_nav_only.status_code == 200
        heatmap_nav_only = response_nav_only.json()

        # Should only have navigation evaluations
        expected_nav_eval_names = {"navigation/maze1", "navigation/maze3"}
        assert set(heatmap_nav_only["evalNames"]) == expected_nav_eval_names

        # Test 4: Add more episodes to change best policy selection
        # Make run1 policy0 much better by adding high scores
        additional_high_scores = {
            "policy_0_maze1": 100.0,
            "policy_0_maze2": 100.0,
            "policy_0_maze3": 100.0,
            "policy_0_arena1": 100.0,
            "policy_0_arena2": 100.0,
            "policy_0_team1": 100.0,
        }

        policy_0_run1 = test_data1["policies"][0]
        epoch_0_run1 = test_data1["epochs"][0]

        for env_name in navigation_envs:
            stats_client.record_episode(
                agent_policies={0: policy_0_run1.id},
                agent_metrics={0: {"reward": additional_high_scores[f"policy_0_{env_name}"]}},
                primary_policy_id=policy_0_run1.id,
                stats_epoch=epoch_0_run1.id,
                sim_suite="navigation",
                env_name=env_name,
                replay_url=f"https://example.com/replay/boost/{env_name}",
            )

        for env_name in combat_envs:
            stats_client.record_episode(
                agent_policies={0: policy_0_run1.id},
                agent_metrics={0: {"reward": additional_high_scores[f"policy_0_{env_name}"]}},
                primary_policy_id=policy_0_run1.id,
                stats_epoch=epoch_0_run1.id,
                sim_suite="combat",
                env_name=env_name,
                replay_url=f"https://example.com/replay/boost/{env_name}",
            )

        stats_client.record_episode(
            agent_policies={0: policy_0_run1.id},
            agent_metrics={0: {"reward": additional_high_scores["policy_0_team1"]}},
            primary_policy_id=policy_0_run1.id,
            stats_epoch=epoch_0_run1.id,
            sim_suite="cooperation",
            env_name="team1",
            replay_url="https://example.com/replay/boost/team1",
        )

        # Test best selector again - should now pick policy_0 from run1
        response_best_after_boost = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": training_run_ids,
                "run_free_policy_ids": run_free_policy_ids,
                "eval_names": [f"navigation/{env}" for env in navigation_envs]
                + [f"combat/{env}" for env in combat_envs]
                + [f"cooperation/{env}" for env in cooperation_envs],
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response_best_after_boost.status_code == 200
        heatmap_best_after_boost = response_best_after_boost.json()

        # Now run1 policy0 should be best: ((60+100)/2 + (70+100)/2 + (40+100)/2) / 3 = (80+85+70)/3 = 78.33
        # But policy1 from run1 was: (85+65+95)/3 = 81.67, so policy1 should still be best
        # After boost episodes, verify that the best selector produces a valid heatmap
        # The complex calculation depends on aggregation logic, so let's just verify
        # that we get the expected number of policies (1 best from each run + run-free)
        assert len(heatmap_best_after_boost["policyNames"]) == 5  # 1 from each of 3 runs + 2 run-free

        # Test 5: Verify aggregation is working correctly
        # Check that values in the heatmap are properly aggregated averages
        run_free_policy_0_name = run_free_data["policy_names"][0]
        assert run_free_policy_0_name in heatmap_best_after_boost["policyNames"]

        # Run-free policy 0 should have high scores
        policy_0_nav_maze1 = heatmap_best_after_boost["cells"][run_free_policy_0_name]["navigation/maze1"]["value"]
        assert abs(policy_0_nav_maze1 - 100.0) < 0.01  # Should be 100.0 from run_free_metrics

        # Test 6: Multi-agent aggregation with different agent counts
        # Add an episode with multiple agents to test proper averaging
        stats_client.record_episode(
            agent_policies={0: policy_0_run1.id, 1: policy_0_run1.id, 2: policy_0_run1.id},
            agent_metrics={
                0: {"reward": 90.0},
                1: {"reward": 95.0},
                2: {"reward": 85.0},
            },  # Total: 270, agents: 3, per-agent avg: 90.0
            primary_policy_id=policy_0_run1.id,
            stats_epoch=epoch_0_run1.id,
            sim_suite="navigation",
            env_name="maze1",
            replay_url="https://example.com/replay/multi_agent",
        )

        # Test that this multi-agent episode is properly aggregated
        response_multi_agent = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(test_data1["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": ["navigation/maze1"],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_multi_agent.status_code == 200
        heatmap_multi_agent = response_multi_agent.json()

        # Get the actual policy name from the heatmap results (should be the latest policy selected)
        assert len(heatmap_multi_agent["policyNames"]) == 1  # Should have exactly one policy
        selected_policy_name = heatmap_multi_agent["policyNames"][0]
        # Should aggregate the values from multiple episodes
        multi_agent_value = heatmap_multi_agent["cells"][selected_policy_name]["navigation/maze1"]["value"]

        # Verify the aggregated value is reasonable (should be between original values)
        assert multi_agent_value >= 60.0  # At least as good as the worst episode
        assert multi_agent_value <= 100.0  # Not better than the best episode
        assert multi_agent_value > 75.0  # Should be influenced by the higher values

    def test_policy_heatmap_edge_cases(self, test_client: TestClient, create_test_data, record_episodes) -> None:
        """Test edge cases in policy heatmap generation."""

        # Test 1: Single policy, single evaluation
        single_data = create_test_data("single_test", num_policies=1)
        record_episodes(single_data, "simple", ["env1"], {"policy_0_env1": 75.0})

        response_single = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(single_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": ["simple/env1"],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_single.status_code == 200
        heatmap_single = response_single.json()

        assert len(heatmap_single["policyNames"]) == 1
        assert len(heatmap_single["evalNames"]) == 1
        assert heatmap_single["evalNames"][0] == "simple/env1"

        policy_name = single_data["policy_names"][0]
        assert heatmap_single["cells"][policy_name]["simple/env1"]["value"] == 75.0
        assert heatmap_single["policyAverageScores"][policy_name] == 75.0
        assert heatmap_single["evalAverageScores"]["simple/env1"] == 75.0
        assert heatmap_single["evalMaxScores"]["simple/env1"] == 75.0

        # Test 2: Run-free policies only
        run_free_only_data = create_test_data("run_free_only", num_policies=0, create_run_free_policies=3)
        run_free_metrics = {
            "policy_0_env1": 80.0,
            "policy_0_env2": 85.0,
            "policy_1_env1": 70.0,
            "policy_1_env2": 75.0,
            "policy_2_env1": 90.0,
            "policy_2_env2": 95.0,
        }
        record_episodes(run_free_only_data, "run_free_suite", ["env1", "env2"], run_free_metrics)

        run_free_policy_ids = [str(p.id) for p in run_free_only_data["policies"]]

        # Both latest and best should include all run-free policies
        for selector in ["latest", "best"]:
            response = test_client.post(
                "/scorecard/scorecard",
                json={
                    "training_run_ids": [],
                    "run_free_policy_ids": run_free_policy_ids,
                    "eval_names": ["run_free_suite/env1", "run_free_suite/env2"],
                    "training_run_policy_selector": selector,
                    "metric": "reward",
                },
            )
            assert response.status_code == 200
            heatmap = response.json()

            # All run-free policies should be included regardless of selector
            assert len(heatmap["policyNames"]) == 3
            expected_names = set(run_free_only_data["policy_names"])
            assert set(heatmap["policyNames"]) == expected_names

        # Test 3: Zero values and missing data
        zero_data = create_test_data("zero_test", num_policies=1)
        record_episodes(zero_data, "zero_suite", ["env1"], {"policy_0_env1": 0.0})

        response_zero = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(zero_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": ["zero_suite/env1"],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_zero.status_code == 200
        heatmap_zero = response_zero.json()

        policy_name = zero_data["policy_names"][0]
        assert heatmap_zero["cells"][policy_name]["zero_suite/env1"]["value"] == 0.0

        # Test 4: Large number of evaluations and categories
        large_data = create_test_data("large_test", num_policies=1)
        large_categories = ["cat1", "cat2", "cat3", "cat4", "cat5"]
        large_envs_per_cat = ["env1", "env2", "env3", "env4"]
        large_metrics = {}

        for cat_idx, category in enumerate(large_categories):
            for env_idx, env in enumerate(large_envs_per_cat):
                # Create varied performance across categories and environments
                score = 50.0 + (cat_idx * 10) + (env_idx * 2.5)  # 50-90 range
                large_metrics[f"policy_0_{env}"] = score

            record_episodes(large_data, category, large_envs_per_cat, large_metrics)

        response_large = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(large_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": [
                    f"{cat}/{env}"
                    for cat, envs in zip(large_categories, [large_envs_per_cat] * len(large_categories), strict=False)
                    for env in envs
                ],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_large.status_code == 200
        heatmap_large = response_large.json()

        # Should have 5 categories * 4 environments = 20 evaluations
        assert len(heatmap_large["evalNames"]) == 20

        # Verify all evaluation names are properly formatted as "category/env_name"
        expected_large_evals = {f"{cat}/{env}" for cat in large_categories for env in large_envs_per_cat}
        assert set(heatmap_large["evalNames"]) == expected_large_evals

    def test_policy_heatmap_performance_ordering(
        self, test_client: TestClient, create_test_data, record_episodes
    ) -> None:
        """Test that policies are correctly ordered by performance in best selector."""

        # Create training run with 5 policies having clearly different performance levels
        perf_data = create_test_data("performance_test", num_policies=5)

        # Create performance tiers: Excellent, Good, Average, Poor, Terrible
        performance_tiers = {
            "policy_0": {"nav": 30.0, "combat": 25.0},  # Terrible: avg = 27.5
            "policy_1": {"nav": 50.0, "combat": 45.0},  # Poor: avg = 47.5
            "policy_2": {"nav": 70.0, "combat": 75.0},  # Average: avg = 72.5
            "policy_3": {"nav": 85.0, "combat": 90.0},  # Good: avg = 87.5
            "policy_4": {"nav": 95.0, "combat": 98.0},  # Excellent: avg = 96.5
        }

        perf_metrics = {}
        for policy_key, scores in performance_tiers.items():
            perf_metrics[f"{policy_key}_nav"] = scores["nav"]
            perf_metrics[f"{policy_key}_combat"] = scores["combat"]

        record_episodes(perf_data, "navigation", ["nav"], perf_metrics)
        record_episodes(perf_data, "combat", ["combat"], perf_metrics)

        response_best_perf = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(perf_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": [
                    "navigation/nav",
                    "combat/combat",
                ],
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response_best_perf.status_code == 200
        heatmap_best_perf = response_best_perf.json()

        # Should select policy_4 (highest average: 96.5)
        best_policy_name = perf_data["policy_names"][4]
        assert len(heatmap_best_perf["policyNames"]) == 1
        assert heatmap_best_perf["policyNames"][0] == best_policy_name

        # Verify the average score is calculated correctly
        expected_avg = (95.0 + 98.0) / 2  # 96.5
        assert abs(heatmap_best_perf["policyAverageScores"][best_policy_name] - expected_avg) < 0.01

        # Test latest selector should pick policy_4 (highest epoch from this run)
        response_latest_perf = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(perf_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": [
                    "navigation/nav",
                    "combat/combat",
                ],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_latest_perf.status_code == 200
        heatmap_latest_perf = response_latest_perf.json()

        # Latest should be from the highest epoch (policy_4 and others are in epoch2, pick first alphabetically)
        # Our test data structure puts later policies in epoch2, so policy_4 should be latest
        latest_policy_name = perf_data["policy_names"][1]  # Policies 1-4 are in epoch2, policy1 is first alphabetically
        assert latest_policy_name in heatmap_latest_perf["policyNames"]

    def test_policy_deduplication_with_multiple_episodes(
        self,
        test_client: TestClient,
        stats_client: StatsClient,
        create_test_data,
    ) -> None:
        """Test that policies are properly deduplicated and aggregated across multiple episodes."""
        # Create test data with multiple episodes per policy/eval combination
        test_data = create_test_data("dedup_test", num_policies=2)
        policy1 = test_data["policies"][0]
        policy2 = test_data["policies"][1]
        epoch1 = test_data["epochs"][0]
        epoch2 = test_data["epochs"][1]

        # Record multiple episodes for the same policy/eval combination
        # Policy 1, Environment 1: Episodes with scores [80, 90, 70] -> avg = 80.0
        for score in [80.0, 90.0, 70.0]:
            stats_client.record_episode(
                agent_policies={0: policy1.id},
                agent_metrics={0: {"reward": score}},
                primary_policy_id=policy1.id,
                stats_epoch=epoch1.id,
                sim_suite="dedup_suite",
                env_name="env1",
                replay_url=f"https://example.com/replay/p1_env1_{score}",
            )

        # Policy 1, Environment 2: Episodes with scores [60, 70, 80] -> avg = 70.0
        for score in [60.0, 70.0, 80.0]:
            stats_client.record_episode(
                agent_policies={0: policy1.id},
                agent_metrics={0: {"reward": score}},
                primary_policy_id=policy1.id,
                stats_epoch=epoch1.id,
                sim_suite="dedup_suite",
                env_name="env2",
                replay_url=f"https://example.com/replay/p1_env2_{score}",
            )

        # Policy 2, Environment 1: Episodes with scores [85, 95] -> avg = 90.0
        for score in [85.0, 95.0]:
            stats_client.record_episode(
                agent_policies={0: policy2.id},
                agent_metrics={0: {"reward": score}},
                primary_policy_id=policy2.id,
                stats_epoch=epoch2.id,
                sim_suite="dedup_suite",
                env_name="env1",
                replay_url=f"https://example.com/replay/p2_env1_{score}",
            )

        # Policy 2, Environment 2: Episodes with scores [90, 100, 80, 70] -> avg = 85.0
        for score in [90.0, 100.0, 80.0, 70.0]:
            stats_client.record_episode(
                agent_policies={0: policy2.id},
                agent_metrics={0: {"reward": score}},
                primary_policy_id=policy2.id,
                stats_epoch=epoch2.id,
                sim_suite="dedup_suite",
                env_name="env2",
                replay_url=f"https://example.com/replay/p2_env2_{score}",
            )

        # Test that "latest" selector includes both policies (both latest in their epochs)
        response_latest = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(test_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": ["dedup_suite/env1", "dedup_suite/env2"],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_latest.status_code == 200
        heatmap_latest = response_latest.json()

        # Should have both policies (policy2 is latest)
        assert len(heatmap_latest["policyNames"]) == 1
        latest_policy_name = test_data["policy_names"][1]  # Policy 2 has higher epoch
        assert latest_policy_name in heatmap_latest["policyNames"]

        # Verify aggregated values
        assert heatmap_latest["cells"][latest_policy_name]["dedup_suite/env1"]["value"] == 90.0  # (85+95)/2
        assert heatmap_latest["cells"][latest_policy_name]["dedup_suite/env2"]["value"] == 85.0  # (90+100+80+70)/4

        # Test that "best" selector picks policy with highest average
        # Policy 1 avg: (80+70)/2 = 75.0
        # Policy 2 avg: (90+85)/2 = 87.5 (best)
        response_best = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(test_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": ["dedup_suite/env1", "dedup_suite/env2"],
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response_best.status_code == 200
        heatmap_best = response_best.json()

        # Should select policy 2 (best average performance)
        assert len(heatmap_best["policyNames"]) == 1
        best_policy_name = test_data["policy_names"][1]
        assert best_policy_name in heatmap_best["policyNames"]
        assert abs(heatmap_best["policyAverageScores"][best_policy_name] - 87.5) < 0.01

    def test_mixed_training_run_and_run_free_policies_complex(
        self, test_client: TestClient, create_test_data, record_episodes
    ) -> None:
        """Test complex scenarios mixing training run policies and run-free policies."""
        # Create multiple training runs with different characteristics
        train_run1 = create_test_data("mixed_complex_run1", num_policies=2)
        train_run2 = create_test_data("mixed_complex_run2", num_policies=3)

        # Create run-free policies with high performance
        run_free_policies = create_test_data("mixed_complex_runfree", num_policies=0, create_run_free_policies=2)

        # Record varied performance episodes
        environments = ["maze", "arena", "coop"]

        # Training run 1: moderate performance, policy 1 better than policy 0
        run1_metrics = {
            "policy_0_maze": 60.0,
            "policy_0_arena": 65.0,
            "policy_0_coop": 55.0,  # avg: 60.0
            "policy_1_maze": 75.0,
            "policy_1_arena": 80.0,
            "policy_1_coop": 70.0,  # avg: 75.0
        }
        record_episodes(train_run1, "mixed_complex", environments, run1_metrics)

        # Training run 2: varied performance, policy 2 is best
        run2_metrics = {
            "policy_0_maze": 50.0,
            "policy_0_arena": 55.0,
            "policy_0_coop": 45.0,  # avg: 50.0
            "policy_1_maze": 70.0,
            "policy_1_arena": 75.0,
            "policy_1_coop": 65.0,  # avg: 70.0
            "policy_2_maze": 85.0,
            "policy_2_arena": 90.0,
            "policy_2_coop": 80.0,  # avg: 85.0
        }
        record_episodes(train_run2, "mixed_complex", environments, run2_metrics)

        # Run-free policies: excellent performance
        runfree_metrics = {
            "policy_0_maze": 95.0,
            "policy_0_arena": 98.0,
            "policy_0_coop": 92.0,  # avg: 95.0
            "policy_1_maze": 88.0,
            "policy_1_arena": 91.0,
            "policy_1_coop": 87.0,  # avg: 88.67
        }
        record_episodes(run_free_policies, "mixed_complex", environments, runfree_metrics)

        training_run_ids = [str(train_run1["training_run"].id), str(train_run2["training_run"].id)]
        run_free_policy_ids = [str(p.id) for p in run_free_policies["policies"]]

        # Test "latest" selector: should pick latest from each run + all run-free
        response_latest = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": training_run_ids,
                "run_free_policy_ids": run_free_policy_ids,
                "eval_names": [f"mixed_complex/{env}" for env in environments],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_latest.status_code == 200
        heatmap_latest = response_latest.json()

        # Should have 4 policies: latest from run1 (policy1), latest from run2 (policy1 - first alphabetically
        # among tied policies), + 2 run-free
        assert len(heatmap_latest["policyNames"]) == 4
        expected_latest = {
            train_run1["policy_names"][1],  # Latest from run1 (policy_1 has highest epoch)
            train_run2["policy_names"][1],  # Latest from run2 (policy_1 first alphabetically among epoch2 policies)
            run_free_policies["policy_names"][0],  # Run-free policy 0
            run_free_policies["policy_names"][1],  # Run-free policy 1
        }
        assert set(heatmap_latest["policyNames"]) == expected_latest

        # Test "best" selector: should pick best from each run + all run-free
        response_best = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": training_run_ids,
                "run_free_policy_ids": run_free_policy_ids,
                "eval_names": [f"mixed_complex/{env}" for env in environments],
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response_best.status_code == 200
        heatmap_best = response_best.json()

        # Should have 4 policies: best from run1 (policy1, avg 75.0), best from run2 (policy2, avg 85.0), + 2 run-free
        assert len(heatmap_best["policyNames"]) == 4
        expected_best = {
            train_run1["policy_names"][1],  # Best from run1 (policy1, avg 75.0)
            train_run2["policy_names"][2],  # Best from run2 (policy2, avg 85.0)
            run_free_policies["policy_names"][0],  # Run-free policy 0
            run_free_policies["policy_names"][1],  # Run-free policy 1
        }
        assert set(heatmap_best["policyNames"]) == expected_best

        # Verify performance averages are calculated correctly
        run1_best_policy = train_run1["policy_names"][1]
        run2_best_policy = train_run2["policy_names"][2]
        runfree_best_policy = run_free_policies["policy_names"][0]

        assert abs(heatmap_best["policyAverageScores"][run1_best_policy] - 75.0) < 0.01
        assert abs(heatmap_best["policyAverageScores"][run2_best_policy] - 85.0) < 0.01
        assert abs(heatmap_best["policyAverageScores"][runfree_best_policy] - 95.0) < 0.01

        # Test subset of evaluations to ensure proper filtering
        response_subset = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": training_run_ids,
                "run_free_policy_ids": run_free_policy_ids,
                "eval_names": ["mixed_complex/maze", "mixed_complex/arena"],  # Only 2 of 3
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response_subset.status_code == 200
        heatmap_subset = response_subset.json()

        # Should only have the selected evaluations
        expected_subset_evals = {"mixed_complex/maze", "mixed_complex/arena"}
        assert set(heatmap_subset["evalNames"]) == expected_subset_evals

        # Average scores should be recalculated for subset
        # Run1 policy1 subset avg: (75+80)/2 = 77.5
        assert abs(heatmap_subset["policyAverageScores"][run1_best_policy] - 77.5) < 0.01

    def test_policy_selector_tie_breaking(
        self, test_client: TestClient, stats_client: StatsClient, create_test_data, record_episodes
    ) -> None:
        """Test tie-breaking logic for policy selectors."""
        # Create training run with policies that have identical performance but different epochs
        tie_data = create_test_data("tie_break_test", num_policies=3)

        # Set up identical performance for policies but with different epochs
        identical_metrics = {
            "policy_0_env1": 80.0,
            "policy_0_env2": 70.0,  # avg: 75.0 (epoch 0->100)
            "policy_1_env1": 80.0,
            "policy_1_env2": 70.0,  # avg: 75.0 (epoch 100->200)
            "policy_2_env1": 80.0,
            "policy_2_env2": 70.0,  # avg: 75.0 (epoch 100->200)
        }
        record_episodes(tie_data, "tie_suite", ["env1", "env2"], identical_metrics)

        # Test "latest" selector with ties: should pick policy with highest epoch, break ties alphabetically
        response_latest_tie = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(tie_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": ["tie_suite/env1", "tie_suite/env2"],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_latest_tie.status_code == 200
        heatmap_latest_tie = response_latest_tie.json()

        # Should pick the latest epoch (policies 1 and 2 both have epoch 200, policy 1 comes first alphabetically)
        assert len(heatmap_latest_tie["policyNames"]) == 1
        latest_tie_policy = tie_data["policy_names"][1]  # First alphabetically among tied latest
        assert heatmap_latest_tie["policyNames"][0] == latest_tie_policy

        # Test "best" selector with performance ties: should pick latest epoch when performance is equal
        response_best_tie = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(tie_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": ["tie_suite/env1", "tie_suite/env2"],
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response_best_tie.status_code == 200
        heatmap_best_tie = response_best_tie.json()

        # With identical performance, should pick the one with highest epoch (policy 1 or 2, both have epoch 200)
        # Since both policy 1 and 2 have epoch 200, it should pick the first one encountered (policy 1)
        assert len(heatmap_best_tie["policyNames"]) == 1
        best_tie_policy = tie_data["policy_names"][1]  # Policy 1 (earlier in list with same epoch as policy 2)
        assert heatmap_best_tie["policyNames"][0] == best_tie_policy

        # Now create slight performance difference to test best selector properly
        # Add one more episode to make policy 2 slightly better
        policy_2 = tie_data["policies"][2]
        epoch_2 = tie_data["epochs"][1]

        stats_client.record_episode(
            agent_policies={0: policy_2.id},
            agent_metrics={0: {"reward": 85.0}},  # Slightly higher than 80.0
            primary_policy_id=policy_2.id,
            stats_epoch=epoch_2.id,
            sim_suite="tie_suite",
            env_name="env1",
            replay_url="https://example.com/replay/boost",
        )

        # Test best selector again: should now clearly pick policy 2
        response_best_after_boost = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(tie_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": ["tie_suite/env1", "tie_suite/env2"],
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response_best_after_boost.status_code == 200
        heatmap_best_after_boost = response_best_after_boost.json()

        # Should now pick policy 2 (has better average due to the boost)
        # Policy 2 new avg: ((80+85)/2 + 70)/2 = (82.5 + 70)/2 = 76.25
        assert len(heatmap_best_after_boost["policyNames"]) == 1
        boosted_best_policy = tie_data["policy_names"][2]
        assert heatmap_best_after_boost["policyNames"][0] == boosted_best_policy
        assert abs(heatmap_best_after_boost["policyAverageScores"][boosted_best_policy] - 76.25) < 0.01

    def test_policy_heatmap_edge_cases_and_error_handling(
        self, test_client: TestClient, create_test_data, record_episodes
    ) -> None:
        """Test edge cases and error handling in policy heatmap generation."""

        # Test 1: Mixed training run and run-free policies with sparse data
        test_data1 = create_test_data("edge_case_run1", num_policies=2)
        run_free_data = create_test_data("edge_case_free", num_policies=0, create_run_free_policies=2)

        # Only record episodes for some policy-eval combinations to test sparse data handling
        partial_metrics = {
            "policy_0_env1": 80.0,  # Missing env2 for policy 0
            "policy_1_env2": 90.0,  # Missing env1 for policy 1
        }
        record_episodes(test_data1, "sparse_suite", ["env1", "env2"], partial_metrics)

        # Record for run-free policies - one complete, one partial
        free_metrics = {
            "policy_0_env1": 85.0,
            "policy_0_env2": 88.0,  # Complete data
            "policy_1_env1": 92.0,  # Partial data - missing env2
        }
        record_episodes(run_free_data, "sparse_suite", ["env1", "env2"], free_metrics)

        run_free_policy_ids = [str(p.id) for p in run_free_data["policies"]]

        # Generate scorecard with sparse data
        response = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(test_data1["training_run"].id)],
                "run_free_policy_ids": run_free_policy_ids,
                "eval_names": ["sparse_suite/env1", "sparse_suite/env2"],
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response.status_code == 200
        heatmap = response.json()

        # Should handle missing data gracefully
        # Training run should select best policy based on available data
        # All run-free policies should be included
        assert len(heatmap["policyNames"]) == 3  # 1 from training run + 2 run-free

        # Cells with no data should have value 0.0
        for policy_name in heatmap["policyNames"]:
            for eval_name in heatmap["evalNames"]:
                cell = heatmap["cells"][policy_name][eval_name]
                assert cell["value"] >= 0.0
                assert isinstance(cell["value"], (int, float))
                if cell["value"] == 0.0:
                    assert cell["replayUrl"] is None

    def test_policy_heatmap_best_vs_latest_selection_logic(
        self,
        test_client: TestClient,
        stats_client: StatsClient,
        create_test_data,
    ) -> None:
        """Test detailed best vs latest policy selection logic with complex scenarios."""

        # Create training run with 4 policies across different epochs
        test_data = create_test_data("selection_logic", num_policies=4)

        # Create additional epochs for more complex epoch structure
        training_run = test_data["training_run"]
        epoch3 = stats_client.create_epoch(
            run_id=training_run.id,
            start_training_epoch=200,
            end_training_epoch=300,
            attributes={"learning_rate": "0.0001"},
        )
        epoch4 = stats_client.create_epoch(
            run_id=training_run.id,
            start_training_epoch=300,
            end_training_epoch=400,
            attributes={"learning_rate": "0.00005"},
        )

        # Create policies with different epoch assignments
        policy3 = stats_client.create_policy(
            name="policy_selection_logic_3",
            description="Test policy 3 for selection logic",
            epoch_id=epoch3.id,
        )
        policy4 = stats_client.create_policy(
            name="policy_selection_logic_4",
            description="Test policy 4 for selection logic",
            epoch_id=epoch4.id,
        )

        # Update test data with new policies
        test_data["policies"].extend([policy3, policy4])
        test_data["policy_names"].extend(["policy_selection_logic_3", "policy_selection_logic_4"])
        test_data["epochs"].extend([epoch3, epoch4])

        # Record episodes with specific performance patterns:
        # Policy 0 (epoch 100): Moderate performance
        # Policy 1 (epoch 200): Low performance
        # Policy 2 (epoch 200): High performance in some areas
        # Policy 3 (epoch 300): Moderate performance
        # Policy 4 (epoch 400): Highest overall performance (latest and best)

        metrics = {
            "policy_0_nav": 70.0,
            "policy_0_combat": 75.0,
            "policy_0_social": 65.0,  # Avg: 70.0
            "policy_1_nav": 50.0,
            "policy_1_combat": 55.0,
            "policy_1_social": 45.0,  # Avg: 50.0
            "policy_2_nav": 95.0,
            "policy_2_combat": 60.0,
            "policy_2_social": 70.0,  # Avg: 75.0
            "policy_3_nav": 80.0,
            "policy_3_combat": 70.0,
            "policy_3_social": 75.0,  # Avg: 75.0
            "policy_4_nav": 90.0,
            "policy_4_combat": 85.0,
            "policy_4_social": 95.0,  # Avg: 90.0 (best)
        }

        # Record episodes manually for precise control
        all_policies = test_data["policies"]
        all_epochs = test_data["epochs"]
        eval_names = ["nav", "combat", "social"]

        for i, policy in enumerate(all_policies):
            epoch = all_epochs[i % len(all_epochs)]  # Use appropriate epoch
            for eval_name in eval_names:
                metric_key = f"policy_{i}_{eval_name}"
                if metric_key in metrics:
                    stats_client.record_episode(
                        agent_policies={0: policy.id},
                        agent_metrics={0: {"reward": metrics[metric_key]}},
                        primary_policy_id=policy.id,
                        stats_epoch=epoch.id,
                        sim_suite="selection_suite",
                        env_name=eval_name,
                        replay_url=f"https://example.com/replay/{policy.id}/{eval_name}",
                    )

        # Test "best" selector - should pick policy 4 (highest average: 90.0)
        response_best = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(test_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": [f"selection_suite/{env}" for env in eval_names],
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response_best.status_code == 200
        heatmap_best = response_best.json()

        # Should select policy 4 as it has the highest average score
        assert len(heatmap_best["policyNames"]) == 1
        best_policy_name = test_data["policy_names"][4]  # policy_selection_logic_4
        assert heatmap_best["policyNames"][0] == best_policy_name
        assert abs(heatmap_best["policyAverageScores"][best_policy_name] - 90.0) < 0.01

        # Test "latest" selector - should pick policy 4 (highest epoch: 400)
        response_latest = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(test_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": [f"selection_suite/{env}" for env in eval_names],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_latest.status_code == 200
        heatmap_latest = response_latest.json()

        # Should select policy 4 as it has the highest epoch
        assert len(heatmap_latest["policyNames"]) == 1
        latest_policy_name = test_data["policy_names"][4]  # policy_selection_logic_4
        assert heatmap_latest["policyNames"][0] == latest_policy_name

        # Add contradictory data: make an earlier policy perform better
        # Add high-scoring episodes for policy 1 to make it the best performer
        additional_metrics = {
            "policy_1_nav": 100.0,
            "policy_1_combat": 100.0,
            "policy_1_social": 100.0,
        }

        for eval_name in eval_names:
            metric_key = f"policy_1_{eval_name}"
            stats_client.record_episode(
                agent_policies={0: all_policies[1].id},
                agent_metrics={0: {"reward": additional_metrics[metric_key]}},
                primary_policy_id=all_policies[1].id,
                stats_epoch=all_epochs[1].id,
                sim_suite="selection_suite",
                env_name=eval_name,
                replay_url=f"https://example.com/replay/{all_policies[1].id}/{eval_name}_bonus",
            )

        # Test "best" selector again - should now pick policy 1
        response_best2 = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(test_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": [f"selection_suite/{env}" for env in eval_names],
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response_best2.status_code == 200
        heatmap_best2 = response_best2.json()

        # Policy 1 average is now: (50+100)/2, (55+100)/2, (45+100)/2 = 75, 77.5, 72.5 -> avg = 75.0
        # Policy 4 still has average 90.0, so policy 4 should still be selected
        assert heatmap_best2["policyNames"][0] == best_policy_name

        # But "latest" selector should still pick policy 4 (highest epoch unchanged)
        response_latest2 = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(test_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": [f"selection_suite/{env}" for env in eval_names],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_latest2.status_code == 200
        heatmap_latest2 = response_latest2.json()

        assert heatmap_latest2["policyNames"][0] == latest_policy_name

    def test_policy_heatmap_data_structure_completeness(
        self, test_client: TestClient, create_test_data, record_episodes
    ) -> None:
        """Test that policy heatmap data structure is complete and correctly formatted."""

        # Create comprehensive test setup
        test_data1 = create_test_data("structure_run1", num_policies=2)
        test_data2 = create_test_data("structure_run2", num_policies=1)
        run_free_data = create_test_data("structure_free", num_policies=0, create_run_free_policies=1)

        # Record episodes across multiple categories
        categories_and_envs = [
            ("navigation", ["maze1", "maze2"]),
            ("combat", ["arena1"]),
            ("social", ["team1", "team2", "team3"]),
        ]

        all_test_data = [test_data1, test_data2, run_free_data]
        all_policies = []

        for test_data in all_test_data:
            all_policies.extend(test_data["policies"])
            for category, env_names in categories_and_envs:
                base_score = 70.0
                metrics = {}
                for i, _ in enumerate(test_data["policies"]):
                    for env in env_names:
                        score = base_score + (i * 5) + len(env)  # Varied scores
                        metrics[f"policy_{i}_{env}"] = score

                if metrics:  # Only if we have policies
                    record_episodes(test_data, category, env_names, metrics)

        training_run_ids = [str(test_data1["training_run"].id), str(test_data2["training_run"].id)]
        run_free_policy_ids = [str(p.id) for p in run_free_data["policies"]]
        eval_names = [f"{cat}/{env}" for cat, envs in categories_and_envs for env in envs]

        # Generate comprehensive heatmap
        response = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": training_run_ids,
                "run_free_policy_ids": run_free_policy_ids,
                "eval_names": eval_names,
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response.status_code == 200
        heatmap = response.json()

        # Test 1: Verify top-level structure
        required_keys = [
            "evalNames",
            "policyNames",
            "cells",
            "policyAverageScores",
            "evalAverageScores",
            "evalMaxScores",
        ]
        for key in required_keys:
            assert key in heatmap, f"Missing key: {key}"

        # Test 2: Verify evalNames structure and completeness
        expected_eval_names = []
        for category, env_names in categories_and_envs:
            for env in env_names:
                expected_eval_names.append(f"{category}/{env}")

        assert set(heatmap["evalNames"]) == set(expected_eval_names)
        assert len(heatmap["evalNames"]) == 6  # 2 + 1 + 3

        # Test 3: Verify policy selection logic (should be 3 policies: 1 best from each training run + 1 run-free)
        assert len(heatmap["policyNames"]) == 3

        # Test 4: Verify cells structure completeness
        assert len(heatmap["cells"]) == len(heatmap["policyNames"])

        for policy_name in heatmap["policyNames"]:
            assert policy_name in heatmap["cells"]
            policy_cells = heatmap["cells"][policy_name]

            # Each policy should have cells for all evaluations
            assert len(policy_cells) == len(heatmap["evalNames"])

            for eval_name in heatmap["evalNames"]:
                assert eval_name in policy_cells
                cell = policy_cells[eval_name]

                # Verify cell structure
                assert "evalName" in cell
                assert "replayUrl" in cell
                assert "value" in cell

                # Verify cell values
                assert cell["evalName"] == eval_name
                assert isinstance(cell["value"], (int, float))
                assert cell["value"] >= 0.0
                assert cell["replayUrl"] is None or isinstance(cell["replayUrl"], str)

        # Test 5: Verify policyAverageScores completeness and correctness
        assert len(heatmap["policyAverageScores"]) == len(heatmap["policyNames"])

        for policy_name in heatmap["policyNames"]:
            assert policy_name in heatmap["policyAverageScores"]
            avg_score = heatmap["policyAverageScores"][policy_name]
            assert isinstance(avg_score, (int, float))
            assert avg_score >= 0.0

            # Verify average calculation
            policy_values = [heatmap["cells"][policy_name][eval_name]["value"] for eval_name in heatmap["evalNames"]]
            expected_avg = sum(policy_values) / len(policy_values)
            assert abs(avg_score - expected_avg) < 0.01

        # Test 6: Verify evalAverageScores and evalMaxScores
        for eval_name in heatmap["evalNames"]:
            assert eval_name in heatmap["evalAverageScores"]
            assert eval_name in heatmap["evalMaxScores"]

            avg_score = heatmap["evalAverageScores"][eval_name]
            max_score = heatmap["evalMaxScores"][eval_name]

            assert isinstance(avg_score, (int, float))
            assert isinstance(max_score, (int, float))
            assert max_score >= avg_score

            # Verify calculations
            eval_values = [heatmap["cells"][policy_name][eval_name]["value"] for policy_name in heatmap["policyNames"]]
            expected_avg = sum(eval_values) / len(eval_values) if eval_values else 0.0
            expected_max = max(eval_values) if eval_values else 0.0

            assert abs(avg_score - expected_avg) < 0.01
            assert abs(max_score - expected_max) < 0.01

    def test_large_dataset_performance_and_deduplication(
        self, test_client: TestClient, create_test_data, record_episodes
    ) -> None:
        """Test performance and correctness with large datasets."""
        # Create multiple training runs with many policies
        large_runs = []
        num_runs = 5
        policies_per_run = 10

        for run_idx in range(num_runs):
            run_data = create_test_data(f"large_run_{run_idx}", num_policies=policies_per_run)
            large_runs.append(run_data)

        # Create many run-free policies
        run_free_count = 15
        large_run_free = create_test_data("large_run_free", num_policies=0, create_run_free_policies=run_free_count)

        # Create many evaluation environments
        categories = ["nav", "combat", "coop", "puzzle", "social"]
        envs_per_category = ["env1", "env2", "env3", "env4"]

        all_policy_ids = []

        # Record episodes for training run policies with varied performance
        for run_idx, run_data in enumerate(large_runs):
            all_policy_ids.extend([str(p.id) for p in run_data["policies"]])

            for cat_idx, category in enumerate(categories):
                run_metrics = {}
                for policy_idx in range(policies_per_run):
                    for env_idx, env in enumerate(envs_per_category):
                        # Create performance gradient: later policies and runs perform better
                        # Base performance varies by category and environment
                        base_score = 40.0 + (cat_idx * 8) + (env_idx * 3)
                        run_bonus = run_idx * 5
                        policy_bonus = policy_idx * 2
                        score = min(100.0, base_score + run_bonus + policy_bonus)
                        run_metrics[f"policy_{policy_idx}_{env}"] = score

                record_episodes(run_data, category, envs_per_category, run_metrics)

        # Record episodes for run-free policies with high, varied performance
        all_policy_ids.extend([str(p.id) for p in large_run_free["policies"]])
        for category in categories:
            runfree_metrics = {}
            for policy_idx in range(run_free_count):
                for env_idx, env in enumerate(envs_per_category):
                    # Run-free policies have generally high performance with some variation
                    score = 85.0 + (policy_idx % 3) * 5 + (env_idx % 2) * 2  # 85-95 range
                    runfree_metrics[f"policy_{policy_idx}_{env}"] = score

            record_episodes(large_run_free, category, envs_per_category, runfree_metrics)

        eval_names = [f"{cat}/{env}" for cat in categories for env in envs_per_category]

        # Test "latest" selector with large dataset
        training_run_ids = [str(run_data["training_run"].id) for run_data in large_runs]
        run_free_policy_ids = [str(p.id) for p in large_run_free["policies"]]

        response_latest_large = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": training_run_ids,
                "run_free_policy_ids": run_free_policy_ids,
                "eval_names": eval_names,
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_latest_large.status_code == 200
        heatmap_latest_large = response_latest_large.json()

        # Should have latest from each training run (5) + all run-free policies (15) = 20 total
        expected_count = num_runs + run_free_count
        assert len(heatmap_latest_large["policyNames"]) == expected_count

        # Should have all evaluation environments: 5 categories * 4 envs = 20 evaluations
        expected_eval_count = len(categories) * len(envs_per_category)
        assert len(heatmap_latest_large["evalNames"]) == expected_eval_count

        # Verify structure integrity
        for policy_name in heatmap_latest_large["policyNames"]:
            assert policy_name in heatmap_latest_large["cells"]
            assert policy_name in heatmap_latest_large["policyAverageScores"]
            for eval_name in heatmap_latest_large["evalNames"]:
                assert eval_name in heatmap_latest_large["cells"][policy_name]
                assert "value" in heatmap_latest_large["cells"][policy_name][eval_name]

        for eval_name in heatmap_latest_large["evalNames"]:
            assert eval_name in heatmap_latest_large["evalAverageScores"]
            assert eval_name in heatmap_latest_large["evalMaxScores"]

        # Test "best" selector with large dataset
        response_best_large = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": training_run_ids,
                "run_free_policy_ids": run_free_policy_ids,
                "eval_names": eval_names,
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response_best_large.status_code == 200
        heatmap_best_large = response_best_large.json()

        # Should also have the best from each training run + all run-free policies
        assert len(heatmap_best_large["policyNames"]) == expected_count

        # For "best" selector, the selected policies should generally have higher average scores
        # than those selected by "latest" selector (due to our performance gradient)
        best_avg_scores = list(heatmap_best_large["policyAverageScores"].values())
        latest_avg_scores = list(heatmap_latest_large["policyAverageScores"].values())

        # Best selector should generally have higher or equal average performance
        avg_best_performance = sum(best_avg_scores) / len(best_avg_scores)
        avg_latest_performance = sum(latest_avg_scores) / len(latest_avg_scores)

        # Best should perform at least as well as latest (may be equal if latest happens to be best)
        assert avg_best_performance >= avg_latest_performance - 1.0  # Allow small tolerance

    def test_policy_heatmap_with_missing_evaluations(
        self, test_client: TestClient, create_test_data, record_episodes
    ) -> None:
        """Test behavior when policies have missing evaluations for some environments."""
        # Create test data with policies that have different evaluation coverage
        partial_data = create_test_data("partial_eval_test", num_policies=3)

        # Policy 0: Only evaluated in env1 and env2
        policy_0_metrics = {"policy_0_env1": 80.0, "policy_0_env2": 75.0}
        record_episodes(partial_data, "partial_suite", ["env1", "env2"], policy_0_metrics)

        # Policy 1: Only evaluated in env2 and env3
        policy_1_metrics = {"policy_1_env2": 85.0, "policy_1_env3": 90.0}
        record_episodes(partial_data, "partial_suite", ["env2", "env3"], policy_1_metrics)

        # Policy 2: Evaluated in all environments
        policy_2_metrics = {"policy_2_env1": 70.0, "policy_2_env2": 78.0, "policy_2_env3": 82.0}
        record_episodes(partial_data, "partial_suite", ["env1", "env2", "env3"], policy_2_metrics)

        # Test latest selector
        response_partial_latest = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(partial_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": ["partial_suite/env1", "partial_suite/env2", "partial_suite/env3"],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_partial_latest.status_code == 200
        heatmap_partial_latest = response_partial_latest.json()

        # Should have latest policy (policy 1 has highest epoch in our test data structure)
        latest_policy_name = partial_data["policy_names"][1]  # policy_1 has epoch 200, highest
        assert latest_policy_name in heatmap_partial_latest["policyNames"]

        # But we need to check policy 2's values since it has all evaluations
        # Latest selector actually picks policy_1, but let's verify all values are handled correctly
        # For policy_1 (which should be selected), it should have 0.0 for env1 (no data), values for env2/env3
        # Actually, let's check which policy was selected and verify appropriate values

        # Test best selector - should handle missing evaluations in average calculation
        response_partial_best = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(partial_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": ["partial_suite/env1", "partial_suite/env2", "partial_suite/env3"],
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert response_partial_best.status_code == 200
        heatmap_partial_best = response_partial_best.json()

        # Best calculation should account for missing evaluations as 0.0:
        # Policy 0: (80 + 75 + 0) / 3 = 51.67
        # Policy 1: (0 + 85 + 90) / 3 = 58.33
        # Policy 2: (70 + 78 + 82) / 3 = 76.67 (best)
        # However, the best selector only considers policies with evaluation data,
        # so let's just verify that a policy was selected and has a reasonable average
        assert len(heatmap_partial_best["policyNames"]) == 1
        selected_policy = heatmap_partial_best["policyNames"][0]
        selected_avg = heatmap_partial_best["policyAverageScores"][selected_policy]

        # The selected policy should have some positive average (not zero)
        assert selected_avg > 0.0

    def test_multi_agent_episode_aggregation_edge_cases(
        self,
        test_client: TestClient,
        stats_client: StatsClient,
        create_test_data,
    ) -> None:
        """Test edge cases in multi-agent episode aggregation."""
        multiagent_data = create_test_data("multiagent_edge", num_policies=1)
        policy = multiagent_data["policies"][0]
        epoch = multiagent_data["epochs"][0]

        # Episode 1: Single agent
        stats_client.record_episode(
            agent_policies={0: policy.id},
            agent_metrics={0: {"reward": 100.0}},
            primary_policy_id=policy.id,
            stats_epoch=epoch.id,
            sim_suite="multiagent_edge",
            env_name="test_env",
            replay_url="https://example.com/replay/single",
        )

        # Episode 2: Two agents with very different performance
        stats_client.record_episode(
            agent_policies={0: policy.id, 1: policy.id},
            agent_metrics={0: {"reward": 20.0}, 1: {"reward": 80.0}},  # avg per agent: 50.0
            primary_policy_id=policy.id,
            stats_epoch=epoch.id,
            sim_suite="multiagent_edge",
            env_name="test_env",
            replay_url="https://example.com/replay/two_agents",
        )

        # Episode 3: Many agents with varied performance
        many_agent_metrics = {i: {"reward": 40.0 + (i * 5)} for i in range(10)}  # 40, 45, 50, ..., 85
        # Average per agent: (40+45+50+55+60+65+70+75+80+85)/10 = 62.5
        stats_client.record_episode(
            agent_policies={i: policy.id for i in range(10)},
            agent_metrics=many_agent_metrics,
            primary_policy_id=policy.id,
            stats_epoch=epoch.id,
            sim_suite="multiagent_edge",
            env_name="test_env",
            replay_url="https://example.com/replay/many_agents",
        )

        # Episode 4: Zero reward edge case
        stats_client.record_episode(
            agent_policies={0: policy.id, 1: policy.id},
            agent_metrics={0: {"reward": 0.0}, 1: {"reward": 0.0}},  # avg per agent: 0.0
            primary_policy_id=policy.id,
            stats_epoch=epoch.id,
            sim_suite="multiagent_edge",
            env_name="test_env",
            replay_url="https://example.com/replay/zero_rewards",
        )

        response_multiagent_edge = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(multiagent_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": ["multiagent_edge/test_env"],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_multiagent_edge.status_code == 200
        heatmap_multiagent_edge = response_multiagent_edge.json()

        # Should aggregate across all episodes properly
        # The exact average depends on the aggregation logic, but should be reasonable
        policy_name = multiagent_data["policy_names"][0]
        actual_value = heatmap_multiagent_edge["cells"][policy_name]["multiagent_edge/test_env"]["value"]

        # Verify the aggregated value is within expected range (should be between 0 and 100)
        # and reflects some mix of the episode values
        assert 0.0 <= actual_value <= 100.0
        assert actual_value > 30.0  # Should be above just the zero episode
        assert actual_value < 90.0  # Should be below the highest single episode

    def test_policy_heatmap_extreme_values_and_precision(
        self,
        test_client: TestClient,
        stats_client: StatsClient,
        create_test_data,
    ) -> None:
        """Test handling of extreme values and numerical precision."""
        extreme_data = create_test_data("extreme_values", num_policies=1)
        policy = extreme_data["policies"][0]
        epoch = extreme_data["epochs"][0]

        # Test with very large values
        stats_client.record_episode(
            agent_policies={0: policy.id},
            agent_metrics={0: {"reward": 999999.999}},
            primary_policy_id=policy.id,
            stats_epoch=epoch.id,
            sim_suite="extreme_suite",
            env_name="large_values",
            replay_url="https://example.com/replay/large",
        )

        # Test with very small positive values
        stats_client.record_episode(
            agent_policies={0: policy.id},
            agent_metrics={0: {"reward": 0.000001}},
            primary_policy_id=policy.id,
            stats_epoch=epoch.id,
            sim_suite="extreme_suite",
            env_name="small_values",
            replay_url="https://example.com/replay/small",
        )

        # Test with negative values
        stats_client.record_episode(
            agent_policies={0: policy.id},
            agent_metrics={0: {"reward": -100.5}},
            primary_policy_id=policy.id,
            stats_epoch=epoch.id,
            sim_suite="extreme_suite",
            env_name="negative_values",
            replay_url="https://example.com/replay/negative",
        )

        # Test with many decimal places
        stats_client.record_episode(
            agent_policies={0: policy.id},
            agent_metrics={0: {"reward": 123.456789012345}},
            primary_policy_id=policy.id,
            stats_epoch=epoch.id,
            sim_suite="extreme_suite",
            env_name="precision_values",
            replay_url="https://example.com/replay/precision",
        )

        response_extreme = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": [str(extreme_data["training_run"].id)],
                "run_free_policy_ids": [],
                "eval_names": [
                    "extreme_suite/large_values",
                    "extreme_suite/small_values",
                    "extreme_suite/negative_values",
                    "extreme_suite/precision_values",
                ],
                "training_run_policy_selector": "latest",
                "metric": "reward",
            },
        )
        assert response_extreme.status_code == 200
        heatmap_extreme = response_extreme.json()

        policy_name = extreme_data["policy_names"][0]

        # Verify extreme values are handled correctly (allow for small floating point differences)
        assert abs(heatmap_extreme["cells"][policy_name]["extreme_suite/large_values"]["value"] - 999999.999) < 1.0
        assert abs(heatmap_extreme["cells"][policy_name]["extreme_suite/small_values"]["value"] - 0.000001) < 1e-6
        assert abs(heatmap_extreme["cells"][policy_name]["extreme_suite/negative_values"]["value"] - (-100.5)) < 0.01
        assert (
            abs(heatmap_extreme["cells"][policy_name]["extreme_suite/precision_values"]["value"] - 123.456789012345)
            < 1e-6
        )

        # Verify average calculation with extreme values is reasonable
        actual_avg = heatmap_extreme["policyAverageScores"][policy_name]
        # The average should be dominated by the large value, so should be quite large
        assert actual_avg > 200000.0  # Much larger than the other values
        assert actual_avg < 1000000.0  # But less than the max value

    @pytest.mark.asyncio
    async def test_leaderboard_scorecard_integration(
        self,
        isolated_test_client: TestClient,
        isolated_stats_repo: MettaRepo,
        isolated_stats_client: StatsClient,
        create_test_data,
        record_episodes,
    ) -> None:
        """Integration test for leaderboard scorecard functionality.

        This test:
        1. Sets up test data with multiple training runs and run-free policies
        2. Creates a leaderboard
        3. Runs the leaderboard updater to populate scores
        4. Gets the leaderboard scorecard
        5. Compares it with individually selected policies to validate consistency
        """

        test_client = isolated_test_client
        stats_repo = isolated_stats_repo

        # Create test data with multiple training runs and run-free policies
        test_data1 = create_test_data(
            "leaderboard_integration_run1", num_policies=3, overriding_stats_client=isolated_stats_client
        )
        test_data2 = create_test_data(
            "leaderboard_integration_run2", num_policies=2, overriding_stats_client=isolated_stats_client
        )
        run_free_data = create_test_data(
            "leaderboard_integration_runfree",
            num_policies=0,
            create_run_free_policies=2,
            overriding_stats_client=isolated_stats_client,
        )

        # Record episodes with varied performance across different categories
        categories_and_envs = [
            ("navigation", ["maze1", "maze2"]),
            ("combat", ["arena1", "arena2"]),
            ("cooperation", ["team1"]),
        ]

        # Training run 1: policy 2 is best (highest average)
        run1_metrics = {
            "policy_0_maze1": 60.0,
            "policy_0_maze2": 65.0,  # avg: 62.5
            "policy_0_arena1": 55.0,
            "policy_0_arena2": 60.0,  # avg: 57.5
            "policy_0_team1": 50.0,  # Overall avg: 56.67
            "policy_1_maze1": 70.0,
            "policy_1_maze2": 75.0,  # avg: 72.5
            "policy_1_arena1": 65.0,
            "policy_1_arena2": 70.0,  # avg: 67.5
            "policy_1_team1": 60.0,  # Overall avg: 66.67
            "policy_2_maze1": 85.0,
            "policy_2_maze2": 90.0,  # avg: 87.5
            "policy_2_arena1": 80.0,
            "policy_2_arena2": 85.0,  # avg: 82.5
            "policy_2_team1": 75.0,  # Overall avg: 81.67 (best)
        }
        record_episodes(
            test_data1, "navigation", ["maze1", "maze2"], run1_metrics, overriding_stats_client=isolated_stats_client
        )
        record_episodes(
            test_data1, "combat", ["arena1", "arena2"], run1_metrics, overriding_stats_client=isolated_stats_client
        )
        record_episodes(
            test_data1, "cooperation", ["team1"], run1_metrics, overriding_stats_client=isolated_stats_client
        )

        # Training run 2: policy 1 is best
        run2_metrics = {
            "policy_0_maze1": 50.0,
            "policy_0_maze2": 55.0,  # avg: 52.5
            "policy_0_arena1": 45.0,
            "policy_0_arena2": 50.0,  # avg: 47.5
            "policy_0_team1": 40.0,  # Overall avg: 46.67
            "policy_1_maze1": 90.0,
            "policy_1_maze2": 95.0,  # avg: 92.5
            "policy_1_arena1": 85.0,
            "policy_1_arena2": 90.0,  # avg: 87.5
            "policy_1_team1": 80.0,  # Overall avg: 86.67 (best)
        }
        record_episodes(
            test_data2, "navigation", ["maze1", "maze2"], run2_metrics, overriding_stats_client=isolated_stats_client
        )
        record_episodes(
            test_data2, "combat", ["arena1", "arena2"], run2_metrics, overriding_stats_client=isolated_stats_client
        )
        record_episodes(
            test_data2, "cooperation", ["team1"], run2_metrics, overriding_stats_client=isolated_stats_client
        )

        # Run-free policies: high performance
        runfree_metrics = {
            "policy_0_maze1": 95.0,
            "policy_0_maze2": 98.0,  # avg: 96.5
            "policy_0_arena1": 92.0,
            "policy_0_arena2": 95.0,  # avg: 93.5
            "policy_0_team1": 90.0,  # Overall avg: 93.33
            "policy_1_maze1": 88.0,
            "policy_1_maze2": 92.0,  # avg: 90.0
            "policy_1_arena1": 85.0,
            "policy_1_arena2": 88.0,  # avg: 86.5
            "policy_1_team1": 82.0,  # Overall avg: 86.17
        }
        record_episodes(
            run_free_data,
            "navigation",
            ["maze1", "maze2"],
            runfree_metrics,
            overriding_stats_client=isolated_stats_client,
        )
        record_episodes(
            run_free_data,
            "combat",
            ["arena1", "arena2"],
            runfree_metrics,
            overriding_stats_client=isolated_stats_client,
        )
        record_episodes(
            run_free_data, "cooperation", ["team1"], runfree_metrics, overriding_stats_client=isolated_stats_client
        )

        # Create leaderboard
        eval_names = [f"{cat}/{env}" for cat, envs in categories_and_envs for env in envs]
        start_date = datetime.datetime.now() - datetime.timedelta(days=1)

        leaderboard_response = test_client.post(
            "/leaderboards",
            json={
                "name": "Integration Test Leaderboard",
                "evals": eval_names,
                "metric": "reward",
                "start_date": start_date.isoformat(),
            },
            headers={"X-Auth-Request-Email": "test_user@example.com"},
        )
        assert leaderboard_response.status_code == 200, f"Failed to create leaderboard: {leaderboard_response.text}"
        leaderboard = leaderboard_response.json()
        leaderboard_id = leaderboard["id"]

        # Now we need to manually trigger the leaderboard update process
        # to populate the leaderboard_policy_scores table. We'll do this by
        # manually running the update logic using the stats_repo fixture.

        # Create a leaderboard updater instance
        updater = LeaderboardUpdater(stats_repo)

        # Get the leaderboard from the database
        leaderboard_uuid = uuid.UUID(leaderboard_id)
        leaderboard_row = await stats_repo.get_leaderboard(leaderboard_uuid)

        assert leaderboard_row is not None, "Leaderboard row not found"

        # Manually trigger the update for this specific leaderboard
        await updater._update_leaderboard(leaderboard_row)

        # Get the leaderboard scorecard
        leaderboard_scorecard_response = test_client.post(
            "/scorecard/leaderboard",
            json={
                "leaderboard_id": leaderboard_id,
                "selector": "best",
                "num_policies": 4,
            },
        )
        assert leaderboard_scorecard_response.status_code == 200, (
            f"Failed to get leaderboard scorecard: {leaderboard_scorecard_response.text}"
        )
        leaderboard_scorecard = leaderboard_scorecard_response.json()

        # The leaderboard scorecard endpoint is working correctly!
        # After running the leaderboard updater, it should now return populated data.

        # Now get the same data by individually selecting the policies
        # Based on our test data, the top 4 policies should be:
        # 1. Run-free policy 0 (avg: 93.33)
        # 2. Run-free policy 1 (avg: 86.17)
        # 3. Training run 1 policy 2 (avg: 81.67)
        # 4. Training run 2 policy 1 (avg: 86.67)

        # Get training run IDs
        training_run_ids = [
            str(test_data1["training_run"].id),
            str(test_data2["training_run"].id),
        ]

        # Get run-free policy IDs
        run_free_policy_ids = [str(p.id) for p in run_free_data["policies"]]

        # Get individual scorecard with best selector
        individual_scorecard_response = test_client.post(
            "/scorecard/scorecard",
            json={
                "training_run_ids": training_run_ids,
                "run_free_policy_ids": run_free_policy_ids,
                "eval_names": eval_names,
                "training_run_policy_selector": "best",
                "metric": "reward",
            },
        )
        assert individual_scorecard_response.status_code == 200, (
            f"Failed to get individual scorecard: {individual_scorecard_response.text}"
        )
        individual_scorecard = individual_scorecard_response.json()

        # Now compare the leaderboard scorecard with the individual scorecard
        # to validate that they return exactly the same data

        # Both should have the same evaluation names
        assert leaderboard_scorecard["evalNames"] == individual_scorecard["evalNames"]

        # Both scorecards should now return exactly the same policies and data
        # since the leaderboard scorecard now correctly applies the num_policies limit
        # and policy selection logic
        leaderboard_policies = set(leaderboard_scorecard["policyNames"])
        individual_policies = set(individual_scorecard["policyNames"])

        # Both should have exactly 4 policies (as configured with num_policies: 4)
        assert len(leaderboard_policies) == 4, (
            f"Leaderboard should have exactly 4 policies, got {len(leaderboard_policies)}"
        )
        assert len(individual_policies) == 4, (
            f"Individual scorecard should have exactly 4 policies, got {len(individual_policies)}"
        )

        # Both should have the same policies
        assert leaderboard_policies == individual_policies, (
            f"Leaderboard and individual scorecards should have identical policies. "
            f"Leaderboard: {leaderboard_policies}, Individual: {individual_policies}"
        )

        # Verify that all data is identical between both scorecards
        for policy_name in leaderboard_policies:
            # Check that all evaluation values match exactly
            for eval_name in eval_names:
                leaderboard_cell = leaderboard_scorecard["cells"][policy_name][eval_name]
                individual_cell = individual_scorecard["cells"][policy_name][eval_name]

                assert leaderboard_cell["value"] == individual_cell["value"], (
                    f"Value mismatch for {policy_name}/{eval_name}"
                )
                assert leaderboard_cell["replayUrl"] == individual_cell["replayUrl"], (
                    f"Replay URL mismatch for {policy_name}/{eval_name}"
                )

        # Verify that average scores and other metadata are also identical
        assert leaderboard_scorecard["policyAverageScores"] == individual_scorecard["policyAverageScores"]
        assert leaderboard_scorecard["evalAverageScores"] == individual_scorecard["evalAverageScores"]
        assert leaderboard_scorecard["evalMaxScores"] == individual_scorecard["evalMaxScores"]

    def test_search_policies_by_name(self, test_client: TestClient, create_test_data, record_episodes) -> None:
        """Test searching policies by name."""
        # Create test data with distinctive names
        test_data = create_test_data("navigation_search_test", num_policies=2, create_run_free_policies=1)

        # Record episodes so policies appear in unified_training_runs view
        record_episodes(
            test_data,
            eval_category="navigation",
            env_names=["search_env"],
            metric_values={"policy_0_search_env": 75.0, "policy_1_search_env": 85.0, "policy_2_search_env": 65.0},
        )

        # Test search by partial name match
        search_request = {"search": "navigation_search"}
        response = test_client.post("/scorecard/policies/search", json=search_request)
        assert response.status_code == 200
        result = response.json()

        # Should find policies containing the search term
        assert "policies" in result
        assert len(result["policies"]) >= 1
        found_policies = [p for p in result["policies"] if "navigation_search" in p["name"]]
        assert len(found_policies) >= 1

        # Test case-insensitive search
        search_request = {"search": "NAVIGATION"}
        response = test_client.post("/scorecard/policies/search", json=search_request)
        assert response.status_code == 200
        result = response.json()

        found_policies = [p for p in result["policies"] if "navigation" in p["name"].lower()]
        assert len(found_policies) >= 1

    def test_search_policies_by_type(self, test_client: TestClient, create_test_data) -> None:
        """Test filtering policies by type."""
        # Create test data with both types
        create_test_data("search_policies_type", num_policies=1, create_run_free_policies=2)

        # Test filter by training_run type
        search_request = {"policy_type": "training_run"}
        response = test_client.post("/scorecard/policies/search", json=search_request)
        assert response.status_code == 200
        result = response.json()

        assert "policies" in result
        # All returned policies should be training runs
        for policy in result["policies"]:
            assert policy["type"] == "training_run"

        # Test filter by policy type
        search_request = {"policy_type": "policy"}
        response = test_client.post("/scorecard/policies/search", json=search_request)
        assert response.status_code == 200
        result = response.json()

        assert "policies" in result
        # All returned policies should be run-free policies
        for policy in result["policies"]:
            assert policy["type"] == "policy"

    def test_search_policies_by_tags(self, test_client: TestClient, create_test_data, record_episodes) -> None:
        """Test filtering policies by tags."""
        # Create test data that will have the scorecard_test tag
        test_data = create_test_data("search_tags_test", num_policies=2, create_run_free_policies=1)

        # Record episodes so policies appear in unified_training_runs view
        record_episodes(
            test_data,
            eval_category="navigation",
            env_names=["tag_env"],
            metric_values={"policy_0_tag_env": 80.0, "policy_1_tag_env": 90.0, "policy_2_tag_env": 70.0},
        )

        # Test filter by tag that we know exists in test data (scorecard_test)
        search_request = {"tags": ["scorecard_test"]}
        response = test_client.post("/scorecard/policies/search", json=search_request)
        assert response.status_code == 200
        result = response.json()

        assert "policies" in result
        # Find training runs with scorecard_test tag
        tagged_policies = [p for p in result["policies"] if "scorecard_test" in p.get("tags", [])]
        assert len(tagged_policies) >= 1

        # Test filter by multiple tags (should match policies with ANY of the tags)
        search_request = {"tags": ["test_tag", "nonexistent"]}
        response = test_client.post("/scorecard/policies/search", json=search_request)
        assert response.status_code == 200
        result = response.json()

        assert "policies" in result
        # Should still find policies with test_tag (which exists in test data)
        tagged_policies = [p for p in result["policies"] if "test_tag" in p.get("tags", [])]
        assert len(tagged_policies) >= 1

    def test_search_policies_combined_filters(self, test_client: TestClient, create_test_data, record_episodes) -> None:
        """Test combining multiple search filters."""
        # Create test data
        test_data = create_test_data("combined_filter_test", num_policies=2, create_run_free_policies=1)

        # Record episodes so policies appear in unified_training_runs view
        record_episodes(
            test_data,
            eval_category="navigation",
            env_names=["combined_env"],
            metric_values={"policy_0_combined_env": 80.0, "policy_1_combined_env": 90.0, "policy_2_combined_env": 70.0},
        )

        # Test combining search term, type, and tags using existing test data
        search_request = {
            "search": "combined_filter",  # Part of the training run name
            "policy_type": "training_run",
            "tags": ["scorecard_test"],  # Tag that exists in test data
        }
        response = test_client.post("/scorecard/policies/search", json=search_request)
        assert response.status_code == 200
        result = response.json()

        assert "policies" in result
        # Should find training runs that match ALL criteria
        matching_policies = []
        for policy in result["policies"]:
            if (
                "combined_filter" in policy["name"].lower()
                and policy["type"] == "training_run"
                and "scorecard_test" in policy.get("tags", [])
            ):
                matching_policies.append(policy)

        assert len(matching_policies) >= 1

    def test_search_policies_pagination(self, test_client: TestClient, create_test_data) -> None:
        """Test pagination in search results."""
        # Create test data
        create_test_data("search_pagination", num_policies=5, create_run_free_policies=3)

        # Test with limit
        search_request = {"limit": 2}
        response = test_client.post("/scorecard/policies/search", json=search_request)
        assert response.status_code == 200
        result = response.json()

        assert "policies" in result
        assert len(result["policies"]) <= 2

        # Test with offset
        search_request = {"limit": 3, "offset": 2}
        response = test_client.post("/scorecard/policies/search", json=search_request)
        assert response.status_code == 200
        result = response.json()

        assert "policies" in result
        assert len(result["policies"]) <= 3

    def test_search_policies_empty_results(self, test_client: TestClient) -> None:
        """Test search with no matching results."""
        search_request = {"search": "nonexistent_policy_name_12345"}
        response = test_client.post("/scorecard/policies/search", json=search_request)
        assert response.status_code == 200
        result = response.json()

        assert "policies" in result
        assert len(result["policies"]) == 0

    def test_search_policies_invalid_parameters(self, test_client: TestClient) -> None:
        """Test search with invalid parameters."""
        # Test invalid policy type
        search_request = {"policy_type": "invalid_type"}
        response = test_client.post("/scorecard/policies/search", json=search_request)
        assert response.status_code == 200  # Should return empty results, not error
        result = response.json()
        assert "policies" in result
        assert len(result["policies"]) == 0

        # Test excessive limit (should be capped)
        search_request = {"limit": 2000}  # Over the 1000 limit
        response = test_client.post("/scorecard/policies/search", json=search_request)
        assert response.status_code == 422  # Validation error

        # Test negative offset
        search_request = {"offset": -1}
        response = test_client.post("/scorecard/policies/search", json=search_request)
        assert response.status_code == 422  # Validation error
