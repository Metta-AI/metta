name: 'Combine All Benchmark Results'
description: 'Combine Python and C++ benchmark results into unified BMF format'
inputs:
  python_files:
    description: 'Comma-separated Python benchmark files'
    required: true
  cpp_files:
    description: 'Glob pattern for C++ benchmark files'
    required: true
  output_file:
    description: 'Unified output file (BMF format)'
    required: true

runs:
  using: 'composite'
  steps:
    - name: Combine all benchmarks into BMF format
      shell: bash
      run: |
        python3 - <<'EOF'
        import json
        import glob
        import os

        def safe_load_json(file_path):
            try:
                if os.path.exists(file_path):
                    with open(file_path, 'r') as f:
                        content = f.read().strip()
                        if content:
                            return json.loads(content)
                return {}
            except (json.JSONDecodeError, FileNotFoundError):
                print(f"Warning: Could not load {file_path}")
                return {}

        def convert_pytest_to_bmf_format(pytest_data):
            """Convert pytest-benchmark format to Bencher Metric Format (BMF)"""
            bmf_results = {}
            
            for bench in pytest_data.get("benchmarks", []):
                # Extract basic info
                name = bench.get("fullname", bench.get("name", "unknown"))
                stats = bench.get("stats", {})
                extra_info = bench.get("extra_info", {})
                
                # Create BMF entry for this benchmark
                bench_metrics = {}
                
                # Add latency metrics from stats if available
                if stats.get("mean") is not None:
                    latency_metric = {"value": stats["mean"]}
                    if stats.get("min") is not None:
                        latency_metric["lower_value"] = stats["min"]
                    if stats.get("max") is not None:
                        latency_metric["upper_value"] = stats["max"]
                    bench_metrics["latency"] = latency_metric
                
                # Add all extra_info metrics (agent_steps_per_second, env_steps_per_second, etc.)
                for metric_name, metric_value in extra_info.items():
                    if isinstance(metric_value, (int, float)):
                        bench_metrics[metric_name] = {"value": metric_value}
                
                # Only add benchmark if we have metrics
                if bench_metrics:
                    bmf_results[name] = bench_metrics
            
            return bmf_results

        def convert_cpp_to_bmf_format(cpp_data):
            """Convert Google Benchmark format to Bencher Metric Format (BMF)"""
            bmf_results = {}
            
            for bench in cpp_data.get("benchmarks", []):
                name = bench.get("name", "unknown")
                bench_metrics = {}
                
                # Only include meaningful performance metrics from C++ benchmarks
                # Filter out metadata and configuration values
                meaningful_metrics = {
                    "real_time": "latency",  # Map real_time to standard latency measure
                    "cpu_time": "cpu_time",  # Keep cpu_time as performance metric
                }
                
                for cpp_key, bmf_key in meaningful_metrics.items():
                    if cpp_key in bench and isinstance(bench[cpp_key], (int, float)):
                        bench_metrics[bmf_key] = {"value": bench[cpp_key]}
                
                # Only add benchmark if we have meaningful metrics
                if bench_metrics:
                    bmf_results[name] = bench_metrics
            
            return bmf_results

        # Initialize unified results in BMF format
        unified_results = {}

        # Process Python benchmark files
        python_files = "${{ inputs.python_files }}".split(',')
        for file_path in python_files:
            file_path = file_path.strip()
            if not file_path:
                continue
            
            data = safe_load_json(file_path)
            if data and "benchmarks" in data and len(data["benchmarks"]) > 0:
                print(f"Processing Python file: {file_path} ({len(data['benchmarks'])} benchmarks)")
                bmf_results = convert_pytest_to_bmf_format(data)
                
                # Add source prefix to benchmark names
                for bench_name, bench_metrics in bmf_results.items():
                    prefixed_name = f"python/{bench_name}"
                    unified_results[prefixed_name] = bench_metrics
                
                print(f"Added {len(bmf_results)} Python benchmarks")
            else:
                print(f"Skipping empty or invalid Python file: {file_path}")

        # Process C++ benchmark files  
        cpp_pattern = "${{ inputs.cpp_files }}"
        cpp_files = glob.glob(cpp_pattern)

        for file_path in cpp_files:
            # Skip if it's actually a Python file that got picked up
            if any(py_file.strip().endswith(os.path.basename(file_path)) for py_file in python_files):
                continue
                
            data = safe_load_json(file_path)
            if data and "benchmarks" in data and len(data["benchmarks"]) > 0:
                print(f"Processing C++ file: {file_path} ({len(data['benchmarks'])} benchmarks)")
                bmf_results = convert_cpp_to_bmf_format(data)
                
                # Add source prefix to benchmark names
                for bench_name, bench_metrics in bmf_results.items():
                    prefixed_name = f"cpp/{bench_name}"
                    unified_results[prefixed_name] = bench_metrics
                    
                print(f"Added {len(bmf_results)} C++ benchmarks")
            else:
                print(f"Skipping empty or invalid C++ file: {file_path}")

        # Write unified results in BMF format
        if len(unified_results) == 0:
            print("‚ö†Ô∏è  Warning: No benchmarks found to combine!")
            # Create empty BMF object
            with open('${{ inputs.output_file }}', 'w') as f:
                json.dump({}, f)
        else:
            with open('${{ inputs.output_file }}', 'w') as f:
                json.dump(unified_results, f, indent=2)

        print(f"‚úÖ Combined {len(unified_results)} benchmarks in BMF format")

        # Debug: Print sample of what we generated
        if unified_results:
            sample_bench = next(iter(unified_results.keys()))
            print(f"\nüìã Sample BMF output for '{sample_bench}':")
            print(json.dumps({sample_bench: unified_results[sample_bench]}, indent=2))
        EOF