# @package __global__

defaults:
  # - /sim/all@sim_job.simulation_suite
  # - /env/mettagrid/game/agent/rewards/shaped@replay_job.sim.env_overrides.game.agent.rewards
  - _self_

trainer:
  # env: /env/mettagrid/multienv_mettagrid
  curriculum: /env/mettagrid/curriculum/resources

  env_overrides:
    sampling: 0.7
    game:
      max_steps: 13

  # simulation:
  #   evaluate_interval: 200
  # optimizer:
  #   type: muon

  # optimizer:
  #   type: muon
  # env_overrides:
  #   sampling: 0.7
  #   game:
  #     num_agents: 36
  #     max_steps: 1000

# policy: wandb://run/b.daveey.train.maze.sm.dr.warm.0
# baselines: wandb://run/b.daveey.train.maze.sm.11x11.0

# policy_uri: wandb://run/b.daveey.t.1.bl
# policy_uri: wandb://run/b.daveey.t.16.dr0

# policy_uri: wandb://run/b.daveey.dr9.muon.latest
# policy_uri: wandb://run/b.daveey.t.1.lra.dr.muon
# policy_uri: pytorch:///tmp/puffer_metta.pt
# policy_uri: pytorch:///tmp/puffer_metta.pt
policy_uri: wandb://run/daveey.arena.ces.32x4.3
# policy_uri: pytorch://${data_dir}/puffer/puffer_metta.pt

# policy_uri: ${trained_policy_uri}
npc_policy_uri: ${policy_uri}
# npc_policy_uri: ${trained_policy_uri}

eval_db_uri: ${run_dir}/eval_stats

analyzer:
  policy_uri: ${..policy_uri}
  view_type: latest
  analysis:
    metrics:
      - metric: episode_reward
      - metric: "heart.get"

replay_job:
  sim:
    env: /env/mettagrid/arena/combat_easy
    env_overrides:
      game:
        objects:
          lasery:
            initial_resource_count: 20

# sim_job:
# policy_agents_pct: 1

# env: /env/mettagrid/reward_dr
# env_overrides:
#   # sampling: 0.7
#   game:
#     num_agents: 16
#     max_steps: 1000
#     map_builder:
#       room:
#         agents: 4
#       num_rooms: 4

run_id: 1.8
run: ${oc.env:USER}.local.${run_id}
trained_policy_uri: ${run_dir}/checkpoints

sweep_params: "sweep/fast"
sweep_name: "${oc.env:USER}.local.sweep.${run_id}"
seed: null
