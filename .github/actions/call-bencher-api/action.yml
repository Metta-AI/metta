name: "Run Unified Bencher Analysis"
description: "Run Bencher analysis with 3 core measures: latency + 2 KPIs"
inputs:
  bencher_token:
    description: "Bencher API token"
    required: true
  github_token:
    description: "GitHub token"
    required: true
  benchmark_file:
    description: "Unified benchmark results file (BMF format)"
    required: true
  project_id:
    description: "Bencher project ID"
    default: "metta"

runs:
  using: "composite"
  steps:
    - name: Install Bencher CLI
      uses: bencherdev/bencher@main

    - name: Install jq
      shell: bash
      run: sudo apt-get install -y jq

    - name: Check main branch status
      id: check-main
      shell: bash
      env:
        BENCHER_API_TOKEN: ${{ inputs.bencher_token }}
      run: |
        # Run the command and capture output
        if bencher branch view ${{ inputs.project_id }} main --token "$BENCHER_API_TOKEN" > /tmp/branch_output.json 2>/dev/null; then
          echo "Branch info retrieved, checking for benchmark data..."
          
          # Debug: show what we got
          echo "Branch data:"
          cat /tmp/branch_output.json | jq '.'
          
          # Check if branch has valid benchmark data
          # We need to check if head exists AND is not null AND has actual benchmark data
          if jq -e '.head != null' /tmp/branch_output.json > /dev/null; then
            # Try to verify the head actually has accessible data by checking for thresholds
            if bencher threshold list ${{ inputs.project_id }} \
                --branch main \
                --token "$BENCHER_API_TOKEN" > /tmp/thresholds.json 2>/dev/null; then
              echo "main_exists=true" >> $GITHUB_OUTPUT
              echo "main_has_data=true" >> $GITHUB_OUTPUT
              echo "‚úÖ Main branch exists and has benchmark data with thresholds"
            else
              echo "main_exists=true" >> $GITHUB_OUTPUT
              echo "main_has_data=false" >> $GITHUB_OUTPUT
              echo "‚ö†Ô∏è Main branch exists but thresholds/data not accessible yet"
            fi
          else
            echo "main_exists=true" >> $GITHUB_OUTPUT
            echo "main_has_data=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è Main branch exists but has no benchmark data yet (head is null)"
          fi
        else
          echo "main_exists=false" >> $GITHUB_OUTPUT
          echo "main_has_data=false" >> $GITHUB_OUTPUT
          echo "‚ö†Ô∏è Main branch does not exist"
        fi

    # Main branch: Upload baseline with ultra-simple thresholds
    - name: Upload baseline with 3-measure thresholds (main branch)
      if: github.ref == 'refs/heads/main'
      shell: bash
      env:
        BENCHER_API_TOKEN: ${{ inputs.bencher_token }}
        GITHUB_TOKEN: ${{ inputs.github_token }}
      run: |
        if [ -f "${{ inputs.benchmark_file }}" ] && [ -s "${{ inputs.benchmark_file }}" ]; then

          # Built-in latency measure (lower is better - alert on increases)
          echo "Setting upper threshold for latency (built-in timing measure)"
          bencher run \
            --project ${{ inputs.project_id }} \
            --token "$BENCHER_API_TOKEN" \
            --branch main \
            --threshold-measure latency \
            --threshold-test percentage \
            --threshold-max-sample-size 2 \
            --threshold-upper-boundary 0.20 \
            --testbed ubuntu-latest \
            --adapter json \
            --github-actions "$GITHUB_TOKEN" \
            --file "${{ inputs.benchmark_file }}" || echo "‚ö†Ô∏è Could not set latency threshold"
          
          # Core throughput KPIs (higher is better - alert on ANY 20% change)
          declare -A THROUGHPUT_METRICS=(
            ["agent_steps_per_second"]="Main agent performance KPI"
            ["env_steps_per_second"]="Main environment performance KPI"
          )
          
          for metric in "${!THROUGHPUT_METRICS[@]}"; do
            echo "Setting bidirectional threshold for ${THROUGHPUT_METRICS[$metric]}: $metric"
            bencher run \
              --project ${{ inputs.project_id }} \
              --token "$BENCHER_API_TOKEN" \
              --branch main \
              --threshold-measure "$metric" \
              --threshold-test percentage \
              --threshold-max-sample-size 2 \
              --threshold-lower-boundary 0.20 \
              --threshold-upper-boundary 0.20 \
              --testbed ubuntu-latest \
              --adapter json \
              --github-actions "$GITHUB_TOKEN" \
              --file "${{ inputs.benchmark_file }}" || echo "‚ö†Ô∏è  Could not set threshold for $metric"
          done
          
          echo "‚úÖ Thresholds configured"
        else
          echo "‚ö†Ô∏è No benchmark file found: ${{ inputs.benchmark_file }}"
        fi

    # PR: Check for significant changes in the 3 core measures
    - name: Check for significant changes in core measures
      if: |
        github.event_name == 'pull_request' &&
        !github.event.pull_request.head.repo.fork &&
        steps.check-main.outputs.main_exists == 'true' &&
        steps.check-main.outputs.main_has_data == 'true'
      shell: bash
      env:
        BENCHER_API_TOKEN: ${{ inputs.bencher_token }}
        GITHUB_TOKEN: ${{ inputs.github_token }}
      run: |
        echo "üö® Checking for significant changes..."
        echo "- latency: ¬±20% timing changes"
        echo "- agent_steps_per_second: ¬±20% throughput changes" 
        echo "- env_steps_per_second: ¬±20% throughput changes"

        if [ -f "${{ inputs.benchmark_file }}" ]; then

          bencher run \
            --project ${{ inputs.project_id }} \
            --token "$BENCHER_API_TOKEN" \
            --branch "$GITHUB_HEAD_REF" \
            --start-point main \
            --start-point-reset \
            --start-point-clone-thresholds \
            --err \
            --testbed ubuntu-latest \
            --adapter json \
            --github-actions "$GITHUB_TOKEN" \
            --file "${{ inputs.benchmark_file }}"
            
          echo "‚úÖ No significant changes detected in core measures"
        else
          echo "‚ö†Ô∏è  No benchmark file found: ${{ inputs.benchmark_file }}"
          exit 1
        fi

    # PR: Skip threshold check if main has no data yet
    - name: Skip threshold check (main has no data)
      if: |
        github.event_name == 'pull_request' &&
        !github.event.pull_request.head.repo.fork &&
        steps.check-main.outputs.main_exists == 'true' &&
        steps.check-main.outputs.main_has_data == 'false'
      shell: bash
      run: |
        echo "‚ö†Ô∏è Skipping performance comparison - main branch has no benchmark data yet"
        echo "This is normal for the first PR after enabling benchmarks."
        echo "Once this PR merges, future PRs will have baseline data for comparison."