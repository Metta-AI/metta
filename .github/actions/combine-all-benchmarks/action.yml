name: "Combine All Benchmark Results"
description: "Combine Python and C++ benchmark results into unified bencher BMF format"
inputs:
  python_files:
    description: "Comma-separated Python benchmark files"
    required: true
  cpp_files:
    description: "Glob pattern for C++ benchmark files"
    required: true
  output_file:
    description: "Unified output file (BMF format)"
    required: true

runs:
  using: "composite"
  steps:
    - name: Combine all benchmarks into BMF format
      shell: bash
      run: |
        python3 - <<'EOF'
        import json
        import glob
        import os

        def safe_load_json(file_path):
            try:
                if os.path.exists(file_path):
                    with open(file_path, 'r') as f:
                        content = f.read().strip()
                        if content:
                            return json.loads(content)
                return {}
            except (json.JSONDecodeError, FileNotFoundError):
                print(f"Warning: Could not load {file_path}")
                return {}

        def convert_pytest_to_bmf_format(pytest_data):
            """Convert pytest-benchmark to BMF using built-in measures + 2 KPIs"""
            bmf_results = {}
            
            for bench in pytest_data.get("benchmarks", []):
                name = bench.get("fullname", bench.get("name", "unknown"))
                stats = bench.get("stats", {})
                extra_info = bench.get("extra_info", {})
                
                bench_metrics = {}
                
                # Use built-in latency measure for timing
                if stats.get("mean") is not None:
                    bench_metrics["latency"] = {
                        "value": stats["mean"],
                        "lower_value": stats.get("min"),
                        "upper_value": stats.get("max")
                    }
                
                # Only add your 2 most important custom KPIs
                key_kpis = ["agent_steps_per_second", "env_steps_per_second"]
                
                for kpi_name in key_kpis:
                    if kpi_name in extra_info and isinstance(extra_info[kpi_name], (int, float)):
                        bench_metrics[kpi_name] = {"value": extra_info[kpi_name]}
                
                if bench_metrics:
                    bmf_results[name] = bench_metrics
            
            return bmf_results

        def convert_cpp_to_bmf_format(cpp_data):
            """Convert Google Benchmark to BMF using built-in measures only"""
            bmf_results = {}
            
            for bench in cpp_data.get("benchmarks", []):
                name = bench.get("name", "unknown")
                bench_metrics = {}
                
                # Use built-in latency measure (from real_time)
                if "real_time" in bench:
                    bench_metrics["latency"] = {"value": bench["real_time"]}
                
                if bench_metrics:
                    bmf_results[name] = bench_metrics
            
            return bmf_results

        # Initialize unified results in BMF format
        unified_results = {}

        # Process Python benchmark files
        python_files = "${{ inputs.python_files }}".split(',')
        for file_path in python_files:
            file_path = file_path.strip()
            if not file_path:
                continue
            
            data = safe_load_json(file_path)
            if data and "benchmarks" in data and len(data["benchmarks"]) > 0:
                print(f"Processing Python file: {file_path} ({len(data['benchmarks'])} benchmarks)")
                bmf_results = convert_pytest_to_bmf_format(data)
                
                # Add source prefix to benchmark names
                for bench_name, bench_metrics in bmf_results.items():
                    prefixed_name = f"python/{bench_name}"
                    unified_results[prefixed_name] = bench_metrics
                
                print(f"Added {len(bmf_results)} Python benchmarks")
            else:
                print(f"Skipping empty or invalid Python file: {file_path}")

        # Process C++ benchmark files  
        cpp_pattern = "${{ inputs.cpp_files }}"
        cpp_files = glob.glob(cpp_pattern)

        for file_path in cpp_files:
            # Skip if it's actually a Python file that got picked up
            if any(py_file.strip().endswith(os.path.basename(file_path)) for py_file in python_files):
                continue
                
            data = safe_load_json(file_path)
            if data and "benchmarks" in data and len(data["benchmarks"]) > 0:
                print(f"Processing C++ file: {file_path} ({len(data['benchmarks'])} benchmarks)")
                bmf_results = convert_cpp_to_bmf_format(data)
                
                # Add source prefix to benchmark names
                for bench_name, bench_metrics in bmf_results.items():
                    prefixed_name = f"cpp/{bench_name}"
                    unified_results[prefixed_name] = bench_metrics
                    
                print(f"Added {len(bmf_results)} C++ benchmarks")
            else:
                print(f"Skipping empty or invalid C++ file: {file_path}")

        # Write unified results in BMF format
        if len(unified_results) == 0:
            print("‚ö†Ô∏è Warning: No benchmarks found to combine!")
            # Create empty BMF object
            with open('${{ inputs.output_file }}', 'w') as f:
                json.dump({}, f)
        else:
            with open('${{ inputs.output_file }}', 'w') as f:
                json.dump(unified_results, f, indent=2)

        print(f"‚úÖ Combined {len(unified_results)} benchmarks in BMF format")
        print("üìä Using 3 measures: latency, agent_steps_per_second, env_steps_per_second")

        # Debug: Print sample of what we generated
        if unified_results:
            sample_bench = next(iter(unified_results.keys()))
            print(f"\nüìã Sample BMF output for '{sample_bench}':")
            print(json.dumps({sample_bench: unified_results[sample_bench]}, indent=2))
        EOF
