\section{Introduction}

Neural network architectures for sequential data typically fall into two broad families: models that reprocess a fixed context window, and models that carry state forward across steps. The first class---often built around attention or convolution---trades compute for flexibility by recomputing representations from a sliding window of recent tokens or observations. The second class---recurrent or state-space models---updates a compact state online, enabling constant-time inference per step. Both families are useful in reinforcement learning, where agents act continuously and experience streams of observations, rewards, and actions.

In RL, the environment advances at every timestep and the agent must update its policy and internal state repeatedly. This creates strong temporal locality: adjacent steps share most of their context, and the effective computation resembles a sliding window with heavy reuse. Efficient inference therefore benefits from architectures that can preserve and update state rather than recomputing history. However, the credit assignment signal often spans much longer horizons than the window used for training (e.g., TBPTT), which puts pressure on the memory mechanism to retain and expose long-range information.

Sequential decision-making under partial observability thus requires explicit memory. Agents must infer latent state from observations spanning thousands of steps, and the memory mechanism must balance long-horizon credit assignment with efficient online updates. Transformers handle long dependencies but suffer from expensive inference. \cite{dai2019transformerxl,parisotto2020gtrxl} Recurrent alternatives offer efficient inference but lack unified implementations. \cite{beck2024xlstm}

We propose \textbf{Cortex}, a modular system for recurrent backbones. Cortex makes memory mechanisms composable. It scales architectures vertically (depth) and horizontally (mixtures of experts) without rewriting system glue. \cite{shazeer2017moe,lepikhin2021gshard,fedus2022switch}

We summarize our contributions as follows: (i) a composable abstraction hierarchy spanning \textbf{Cells}, \textbf{Blocks}, \textbf{Columns}, and \textbf{Stacks} with a uniform interface and explicit state; (ii) \textbf{AxonLayer} as a stateful replacement for \code{nn.Linear} alongside a two-axis taxonomy of memory cells; and (iii) scalable architectures that grow in width via routed Columns and in depth via Stacks, together with an evaluation suite spanning diagnostics and RL benchmarks.
