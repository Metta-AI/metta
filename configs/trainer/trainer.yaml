_target_: metta.rl.trainer.MettaTrainer

num_workers: ???

curriculum: /env/mettagrid/curriculum/simple
env_overrides: {}

initial_policy:
  uri: null
  type: top
  range: 1
  metric: epoch
  filters: {}

checkpoint_dir: ${run_dir}/checkpoints

evaluate_interval: 300
checkpoint_interval: 60
wandb_checkpoint_interval: 300
grad_mean_variance_interval: 0 # 0 to disable
replay_interval: ${trainer.evaluate_interval}
replay_dir: s3://softmax-public/replays/${run}

average_reward: false # Set to true to use average reward optimization
average_reward_alpha: 0.01 # Smoothing factor for 100k step average (ln(2)/100000)

resume: true
use_e3b: false

total_timesteps: 50_000_000_000

clip_coef: 0.1
ent_coef: 0.0021
gae_lambda: 0.916
gamma: 0.977

# optimizer:
#   type: muon
#   beta1: 0.943
#   beta2: 0.9997
#   eps: 1.82e-13
#   learning_rate: 0.000238167
#   weight_decay: 0

optimizer:
  type: adam
  beta1: 0.9
  beta2: 0.999
  eps: 1e-12
  learning_rate: 0.0004573146765703167
  weight_decay: 0

lr_scheduler:
  enabled: false
  anneal_lr: false

max_grad_norm: 0.5
vf_clip_coef: 0.1
vf_coef: 0.44
l2_reg_loss_coef: 0
l2_init_loss_coef: 0

# Prioritized experience replay parameters
prioritized_experience_replay:
  prio_alpha: 0.0  # Default to uniform sampling (0.0 = uniform, >0 = prioritized)
  prio_beta0: 0.6  # Initial importance sampling correction

norm_adv: true
clip_vloss: true
target_kl: null

# V-trace clipping parameters (for off-policy corrections)
vtrace:
  vtrace_rho_clip: 1.0  # Default: on-policy (no off-policy correction)
  vtrace_c_clip: 1.0    # Default: on-policy bootstrapping

zero_copy: true
require_contiguous_env_ids: false
verbose: true

batch_size: 262144
minibatch_size: 16384
bptt_horizon: 64
update_epochs: 1

cpu_offload: false
compile: false
compile_mode: reduce-overhead
profiler_interval_epochs: 10000

forward_pass_minibatch_target_size: 2048
async_factor: 2

kickstart:
  teacher_uri: null
  action_loss_coef: 1
  value_loss_coef: 1
  anneal_ratio: 0.65 # ratio of kickstart_steps to anneal to 0 coef
  kickstart_steps: 1_000_000_000
  additional_teachers:
    # - teacher_uri: wandb://run/m_alexv_ks_dr_lam_001:v22
    #   action_loss_coef: 0.5
    #   value_loss_coef: 0.6
    #  - teacher_uri: wandb://run/mettabox_cogeval_defaults_lowent_initialized0005:v95
    #    action_loss_coef: 1
    #    value_loss_coef: 1
