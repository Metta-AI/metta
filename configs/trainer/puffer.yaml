_target_: rl.pufferlib.trainer.PufferTrainer

defaults:
  - trainer

resume: true
use_e3b: false

total_timesteps: 50_000_000_000

clip_coef: 0.1
ent_coef: 0.002
gae_lambda: 0.9
gamma: 0.97
learning_rate: 0.0006
max_grad_norm: 0.5
vf_clip_coef: 0.1
vf_coef: 0.5
l2_reg_loss_coef: 0
l2_init_loss_coef: 0

norm_adv: true
clip_vloss: true
target_kl: null
anneal_lr: false

zero_copy: true
verbose: true

checkpoint_interval: 60
wandb_checkpoint_interval: 300
evaluate_interval: 10

batch_size: 262144
minibatch_size: 4096
bptt_horizon: 16
update_epochs: 1

cpu_offload: false
compile: false
compile_mode: reduce-overhead

forward_pass_minibatch_target_size: 4096
async_factor: 2

stats:
  overview:
    episode/reward.mean: episode_reward
  step: train/agent_step

# Distributed training configuration
dist:
  master_addr: localhost
  master_port: 29500

  # Basic distributed settings
  num_gpus: 1
  sync_gpus: false

  # NCCL configuration
  nccl:
    # Basic settings
    timeout: 60  # Timeout in seconds (10 minutes)
    blocking_wait: 0  # Enable blocking wait
    async_error_handling: 1  # Enable async error handling

    # Debug settings
    debug: INFO  # Debug level (INFO, WARN, or ERROR)
    debug_subsys: ALL  # Debug subsystems to log (ALL, INIT, COLL, P2P, SHM, NET, etc.)
    trace_buffer_size: 1048576  # Size of trace buffer for debugging (1MB)
    debug_file: nccl_debug.log  # Debug log file name (relative to run_dir)

