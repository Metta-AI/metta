"""Curiosity Model Policy Optimization loss implementation.

This module replaces the previous clip-mirror surrogate with a faithful
implementation of Curiosity Model Policy Optimization (CMPO). CMPO augments
policy-gradient updates with a learned world-model, a curiosity module that
distinguishes positive and negative novelty, and adaptive scheduling of model
and environment data.
"""

from __future__ import annotations

import math
import random
from collections import deque
from typing import Any, Callable

import torch
import torch.nn as nn
import torch.nn.functional as F
from pydantic import Field
from tensordict import NonTensorData, TensorDict
from torch import Tensor
from torchrl.data import Composite, UnboundedContinuous

try:  # Prefer gymnasium but fall back to gym if needed.
    from gymnasium import spaces as gym_spaces
except ImportError:  # pragma: no cover - fallback for older envs.
    from gym import spaces as gym_spaces  # type: ignore[no-redef]

from metta.agent.policy import Policy
from metta.rl.advantage import compute_advantage, normalize_advantage_distributed
from metta.rl.loss.ppo import PPO, PPOConfig
from metta.rl.training import ComponentContext, Experience, TrainingEnvironment
from mettagrid.base_config import Config


def _build_mlp(
    input_dim: int,
    hidden_dims: list[int],
    output_dim: int,
    activation: Callable[[Tensor], Tensor] = F.relu,
) -> nn.Sequential:
    """Construct a simple feed-forward network."""
    layers: list[nn.Module] = []
    last_dim = input_dim
    for hidden in hidden_dims:
        layers.append(nn.Linear(last_dim, hidden))
        layers.append(nn.LayerNorm(hidden))
        layers.append(nn.ReLU())
        last_dim = hidden
    layers.append(nn.Linear(last_dim, output_dim))
    net = nn.Sequential(*layers)

    # Store activation for forward usage when needed.
    net.activation = activation  # type: ignore[attr-defined]
    return net


def _stack_trajectories(trajs: list[TensorDict]) -> TensorDict:
    """Stack a list of trajectory tensordicts into a single batch."""
    if not trajs:
        raise ValueError("Cannot stack empty trajectory list")
    keys = list(trajs[0].keys())
    stacked = {key: torch.stack([traj[key] for traj in trajs], dim=0) for key in keys}
    batch_size = (len(trajs),) + trajs[0].batch_size
    return TensorDict(stacked, batch_size=batch_size)


class WorldModelConfig(Config):
    """Hyper-parameters for the dynamics ensemble."""

    ensemble_size: int = Field(default=5, ge=1)
    hidden_dims: list[int] = Field(default_factory=lambda: [512, 512])
    learning_rate: float = Field(default=3e-4, gt=0)
    batch_size: int = Field(default=256, gt=0)
    train_steps: int = Field(default=50, ge=1)
    buffer_size: int = Field(default=50000, gt=1)
    warmup_transitions: int = Field(default=2048, ge=0)
    gradient_clip: float = Field(default=1.0, gt=0)


class CuriosityConfig(Config):
    """Configuration for the curiosity module."""

    hidden_dims: list[int] = Field(default_factory=lambda: [256, 256])
    feature_dim: int = Field(default=128, ge=8)
    learning_rate: float = Field(default=1e-4, gt=0)
    weight_initial: float = Field(default=0.1, ge=0)
    weight_min: float = Field(default=0.01, ge=0)
    weight_max: float = Field(default=1.0, ge=0)
    adjustment_rate: float = Field(default=0.05, ge=0)
    positive_coef: float = Field(default=1.0, ge=0)
    negative_coef: float = Field(default=1.0, ge=0)
    reward_clip: float = Field(default=1.5, ge=0)
    train_steps: int = Field(default=50, ge=1)


class AdaptiveBufferSchedulerConfig(Config):
    """Adjusts the synthetic-vs-real sampling ratio."""

    min_ratio: float = Field(default=0.0, ge=0, le=1)
    max_ratio: float = Field(default=0.5, ge=0, le=1)
    target_model_error: float = Field(default=0.02, ge=0)
    adjustment_rate: float = Field(default=0.05, ge=0, le=1)


class ModelRolloutConfig(Config):
    """Controls synthetic rollouts generated by the world model."""

    rollout_length: int = Field(default=5, ge=1)
    batch_size: int = Field(default=128, ge=1)
    total_transitions: int = Field(default=4096, ge=1)
    termination_variance_threshold: float = Field(default=0.25, ge=0)


class SyntheticBufferConfig(Config):
    """Buffer capacity for storing synthetic trajectories."""

    capacity: int = Field(default=2048, ge=1)
    horizon: int = Field(default=5, ge=1)


class CMPOConfig(PPOConfig):
    """Full CMPO configuration."""

    world_model: WorldModelConfig = Field(default_factory=WorldModelConfig)
    curiosity: CuriosityConfig = Field(default_factory=CuriosityConfig)
    scheduler: AdaptiveBufferSchedulerConfig = Field(default_factory=AdaptiveBufferSchedulerConfig)
    model_rollouts: ModelRolloutConfig = Field(default_factory=ModelRolloutConfig)
    synthetic_buffer: SyntheticBufferConfig = Field(default_factory=SyntheticBufferConfig)

    def create(
        self,
        policy: Policy,
        trainer_cfg: Any,
        env: TrainingEnvironment,
        device: torch.device,
        instance_name: str,
        loss_config: Config,
    ) -> "CMPO":
        return CMPO(
            policy=policy,
            trainer_cfg=trainer_cfg,
            env=env,
            device=device,
            instance_name=instance_name,
            loss_config=loss_config,
        )


class FeedForwardDynamics(nn.Module):
    """Single ensemble member predicting state deltas and rewards."""

    def __init__(self, state_dim: int, action_dim: int, hidden_dims: list[int]) -> None:
        super().__init__()
        self.net = _build_mlp(state_dim + action_dim, hidden_dims, 2 * state_dim + 1)
        self.state_dim = state_dim

    def forward(self, state: Tensor, action: Tensor) -> tuple[Tensor, Tensor]:
        x = torch.cat([state, action], dim=-1)
        output = self.net(x)
        delta = output[..., : self.state_dim]
        reward = output[..., -1]
        next_state = state + delta
        return next_state, reward


class WorldModelEnsemble(nn.Module):
    """Ensemble of dynamics models used for MBPO rollouts."""

    def __init__(self, cfg: WorldModelConfig, state_dim: int, action_dim: int) -> None:
        super().__init__()
        self.members = nn.ModuleList(
            FeedForwardDynamics(state_dim=state_dim, action_dim=action_dim, hidden_dims=cfg.hidden_dims)
            for _ in range(cfg.ensemble_size)
        )

    def forward(self, state: Tensor, action: Tensor) -> tuple[Tensor, Tensor]:
        preds_state: list[Tensor] = []
        preds_reward: list[Tensor] = []
        for member in self.members:
            next_state, reward = member(state, action)
            preds_state.append(next_state)
            preds_reward.append(reward)
        next_state_stack = torch.stack(preds_state, dim=0)
        reward_stack = torch.stack(preds_reward, dim=0)
        next_state_mean = next_state_stack.mean(dim=0)
        reward_mean = reward_stack.mean(dim=0)
        return next_state_mean, reward_mean

    def variance(self, state: Tensor, action: Tensor) -> tuple[Tensor, Tensor]:
        preds_state: list[Tensor] = []
        preds_reward: list[Tensor] = []
        for member in self.members:
            next_state, reward = member(state, action)
            preds_state.append(next_state)
            preds_reward.append(reward)
        next_state_stack = torch.stack(preds_state, dim=0)
        reward_stack = torch.stack(preds_reward, dim=0)
        next_state_var = next_state_stack.var(dim=0, unbiased=False)
        reward_var = reward_stack.var(dim=0, unbiased=False)
        return next_state_var, reward_var


class TransitionBuffer:
    """Stores real environment transitions for model and curiosity updates."""

    def __init__(self, capacity: int) -> None:
        self.capacity = capacity
        self._buffer: deque[dict[str, Tensor]] = deque(maxlen=capacity)

    def add_batch(
        self,
        states: Tensor,
        actions_enc: Tensor,
        actions_raw: Tensor,
        rewards: Tensor,
        next_states: Tensor,
        dones: Tensor,
    ) -> None:
        batch_size = states.shape[0]
        for idx in range(batch_size):
            self._buffer.append(
                {
                    "state": states[idx].detach().cpu(),
                    "action_enc": actions_enc[idx].detach().cpu(),
                    "action_raw": actions_raw[idx].detach().cpu(),
                    "reward": rewards[idx].detach().cpu(),
                    "next_state": next_states[idx].detach().cpu(),
                    "done": dones[idx].detach().cpu(),
                }
            )

    def __len__(self) -> int:
        return len(self._buffer)

    def sample(self, batch_size: int, device: torch.device) -> TensorDict | None:
        if not self._buffer:
            return None
        actual = min(batch_size, len(self._buffer))
        indices = random.sample(range(len(self._buffer)), actual)
        batch = [self._buffer[i] for i in indices]
        stacked = {k: torch.stack([item[k] for item in batch], dim=0).to(device=device) for k in batch[0].keys()}
        return TensorDict(stacked, batch_size=(actual,))


class CuriosityModule(nn.Module):
    """Forward and inverse curiosity models with shared encoder."""

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        cfg: CuriosityConfig,
        discrete_actions: bool,
    ) -> None:
        super().__init__()
        self.encoder = _build_mlp(state_dim, cfg.hidden_dims, cfg.feature_dim)
        forward_input = cfg.feature_dim + action_dim
        self.forward_model = _build_mlp(forward_input, cfg.hidden_dims, cfg.feature_dim)
        inverse_input = cfg.feature_dim * 2
        self.discrete_actions = discrete_actions
        if discrete_actions:
            self.inverse_head = _build_mlp(inverse_input, cfg.hidden_dims, action_dim)
        else:
            self.inverse_head = _build_mlp(inverse_input, cfg.hidden_dims, action_dim)

    def encode(self, state: Tensor) -> Tensor:
        return self.encoder(state)

    def predict_feature(self, feature: Tensor, action: Tensor) -> Tensor:
        concat = torch.cat([feature, action], dim=-1)
        return self.forward_model(concat)

    def predict_action(self, feature: Tensor, next_feature: Tensor) -> Tensor:
        concat = torch.cat([feature, next_feature], dim=-1)
        return self.inverse_head(concat)


class AdaptiveBufferScheduler:
    """Tracks appropriate synthetic sampling ratio."""

    def __init__(self, cfg: AdaptiveBufferSchedulerConfig) -> None:
        self.cfg = cfg
        self.current_ratio: float = cfg.min_ratio

    def update(self, model_error: float) -> None:
        if model_error < self.cfg.target_model_error:
            self.current_ratio = min(self.cfg.max_ratio, self.current_ratio + self.cfg.adjustment_rate)
        else:
            self.current_ratio = max(self.cfg.min_ratio, self.current_ratio - self.cfg.adjustment_rate)


class ModelRolloutBuffer:
    """Stores synthetic rollouts to sample during policy updates."""

    def __init__(self, capacity: int) -> None:
        self.capacity = capacity
        self.storage: deque[TensorDict] = deque(maxlen=capacity)

    def add(self, traj: TensorDict) -> None:
        self.storage.append(traj.cpu())

    def __len__(self) -> int:
        return len(self.storage)

    def sample(self, batch_size: int, device: torch.device) -> TensorDict | None:
        if not self.storage:
            return None
        actual = min(batch_size, len(self.storage))
        idx = random.sample(range(len(self.storage)), actual)
        trajs = [self.storage[i] for i in idx]
        stacked = _stack_trajectories(trajs).to(device=device)
        return stacked


class CMPO(PPO):
    """Curiosity Model Policy Optimization loss."""

    def __init__(
        self,
        policy: Policy,
        trainer_cfg: Any,
        env: TrainingEnvironment,
        device: torch.device,
        instance_name: str,
        loss_config: CMPOConfig,
    ) -> None:
        super().__init__(policy, trainer_cfg, env, device, instance_name, loss_config)
        self.loss_cfg: CMPOConfig

        obs_space = env.single_observation_space
        if not hasattr(obs_space, "shape"):
            raise ValueError("Environment observation space must define shape for CMPO.")
        self.obs_shape = tuple(int(dim) for dim in obs_space.shape)
        self.obs_dim = int(torch.tensor(self.obs_shape).prod().item())

        action_space = env.single_action_space
        if isinstance(action_space, gym_spaces.Discrete):
            self.action_dim = int(action_space.n)
            self._action_dtype = torch.int64
            self._encode_action_fn = self._encode_discrete_action
            self._decode_action_shape = ()
            discrete_actions = True
            self._raw_action_dtype = torch.int64
            self._raw_action_shape = (1,)
        elif isinstance(action_space, gym_spaces.Box):
            self.action_dim = int(math.prod(action_space.shape))
            self._action_dtype = torch.float32
            self._encode_action_fn = self._encode_box_action
            self._decode_action_shape = tuple(int(dim) for dim in action_space.shape)
            discrete_actions = False
            self._raw_action_dtype = torch.float32
            self._raw_action_shape = self._decode_action_shape
        else:  # pragma: no cover - other spaces currently unsupported.
            raise NotImplementedError(f"Unsupported action space for CMPO: {action_space}")

        self.world_model = WorldModelEnsemble(loss_config.world_model, self.obs_dim, self.action_dim).to(device)
        self.world_model_opt = torch.optim.Adam(self.world_model.parameters(), lr=loss_config.world_model.learning_rate)

        self.curiosity_module = CuriosityModule(
            state_dim=self.obs_dim,
            action_dim=self.action_dim,
            cfg=loss_config.curiosity,
            discrete_actions=discrete_actions,
        ).to(device)
        self.curiosity_opt = torch.optim.Adam(
            self.curiosity_module.parameters(), lr=loss_config.curiosity.learning_rate
        )

        self.transition_buffer = TransitionBuffer(loss_config.world_model.buffer_size)
        self.model_buffer = ModelRolloutBuffer(loss_config.synthetic_buffer.capacity)
        self.scheduler = AdaptiveBufferScheduler(loss_config.scheduler)

        self.curiosity_weight = loss_config.curiosity.weight_initial
        self._last_model_error = 1.0

        self._prev_obs: Tensor | None = None
        self._prev_action_enc: Tensor | None = None
        self._prev_action_raw: Tensor | None = None
        self._prev_done: Tensor | None = None
        self._has_prev: Tensor | None = None

        self.register_state_attr(
            "curiosity_weight",
            "_last_model_error",
        )

    # ------------------------------------------------------------------
    # Loss interface overrides
    # ------------------------------------------------------------------
    def attach_replay_buffer(self, experience: Experience) -> None:  # type: ignore[override]
        super().attach_replay_buffer(experience)
        segments = experience.segments
        device = self.device
        self._prev_obs = torch.zeros((segments, self.obs_dim), dtype=torch.float32, device=device)
        self._prev_action_enc = torch.zeros((segments, self.action_dim), dtype=torch.float32, device=device)
        self._prev_action_raw = torch.zeros(
            (segments,) + (self._raw_action_shape or (1,)),
            dtype=self._raw_action_dtype,
            device=device,
        )
        self._prev_done = torch.ones((segments,), dtype=torch.bool, device=device)
        self._has_prev = torch.zeros((segments,), dtype=torch.bool, device=device)

    def get_experience_spec(self) -> Composite:
        base = super().get_experience_spec()
        scalar_f32 = UnboundedContinuous(shape=torch.Size([]), dtype=torch.float32)
        base["intrinsic_rewards"] = scalar_f32
        base["extrinsic_rewards"] = scalar_f32
        return base

    # ------------------------------------------------------------------
    # Helper utilities
    # ------------------------------------------------------------------
    def _flatten_obs(self, obs: Tensor) -> Tensor:
        obs_f = obs.to(dtype=torch.float32)
        if obs_f.max() > 1.0:
            obs_f = obs_f / 255.0
        return obs_f.view(obs.shape[0], -1)

    def _unflatten_obs(self, obs_flat: Tensor) -> Tensor:
        obs = obs_flat.view(obs_flat.shape[0], *self.obs_shape)
        obs = (obs * 255.0).clamp(0, 255)
        return obs.to(dtype=torch.uint8)

    def _encode_discrete_action(self, action: Tensor) -> Tensor:
        action = action.view(-1).long()
        return F.one_hot(action, num_classes=self.action_dim).to(dtype=torch.float32)

    def _encode_box_action(self, action: Tensor) -> Tensor:
        return action.view(action.shape[0], -1).to(dtype=torch.float32)

    def _encode_action(self, action: Tensor) -> Tensor:
        return self._encode_action_fn(action)

    def _world_model_predict(self, state: Tensor, action: Tensor) -> tuple[Tensor, Tensor, Tensor, Tensor]:
        next_state_mean, reward_mean = self.world_model(state, action)
        next_state_var, reward_var = self.world_model.variance(state, action)
        return next_state_mean, reward_mean, next_state_var.mean(dim=-1), reward_var

    def _compute_intrinsic_reward(
        self,
        state: Tensor,
        action: Tensor,
        next_state: Tensor,
        reward: Tensor,
        pred_next: Tensor,
        pred_reward: Tensor,
    ) -> Tensor:
        cfg = self.loss_cfg.curiosity
        with torch.no_grad():
            feature = self.curiosity_module.encode(state)
            next_feature = self.curiosity_module.encode(next_state)
            pred_feature = self.curiosity_module.predict_feature(feature, action)
            feature_error = F.mse_loss(pred_feature, next_feature, reduction="none").mean(dim=-1)

            reward_diff = reward - pred_reward
            positive_mask = (reward_diff >= 0).float()
            negative_mask = (reward_diff < 0).float()

            positive_curiosity = feature_error * positive_mask
            negative_curiosity = feature_error * negative_mask

            intrinsic = self.curiosity_weight * (
                cfg.positive_coef * positive_curiosity - cfg.negative_coef * negative_curiosity
            )
            intrinsic = intrinsic.clamp(-cfg.reward_clip, cfg.reward_clip)
        return intrinsic

    def _update_curiosity_weight(self, model_error: float) -> None:
        cfg = self.loss_cfg.curiosity
        if model_error < self.loss_cfg.scheduler.target_model_error:
            self.curiosity_weight = min(cfg.weight_max, self.curiosity_weight + cfg.adjustment_rate)
        else:
            self.curiosity_weight = max(cfg.weight_min, self.curiosity_weight - cfg.adjustment_rate)

    # ------------------------------------------------------------------
    # Training helpers
    # ------------------------------------------------------------------
    def _train_world_model(self) -> float:
        cfg = self.loss_cfg.world_model
        if len(self.transition_buffer) < cfg.warmup_transitions:
            return self._last_model_error

        total_loss = 0.0
        steps = 0
        for _ in range(cfg.train_steps):
            batch = self.transition_buffer.sample(cfg.batch_size, self.device)
            if batch is None:
                break
            states = batch["state"]
            actions = batch["action_enc"]
            rewards = batch["reward"]
            targets = batch["next_state"]

            self.world_model_opt.zero_grad()
            pred_next, pred_reward = self.world_model(states, actions)
            loss_state = F.mse_loss(pred_next, targets)
            loss_reward = F.mse_loss(pred_reward, rewards)
            loss = loss_state + loss_reward
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.world_model.parameters(), cfg.gradient_clip)
            self.world_model_opt.step()

            total_loss += float(loss.item())
            steps += 1

        if steps == 0:
            return self._last_model_error
        avg_loss = total_loss / steps
        self._last_model_error = avg_loss
        self.loss_tracker["world_model_loss"].append(avg_loss)
        return avg_loss

    def _train_curiosity_model(self) -> None:
        cfg = self.loss_cfg.curiosity
        if len(self.transition_buffer) < self.loss_cfg.world_model.warmup_transitions:
            return

        for _ in range(cfg.train_steps):
            batch = self.transition_buffer.sample(cfg.feature_dim * 2, self.device)
            if batch is None:
                break
            states = batch["state"]
            actions_enc = batch["action_enc"]
            actions_raw = batch["action_raw"]
            next_states = batch["next_state"]

            self.curiosity_opt.zero_grad()
            feature = self.curiosity_module.encode(states)
            next_feature = self.curiosity_module.encode(next_states)
            pred_next_feature = self.curiosity_module.predict_feature(feature, actions_enc)
            forward_loss = F.mse_loss(pred_next_feature, next_feature)

            action_pred = self.curiosity_module.predict_action(feature, next_feature)
            if action_pred.shape[-1] == self.action_dim and self._action_dtype == torch.int64:
                target = actions_raw.view(-1).long()
                action_loss = F.cross_entropy(action_pred, target)
            else:
                action_loss = F.mse_loss(action_pred, actions_enc)

            loss = forward_loss + action_loss
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.curiosity_module.parameters(), 1.0)
            self.curiosity_opt.step()

            self.loss_tracker["curiosity_forward_loss"].append(float(forward_loss.item()))
            self.loss_tracker["curiosity_inverse_loss"].append(float(action_loss.item()))

    def _generate_model_rollouts(self) -> None:
        cfg = self.loss_cfg.model_rollouts
        if len(self.transition_buffer) < self.loss_cfg.world_model.warmup_transitions:
            return

        starts = self.transition_buffer.sample(cfg.batch_size, self.device)
        if starts is None:
            return

        start_states = starts["next_state"]
        start_obs = self._unflatten_obs(start_states)
        trajectories: list[TensorDict] = []
        for idx in range(start_states.shape[0]):
            traj = self._simulate_rollout(
                state=start_states[idx : idx + 1],
                obs=start_obs[idx : idx + 1],
                max_length=cfg.rollout_length,
            )
            if traj is not None:
                trajectories.append(traj)

        if trajectories:
            for traj in trajectories:
                self.model_buffer.add(traj)

    def _simulate_rollout(self, state: Tensor, obs: Tensor, max_length: int) -> TensorDict | None:
        device = state.device
        values: list[Tensor] = []
        rewards: list[Tensor] = []
        extrinsic: list[Tensor] = []
        intrinsic: list[Tensor] = []
        actions_list: list[Tensor] = []
        logprob_list: list[Tensor] = []
        entropy_list: list[Tensor] = []
        dones_list: list[Tensor] = []
        obs_list: list[Tensor] = []

        current_state = state
        current_obs = obs
        done_flag = torch.zeros((1,), dtype=torch.float32, device=device)

        for _ in range(max_length):
            obs_list.append(current_obs.squeeze(0))
            policy_input = TensorDict(
                {
                    "env_obs": current_obs.to(device=device, dtype=torch.uint8),
                    "batch": torch.ones(1, dtype=torch.long, device=device),
                    "bptt": torch.ones(1, dtype=torch.long, device=device),
                },
                batch_size=(1,),
            )
            with torch.no_grad():
                policy_td = self.policy.forward(policy_input)
            action = policy_td["actions"]
            actions_list.append(action.squeeze(0))
            logprob_list.append(policy_td["act_log_prob"].squeeze(0))
            entropy_list.append(policy_td.get("entropy", torch.zeros_like(policy_td["act_log_prob"])).squeeze(0))
            values.append(policy_td["values"].squeeze(0))

            action_enc = self._encode_action(action)
            next_state, pred_reward, state_var, reward_var = self._world_model_predict(current_state, action_enc)
            next_obs = self._unflatten_obs(next_state)

            state_uncertainty = state_var.mean(dim=-1)
            reward_uncertainty = reward_var.squeeze(-1)
            intrinsic_reward = self.curiosity_weight * (
                self.loss_cfg.curiosity.positive_coef * state_uncertainty
                - self.loss_cfg.curiosity.negative_coef * reward_uncertainty
            )
            intrinsic_reward = intrinsic_reward.clamp(
                -self.loss_cfg.curiosity.reward_clip, self.loss_cfg.curiosity.reward_clip
            )
            total_reward = pred_reward + intrinsic_reward
            rewards.append(total_reward.squeeze(0))
            extrinsic.append(pred_reward.squeeze(0))
            intrinsic.append(intrinsic_reward.squeeze(0))
            dones_list.append(done_flag.squeeze(0))

            if reward_var.mean().item() > self.loss_cfg.model_rollouts.termination_variance_threshold:
                break

            current_state = next_state
            current_obs = next_obs

        if not rewards:
            return None

        traj_len = len(rewards)
        traj = TensorDict(
            {
                "env_obs": torch.stack(obs_list, dim=0),
                "actions": torch.stack(actions_list, dim=0),
                "act_log_prob": torch.stack(logprob_list, dim=0),
                "entropy": torch.stack(entropy_list, dim=0),
                "values": torch.stack(values, dim=0),
                "rewards": torch.stack(rewards, dim=0),
                "extrinsic_rewards": torch.stack(extrinsic, dim=0),
                "intrinsic_rewards": torch.stack(intrinsic, dim=0),
                "dones": torch.stack(dones_list, dim=0),
                "truncateds": torch.zeros(traj_len, dtype=torch.float32, device=device),
            },
            batch_size=(traj_len,),
        )

        advantage = torch.zeros_like(traj["values"])
        advantage = compute_advantage(
            traj["values"],
            traj["rewards"],
            traj["dones"],
            torch.ones_like(traj["values"]),
            advantage,
            self.loss_cfg.gamma,
            self.loss_cfg.gae_lambda,
            self.loss_cfg.vtrace.rho_clip,
            self.loss_cfg.vtrace.c_clip,
            device,
        )
        traj.set("advantages", advantage)
        traj.set("returns", advantage + traj["values"])
        return traj.detach()

    def _sample_model_batch(self, segments: int, device: torch.device) -> TensorDict | None:
        if segments <= 0:
            return None
        model_batch = self.model_buffer.sample(segments, device=device)
        if model_batch is None:
            return None

        traj_len = model_batch.batch_size[1]
        flat_actions = model_batch["actions"].reshape(segments * traj_len, -1)
        policy_input = TensorDict(
            {
                "env_obs": model_batch["env_obs"].reshape(segments * traj_len, *self.obs_shape),
                "batch": torch.full((segments * traj_len,), traj_len, dtype=torch.long, device=device),
                "bptt": torch.full((segments * traj_len,), traj_len, dtype=torch.long, device=device),
            },
            batch_size=(segments * traj_len,),
        )
        policy_td = self.policy.forward(policy_input, action=flat_actions)
        model_batch.set("policy_td", policy_td.reshape(segments, traj_len))
        return model_batch

    def _process_model_batch(self, model_batch: TensorDict) -> Tensor:
        policy_td: TensorDict = model_batch["policy_td"]
        old_logprob = model_batch["act_log_prob"]
        new_logprob = policy_td["act_log_prob"].reshape(old_logprob.shape)
        entropy = policy_td["entropy"]
        newvalue = policy_td["values"]

        importance = self._importance_ratio(new_logprob, old_logprob)
        adv = compute_advantage(
            model_batch["values"],
            model_batch["rewards"],
            model_batch["dones"],
            importance,
            model_batch["advantages"],
            self.loss_cfg.gamma,
            self.loss_cfg.gae_lambda,
            self.loss_cfg.vtrace.rho_clip,
            self.loss_cfg.vtrace.c_clip,
            self.device,
        )
        adv = normalize_advantage_distributed(adv, self.loss_cfg.norm_adv)

        pg_loss, v_loss, entropy_loss, approx_kl, clipfrac = self.compute_ppo_losses(
            model_batch,
            new_logprob,
            entropy,
            newvalue,
            importance,
            adv,
        )
        loss = pg_loss - self.loss_cfg.ent_coef * entropy_loss + v_loss * self.loss_cfg.vf_coef

        self._track("model_policy_loss", pg_loss)
        self._track("model_value_loss", v_loss)
        self._track("model_entropy", entropy_loss)
        self._track("model_clipfrac", clipfrac)
        self._track("model_approx_kl", approx_kl)
        return loss

    # ------------------------------------------------------------------
    # Core rollout/training loop overrides
    # ------------------------------------------------------------------
    def run_rollout(self, td: TensorDict, context: ComponentContext) -> None:
        with torch.no_grad():
            self.policy.forward(td)

        env_slice = context.training_env_id
        if env_slice is None:
            raise RuntimeError("ComponentContext.training_env_id is required for CMPO rollout")

        rewards = td["rewards"].to(dtype=torch.float32)
        dones = td["dones"].to(dtype=torch.float32)
        truncateds = td["truncateds"].to(dtype=torch.float32)
        terminals = torch.logical_or(dones > 0.5, truncateds > 0.5)

        obs = td["env_obs"]
        obs_flat = self._flatten_obs(obs)
        actions = td["actions"]
        actions_enc = self._encode_action(actions)
        actions_raw = actions.to(dtype=self._raw_action_dtype).view(actions.shape[0], *self._raw_action_shape)

        intrinsic_rewards = torch.zeros_like(rewards)
        extrinsic_rewards = rewards.clone()

        if self._has_prev is not None and self._has_prev[env_slice].any():
            mask = self._has_prev[env_slice]
            prev_states = self._prev_obs[env_slice][mask]
            prev_actions_enc = self._prev_action_enc[env_slice][mask]
            prev_actions_raw = self._prev_action_raw[env_slice][mask]
            prev_dones = self._prev_done[env_slice][mask].float()

            current_states = obs_flat[mask]
            current_rewards = rewards[mask]

            pred_next, pred_reward, _, _ = self._world_model_predict(prev_states, prev_actions_enc)
            intrinsic = self._compute_intrinsic_reward(
                prev_states,
                prev_actions_enc,
                current_states,
                current_rewards,
                pred_next,
                pred_reward,
            )
            intrinsic_rewards[mask] = intrinsic
            total_rewards = current_rewards + intrinsic
            td["rewards"][mask] = total_rewards

            self.transition_buffer.add_batch(
                states=prev_states.detach(),
                actions_enc=prev_actions_enc.detach(),
                actions_raw=prev_actions_raw.detach(),
                rewards=current_rewards.detach(),
                next_states=current_states.detach(),
                dones=prev_dones.detach(),
            )

        td["intrinsic_rewards"] = intrinsic_rewards
        td["extrinsic_rewards"] = extrinsic_rewards

        if self.burn_in_steps_iter < self.burn_in_steps:
            self.burn_in_steps_iter += 1
        else:
            self.replay.store(data_td=td, env_id=env_slice)

        if self._prev_obs is None or self._prev_action_enc is None or self._prev_action_raw is None:
            raise RuntimeError("CMPO replay buffers not initialized before rollout")

        self._prev_obs[env_slice] = obs_flat.detach()
        self._prev_action_enc[env_slice] = actions_enc.detach()
        self._prev_action_raw[env_slice] = actions_raw.detach()
        done_flags = terminals
        self._prev_done[env_slice] = done_flags
        self._has_prev[env_slice] = ~done_flags.bool()

    def run_train(
        self,
        shared_loss_data: TensorDict,
        context: ComponentContext,
        mb_idx: int,
    ) -> tuple[Tensor, TensorDict, bool]:
        stop_update_epoch = False

        if mb_idx == 0:
            model_error = self._train_world_model()
            self._train_curiosity_model()
            self._update_curiosity_weight(model_error)
            self.scheduler.update(model_error)
            self._generate_model_rollouts()
            self.advantages, self.anneal_beta = self._on_first_mb(context)

        minibatch, indices, prio_weights = self._sample_minibatch(
            advantages=self.advantages,
            prio_alpha=self.loss_cfg.prioritized_experience_replay.prio_alpha,
            prio_beta=self.anneal_beta,
        )

        shared_loss_data["sampled_mb"] = minibatch
        shared_loss_data["indices"] = NonTensorData(indices)
        policy_td = self._prepare_policy_input(minibatch)
        shared_loss_data["policy_td"] = policy_td

        real_loss = self._process_minibatch_update(
            minibatch=minibatch,
            policy_td=policy_td,
            indices=indices,
            prio_weights=prio_weights,
        )

        model_ratio = self.scheduler.current_ratio
        model_loss = torch.tensor(0.0, device=self.device)
        if model_ratio > 0:
            synthetic_segments = max(1, int(self.replay.minibatch_segments * model_ratio))
            model_batch = self._sample_model_batch(synthetic_segments, self.device)
            if model_batch is not None:
                model_loss = self._process_model_batch(model_batch)

        total_loss = (1 - model_ratio) * real_loss + model_ratio * model_loss
        return total_loss, shared_loss_data, stop_update_epoch

    def _prepare_policy_input(self, minibatch: TensorDict) -> TensorDict:
        policy_td = minibatch.select(*self.policy_experience_spec.keys(include_nested=True))
        B, TT = policy_td.batch_size
        policy_td = policy_td.reshape(B * TT)
        policy_td.set("bptt", torch.full((B * TT,), TT, device=policy_td.device, dtype=torch.long))
        policy_td.set("batch", torch.full((B * TT,), B, device=policy_td.device, dtype=torch.long))
        flat_actions = minibatch["actions"].reshape(B * TT, -1)
        policy_td = self.policy.forward(policy_td, action=flat_actions)
        return policy_td.reshape(B, TT)
