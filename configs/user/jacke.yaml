# @package __global__

defaults:
  - _self_

wandb:
  enabled: true

run_id: 492
run: ${oc.env:USER}.local.${run_id}

# LSTM policy for ColorTree tasks
agent:
  _target_: metta.agent.external.example.Recurrent

# PPO hyperparameters optimized for ColorTree
trainer:
  ppo:
    clip_coef: 0.2        # Higher clip helps small advantages
    ent_coef: 0.001       # Lower entropy bonus
    vf_coef: 0.5
    gamma: 0.95           # Shorter horizon matches 64-step episodes
  optimizer:
    learning_rate: 0.0008 # Larger step for small rewards

  # Optimized batch sizes for single-agent training
  batch_size: 2048        # Much smaller for single agent
  minibatch_size: 256     # Proportionally smaller
  bptt_horizon: 64        # Shorter horizon for simple task
  update_epochs: 2        # Fewer epochs per update
  forward_pass_minibatch_target_size: 256

  # More frequent checkpointing for single agent
  checkpoint:
    checkpoint_interval: 100
    wandb_checkpoint_interval: 200

  # More frequent evaluation
  simulation:
    evaluate_interval: 100

  # Fewer workers for single agent
  num_workers: 8
