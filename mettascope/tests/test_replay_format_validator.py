"""Tests for replay format validation.

Validates that replay files match the format specification in `mettascope/docs/replay_spec.md`.
"""

from __future__ import annotations

import json
import subprocess
import tempfile
import zlib
from pathlib import Path
from typing import Any

import pytest


def load_replay(path: str | Path) -> dict[str, Any]:
    """Load and decompress a .json.z replay file."""
    path = Path(path)
    if not path.name.endswith(".json.z"):
        raise ValueError("Replay file name must end with '.json.z'")

    with path.open("rb") as fh:
        compressed_data = fh.read()

    try:
        decompressed = zlib.decompress(compressed_data)
    except zlib.error as exc:
        raise ValueError("Failed to decompress replay file") from exc

    try:
        data: dict[str, Any] = json.loads(decompressed)
    except json.JSONDecodeError as exc:
        raise ValueError("Invalid JSON in replay file") from exc

    return data


# Required top-level keys for replay version 2.
_REQUIRED_KEYS = {
    "version",
    "num_agents",
    "max_steps",
    "map_size",
    "action_names",
    "item_names",
    "type_names",
    "objects",
}

# Optional new top-level keys for replay version 2.
_OPTIONAL_KEYS = {
    "file_name",
    "group_names",
    "reward_sharing_matrix",
    "mg_config",
}


def _require_fields(obj: dict[str, Any], fields: list[str], obj_name: str) -> None:
    """Assert that all required fields are present."""
    missing = [f for f in fields if f not in obj]
    if missing:
        raise ValueError(f"{obj_name} missing required fields: {missing}")


def _validate_type(value: Any, expected_type: type | tuple[type, ...], field_name: str) -> None:
    """Validate that value has the expected type."""
    if not isinstance(value, expected_type):
        type_name = (
            expected_type.__name__
            if isinstance(expected_type, type)
            else " or ".join(t.__name__ for t in expected_type)
        )
        raise ValueError(f"'{field_name}' must be {type_name}, got {type(value).__name__}")


def _validate_positive_int(value: Any, field_name: str) -> None:
    """Validate that value is a positive integer."""
    _validate_type(value, int, field_name)
    assert value > 0, f"'{field_name}' must be positive, got {value}"


def _validate_non_negative_number(value: Any, field_name: str) -> None:
    """Validate that value is a non-negative number."""
    _validate_type(value, (int, float), field_name)
    assert value >= 0, f"'{field_name}' must be non-negative, got {value}"


def _validate_string_list(lst: Any, field_name: str, allow_empty_strings: bool = False) -> None:
    """Validate that value is a list of strings."""
    _validate_type(lst, list, field_name)
    assert len(lst) > 0, f"'{field_name}' must not be empty"
    invalid_entries: list[str] = []
    for idx, value in enumerate(lst):
        if not isinstance(value, str):
            invalid_entries.append(f"index {idx}: expected str, got {type(value).__name__} ({value!r})")
        elif not value and not allow_empty_strings:
            invalid_entries.append(f"index {idx}: empty string")
    if invalid_entries:
        requirement = "strings" if allow_empty_strings else "non-empty strings"
        raise AssertionError(
            f"'{field_name}' must contain {requirement}; invalid entries: {', '.join(invalid_entries)}"
        )


def _validate_static_value(value: Any, field_name: str, expected_type: type | tuple[type, ...]) -> None:
    """Validate that value is a static value of the expected type (never a time series)."""
    _validate_type(value, expected_type, field_name)


def _validate_time_series(data: Any, field_name: str, expected_type: type | tuple[type, ...]) -> None:
    """Validate time series values: either single values (never changed) or arrays of [step, value] pairs."""

    # Check if it's a single value (field never changes)
    if isinstance(data, expected_type):
        return

    # Check if it's a time series array (field changes on specific steps)
    if isinstance(data, list):
        if len(data) == 0:
            return
        # Validate time series of [step, value] pairs
        for item in data:
            assert isinstance(item, list) and len(item) == 2, (
                f"'{field_name}' time series items must be [step, value] pairs"
            )
            assert isinstance(item[0], int) and item[0] >= 0, f"'{field_name}' time series step must be non-negative"
            assert isinstance(item[1], expected_type), (
                f"'{field_name}' time series value must be {_get_type_name(expected_type)}"
            )

        # First entry should be step 0
        assert data[0][0] == 0, f"'{field_name}' time series must start with step 0"
        return

    # Neither single value nor valid time series
    type_name = _get_type_name(expected_type)
    raise AssertionError(f"'{field_name}' must be {type_name} or time series of [step, {type_name}] pairs")


def _get_type_name(expected_type: type | tuple[type, ...]) -> str:
    """Get a readable name for a type or tuple of types."""
    return expected_type.__name__ if isinstance(expected_type, type) else " or ".join(t.__name__ for t in expected_type)


def _validate_inventory_format(inventory: Any, field_name: str) -> None:
    """Validate inventory format: single inventory list or time series of [step, inventory_list] pairs."""
    if inventory is None:
        return

    _validate_type(inventory, list, field_name)
    if len(inventory) == 0:
        return

    # Check if it's a single inventory list (never changed during replay)
    # Single inventory format: [[item_id, amount], [item_id, amount], ...]
    if len(inventory) > 0 and all(
        isinstance(item, list) and len(item) == 2 and isinstance(item[0], int) and isinstance(item[1], (int, float))
        for item in inventory
    ):
        _validate_inventory_list(inventory, field_name)
        return

    # Check if it's a time series format: [[step, inventory_list], ...]
    for item in inventory:
        assert isinstance(item, list) and len(item) == 2, (
            f"'{field_name}' time series items must be [step, inventory_list] pairs"
        )
        assert isinstance(item[0], int) and item[0] >= 0, f"'{field_name}' time series step must be non-negative"
        _validate_inventory_list(item[1], field_name)


def _validate_inventory_list(inventory_list: Any, field_name: str) -> None:
    """Validate a single inventory list: list of [item_id, amount] pairs."""
    _validate_type(inventory_list, list, field_name)

    for pair in inventory_list:
        assert isinstance(pair, list) and len(pair) == 2, f"'{field_name}' must contain [item_id, amount] pairs"
        assert isinstance(pair[0], int) and pair[0] >= 0, f"'{field_name}' item_id must be non-negative integer"
        assert isinstance(pair[1], (int, float)) and pair[1] >= 0, f"'{field_name}' amount must be non-negative number"


def validate_replay_schema(data: dict[str, Any]) -> None:
    """Validate that replay data matches the version 2 schema specification."""
    # Check required keys and absence of unexpected keys.
    data_keys = set(data.keys())
    missing = _REQUIRED_KEYS - data_keys
    allowed_keys = _REQUIRED_KEYS | _OPTIONAL_KEYS
    unexpected = data_keys - allowed_keys
    assert not missing, f"Missing required keys: {sorted(missing)}"
    assert not unexpected, f"Unexpected keys present: {sorted(unexpected)}"

    # Top-level field validation.
    assert data.get("version") == 2, f"'version' must equal 2, got {data.get('version')}"

    _validate_positive_int(data["num_agents"], "num_agents")
    _validate_non_negative_number(data["max_steps"], "max_steps")

    # Validate map_size.
    map_size = data["map_size"]
    _validate_type(map_size, list, "map_size")
    assert len(map_size) == 2, "'map_size' must have exactly 2 dimensions"
    for i, dim in enumerate(map_size):
        _validate_positive_int(dim, f"map_size[{i}]")

    # Required string lists.
    for field in ["action_names", "item_names", "type_names"]:
        _validate_string_list(data[field], field, allow_empty_strings=True)

    # Optional file_name validation.
    if "file_name" in data:
        file_name = data["file_name"]
        _validate_type(file_name, str, "file_name")
        assert file_name, "'file_name' must be non-empty"

    # Optional string lists.
    if "group_names" in data:
        _validate_string_list(data["group_names"], "group_names", allow_empty_strings=True)

    # Optional reward sharing matrix.
    if "reward_sharing_matrix" in data:
        matrix = data["reward_sharing_matrix"]
        _validate_type(matrix, list, "reward_sharing_matrix")
        num_agents = data["num_agents"]
        assert len(matrix) == num_agents, f"'reward_sharing_matrix' must have {num_agents} rows"
        for i, row in enumerate(matrix):
            _validate_type(row, list, f"reward_sharing_matrix[{i}]")
            assert len(row) == num_agents, f"'reward_sharing_matrix[{i}]' must have {num_agents} columns"
            assert all(isinstance(v, (int, float)) for v in row), f"'reward_sharing_matrix[{i}]' must contain numbers"

    # Objects validation.
    objects = data["objects"]
    _validate_type(objects, list, "objects")
    assert len(objects) > 0, "'objects' must not be empty"
    assert all(isinstance(obj, dict) for obj in objects), "'objects' must contain dictionaries"

    # Validate each object and count agents.
    agent_count = 0
    for i, obj in enumerate(objects):
        _validate_object(obj, i, data)
        if obj.get("is_agent") or "agent_id" in obj:
            agent_count += 1

    assert agent_count == data["num_agents"], f"Expected {data['num_agents']} agents, found {agent_count}"


def _validate_object(obj: dict[str, Any], obj_index: int, replay_data: dict[str, Any]) -> None:
    """Validate a single object in the replay."""
    obj_name = f"Object {obj_index}"

    # All objects have these required fields.
    required_fields = [
        "id",
        "type_id",
        "location",
        "orientation",
        "inventory",
        "inventory_max",
        "color",
        "is_swappable",
    ]
    _require_fields(obj, required_fields, obj_name)

    # Validate static fields.
    _validate_static_value(obj["id"], f"{obj_name}.id", int)
    _validate_positive_int(obj["id"], f"{obj_name}.id")

    type_id = obj["type_id"]
    _validate_static_value(type_id, f"{obj_name}.type_id", int)
    _validate_non_negative_number(type_id, f"{obj_name}.type_id")
    assert type_id < len(replay_data["type_names"]), f"{obj_name}.type_id {type_id} out of range"

    _validate_static_value(obj["is_swappable"], f"{obj_name}.is_swappable", bool)

    # Validate dynamic fields (always time series).
    _validate_location(obj["location"], obj_name)
    _validate_time_series(obj["orientation"], f"{obj_name}.orientation", (int, float))
    _validate_inventory_format(obj["inventory"], f"{obj_name}.inventory")
    _validate_time_series(obj["inventory_max"], f"{obj_name}.inventory_max", (int, float))
    _validate_time_series(obj["color"], f"{obj_name}.color", int)

    # Validate specific object types.
    if obj.get("is_agent") or "agent_id" in obj:
        _validate_agent_fields(obj, obj_name, replay_data)
    elif "input_resources" in obj:
        _validate_building_fields(obj, obj_name)


def _validate_location(location: Any, obj_name: str) -> None:
    """Validate location field format: single [x, y, z] or time series of [step, [x, y, z]] pairs."""
    field_name = f"{obj_name}.location"

    # Check if it's a single location (never changed during replay)
    # also make sure it's not a time series array with only 3 elements
    if isinstance(location, list) and len(location) == 3 and all(isinstance(coord, (int, float)) for coord in location):
        for i, coord in enumerate(location):
            _validate_type(coord, (int, float), f"{field_name}[{i}]")
        return

    # Check if it's a time series array (location changed during replay)
    _validate_type(location, list, field_name)
    assert len(location) > 0, f"{field_name} must have at least one entry"

    # Validate time series of [step, [x, y, z]] pairs
    for step_data in location:
        assert isinstance(step_data, list) and len(step_data) == 2, (
            f"{field_name} items must be [step, [x, y, z]] pairs"
        )
        assert isinstance(step_data[0], int) and step_data[0] >= 0, f"{field_name} step must be non-negative"
        coords = step_data[1]
        assert isinstance(coords, list) and len(coords) == 3, f"{field_name} coordinates must be [x, y, z]"
        for i, coord in enumerate(coords):
            _validate_type(coord, (int, float), f"{field_name} coord[{i}]")

    # Must start with step 0
    assert location[0][0] == 0, f"{field_name} must start with step 0"


def _validate_agent_fields(obj: dict[str, Any], obj_name: str, replay_data: dict[str, Any]) -> None:
    """Validate all agent-specific fields."""
    agent_fields = [
        "agent_id",
        "is_agent",
        "vision_size",
        "action_id",
        "action_param",
        "action_success",
        "current_reward",
        "total_reward",
        "freeze_remaining",
        "is_frozen",
        "freeze_duration",
        "group_id",
    ]
    _require_fields(obj, agent_fields, obj_name)

    # Validate static agent fields.
    agent_id = obj["agent_id"]
    _validate_static_value(agent_id, f"{obj_name}.agent_id", int)
    _validate_non_negative_number(agent_id, f"{obj_name}.agent_id")
    assert agent_id < replay_data["num_agents"], f"{obj_name}.agent_id {agent_id} out of range"

    _validate_static_value(obj["is_agent"], f"{obj_name}.is_agent", bool)
    assert obj["is_agent"] is True, f"{obj_name}.is_agent must be True"

    _validate_static_value(obj["vision_size"], f"{obj_name}.vision_size", int)
    _validate_positive_int(obj["vision_size"], f"{obj_name}.vision_size")

    _validate_static_value(obj["group_id"], f"{obj_name}.group_id", int)
    _validate_non_negative_number(obj["group_id"], f"{obj_name}.group_id")

    # Validate dynamic agent fields (always time series).
    _validate_time_series(obj["action_id"], f"{obj_name}.action_id", int)
    _validate_time_series(obj["action_param"], f"{obj_name}.action_param", int)
    _validate_time_series(obj["action_success"], f"{obj_name}.action_success", bool)
    _validate_time_series(obj["current_reward"], f"{obj_name}.current_reward", (int, float))
    _validate_time_series(obj["total_reward"], f"{obj_name}.total_reward", (int, float))
    _validate_time_series(obj["freeze_remaining"], f"{obj_name}.freeze_remaining", (int, float))
    _validate_time_series(obj["is_frozen"], f"{obj_name}.is_frozen", bool)
    _validate_time_series(obj["freeze_duration"], f"{obj_name}.freeze_duration", (int, float))

    # Validate action_id values are in range.
    _validate_action_id_range(obj["action_id"], obj_name, replay_data["action_names"])


def _validate_action_id_range(action_ids: Any, obj_name: str, action_names: list[str]) -> None:
    """Validate that action_id values are within the valid range."""
    # Handle single value case
    if isinstance(action_ids, int):
        assert 0 <= action_ids < len(action_names), f"{obj_name}.action_id {action_ids} out of range"
        return

    # Handle time series case
    if isinstance(action_ids, list):
        for step_data in action_ids:
            if isinstance(step_data, list) and len(step_data) == 2:
                action_id = step_data[1]
                assert 0 <= action_id < len(action_names), f"{obj_name}.action_id {action_id} out of range"


def _validate_building_fields(obj: dict[str, Any], obj_name: str) -> None:
    """Validate all building-specific fields."""
    building_fields = [
        "input_resources",
        "output_resources",
        "output_limit",
        "conversion_remaining",
        "is_converting",
        "conversion_duration",
        "cooldown_remaining",
        "is_cooling_down",
        "cooldown_duration",
    ]
    _require_fields(obj, building_fields, obj_name)

    # Validate static building fields.
    _validate_static_value(obj["output_limit"], f"{obj_name}.output_limit", (int, float))
    _validate_non_negative_number(obj["output_limit"], f"{obj_name}.output_limit")

    _validate_static_value(obj["conversion_duration"], f"{obj_name}.conversion_duration", (int, float))
    _validate_non_negative_number(obj["conversion_duration"], f"{obj_name}.conversion_duration")

    _validate_static_value(obj["cooldown_duration"], f"{obj_name}.cooldown_duration", (int, float))
    _validate_non_negative_number(obj["cooldown_duration"], f"{obj_name}.cooldown_duration")

    # Validate dynamic building fields (always time series).
    _validate_inventory_format(obj["input_resources"], f"{obj_name}.input_resources")
    _validate_inventory_format(obj["output_resources"], f"{obj_name}.output_resources")
    _validate_time_series(obj["conversion_remaining"], f"{obj_name}.conversion_remaining", (int, float))
    _validate_time_series(obj["is_converting"], f"{obj_name}.is_converting", bool)
    _validate_time_series(obj["cooldown_remaining"], f"{obj_name}.cooldown_remaining", (int, float))
    _validate_time_series(obj["is_cooling_down"], f"{obj_name}.is_cooling_down", bool)


def _make_valid_replay(file_name: str = "sample.json.z") -> dict[str, Any]:
    """Create a minimal valid replay dict per the spec."""
    return {
        "version": 2,
        "num_agents": 2,
        "max_steps": 100,
        "map_size": [10, 10],
        "file_name": "test replay file format",
        "type_names": ["agent", "resource"],
        "action_names": ["move", "collect"],
        "item_names": ["wood", "stone"],
        "group_names": ["group1", "group2"],
        "reward_sharing_matrix": [[1, 0], [0, 1]],
        "objects": [
            {
                "id": 1,
                "type_id": 0,
                "agent_id": 0,
                "is_agent": True,
                "vision_size": 11,
                "group_id": 0,
                "is_swappable": False,
                # Time series fields (some single values, some arrays for testing)
                "location": [[0, [5, 5, 0]], [1, [6, 5, 0]], [2, [7, 5, 0]]],
                "action_id": 0,
                "action_param": 0,
                "action_success": True,
                "current_reward": 0.0,
                "total_reward": 0.0,
                "freeze_remaining": 0,
                "is_frozen": False,
                "freeze_duration": 0,
                "orientation": 0,
                "inventory": [],
                "inventory_max": 10,
                "color": 0,
            },
            {
                "id": 2,
                "type_id": 0,
                "agent_id": 1,
                "is_agent": True,
                "vision_size": 11,
                "group_id": 0,
                "is_swappable": False,
                # Time series fields (mix of single values and arrays for testing)
                "location": [[0, [3, 3, 0]], [5, [4, 3, 0]]],
                "action_id": [[0, 1], [10, 0]],
                "action_param": 0,
                "action_success": [[0, False], [10, True]],
                "current_reward": 1.5,
                "total_reward": [[0, 0.0], [10, 1.5]],
                "freeze_remaining": 0,
                "is_frozen": False,
                "freeze_duration": 0,
                "orientation": 1,
                "inventory": [[0, []], [20, [[0, 2], [1, 1]]]],
                "inventory_max": 10,
                "color": 1,
            },
        ],
    }


def test_validate_replay_schema_valid() -> None:
    """The validator should accept a properly-formed replay."""
    valid_replay = _make_valid_replay()
    validate_replay_schema(valid_replay)


@pytest.mark.parametrize(
    "mutation, error_substr",
    [
        (lambda r: r.pop("version"), "Missing required keys"),
        (lambda r: r.update({"unexpected": 123}), "Unexpected keys present"),
        (lambda r: r.update({"version": 1}), "'version' must equal 2"),
        (lambda r: r.update({"num_agents": -1}), "'num_agents' must be positive"),
        (lambda r: r.update({"map_size": [0, 5]}), "'map_size\\[0\\]' must be positive"),
        (lambda r: r.update({"file_name": ""}), "'file_name' must be non-empty"),
        (lambda r: r.update({"action_names": ["collect", 1]}), "'action_names' must contain strings"),
        (lambda r: r.update({"objects": [123]}), "'objects' must contain dictionaries"),
    ],
)
def test_validate_replay_schema_invalid(mutation, error_substr: str) -> None:
    """Verify that the validator can detect invalid replays."""
    replay_dict = _make_valid_replay()
    mutation(replay_dict)

    with pytest.raises(AssertionError, match=error_substr):
        validate_replay_schema(replay_dict)


def test_validate_real_generated_replay_fast() -> None:
    """Generate a minimal fresh replay and validate it against the strict schema (fast version)."""
    with tempfile.TemporaryDirectory() as tmp_dir:
        # Generate a replay using the same command as CI but with a custom output directory.
        # Use minimal steps to speed up the test - we only need to validate the format, not run a full simulation
        cmd = [
            "uv",
            "run",
            "--no-sync",
            "tools/run.py",
            "experiments.recipes.scratchpad.ci.replay_null",
            "--overrides",
            f"replay_dir={tmp_dir}",
            f"stats_dir={tmp_dir}",
            "sim.env.game.max_steps=5",  # Reduce from 100 to 5 steps for faster test
        ]

        # Run from the project root (parent of mettascope).
        project_root = Path(__file__).parent.parent.parent
        result = subprocess.run(cmd, cwd=project_root, capture_output=True, text=True, timeout=30)  # Reduced timeout

        replay_files = list(Path(tmp_dir).glob("**/*.json.z"))
        if len(replay_files) == 0:
            raise AssertionError(
                f"No replay files were generated. Process exited with code {result.returncode}. "
                f"Error output: {result.stderr}"
            )

        # Should have exactly one replay file.
        assert len(replay_files) == 1, f"Expected exactly 1 replay file, found {len(replay_files)}: {replay_files}"

        # Validate the replay file.
        replay_path = replay_files[0]
        loaded_replay = load_replay(replay_path)
        validate_replay_schema(loaded_replay)

        print(f"✓ Successfully generated and validated fresh replay: {replay_path.name}")


def test_validate_real_generated_replay_comprehensive() -> None:
    """Generate a full-length replay using the CI setup and validate it against the strict schema."""
    with tempfile.TemporaryDirectory() as tmp_dir:
        # Generate a replay using the original CI configuration (100 steps)
        cmd = [
            "uv",
            "run",
            "--no-sync",
            "tools/run.py",
            "experiments.recipes.scratchpad.ci.replay_null",
            "--overrides",
            f"replay_dir={tmp_dir}",
            f"stats_dir={tmp_dir}",
        ]

        # Run from the project root (parent of mettascope).
        project_root = Path(__file__).parent.parent.parent
        result = subprocess.run(cmd, cwd=project_root, capture_output=True, text=True, timeout=120)

        replay_files = list(Path(tmp_dir).glob("**/*.json.z"))
        if len(replay_files) == 0:
            raise AssertionError(
                f"No replay files were generated. Process exited with code {result.returncode}. "
                f"Error output: {result.stderr}"
            )

        # Should have exactly one replay file.
        assert len(replay_files) == 1, f"Expected exactly 1 replay file, found {len(replay_files)}: {replay_files}"

        # Validate the replay file.
        replay_path = replay_files[0]
        loaded_replay = load_replay(replay_path)
        validate_replay_schema(loaded_replay)

        print(f"✓ Successfully generated and validated comprehensive replay: {replay_path.name}")
