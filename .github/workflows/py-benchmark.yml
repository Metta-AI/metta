name: py-benchmark
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}-${{ github.run_id }}
  cancel-in-progress: ${{ github.event_name != 'workflow_call' }}
on:
  workflow_dispatch: {}
  workflow_call:
    inputs:
      ref:
        required: false
        type: string


jobs:
  py-benchmark:
    if: >-
      github.event.pull_request.draft == false || 
      github.event_name == 'push' || 
      github.event_name == 'workflow_dispatch' || 
      github.event_name == 'workflow_call'
    runs-on: ubuntu-latest
    timeout-minutes: 30
    permissions:
      checks: write
      pull-requests: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ inputs.ref || github.ref }}

      # Skip file change check if triggered from py-checks workflow
      - name: Check for Python file changes
        id: py_benchmark_check_files
        if: github.event_name != 'workflow_call'
        uses: ./.github/actions/file-changes
        with:
          patterns: '*.py'
          specific_files: 'requirements.txt,setup.py'
          directory_paths: 'tests/,benchmarks/,deps/mettagrid/tests/,deps/mettagrid/benchmarks/'
          github-token: ${{ secrets.GITHUB_TOKEN }}
                  
      - name: Set early exit flag
        id: set_early_exit
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            # If called from a manual workflow, don't skip anything
            echo "EARLY_EXIT=false" >> $GITHUB_ENV
          elif [[ "${{ github.event_name }}" != "workflow_call" && "${{ steps.py_benchmark_check_files.outputs.has_relevant_changes }}" == "false" ]]; then
            # If we looked for changed files and found no relevant changes, exit early
            echo "EARLY_EXIT=true" >> $GITHUB_ENV
          else
            # In all other cases, don't exit early
            echo "EARLY_EXIT=false" >> $GITHUB_ENV
          fi
            
      - name: Skip benchmarking (no relevant changes)
        if: env.EARLY_EXIT == 'true'
        run: |
          echo "No Python files or benchmark-related files were changed. Skipping benchmarks."
          echo "This check passes automatically when no relevant files are changed."

      - name: Get main branch head SHA
        if: env.EARLY_EXIT == 'false'
        id: get-main-sha
        run: |
          git fetch origin main
          echo "MAIN_SHA=$(git rev-parse origin/main)" >> $GITHUB_OUTPUT

      - name: Setup Python
        if: env.EARLY_EXIT == 'false'
        uses: ./.github/actions/py-setup

      - name: Run Main Python benchmarks
        if: env.EARLY_EXIT == 'false'
        env:
          HYDRA_FULL_ERROR: 1
          PYTHONPATH: ${{ github.workspace }}
        run: |
          source venv/bin/activate
          pytest --benchmark-json=main_benchmark_results.json

      - name: Run Mettagrid Python benchmarks
        if: env.EARLY_EXIT == 'false'
        env:
          HYDRA_FULL_ERROR: 1
          PYTHONPATH: ${{ github.workspace }}
        run: |
          source venv/bin/activate
          cd deps/mettagrid
          pytest --benchmark-json=mettagrid_benchmark_results.json
          mv mettagrid_benchmark_results.json ../..

      - name: Combine benchmark results
        if: env.EARLY_EXIT == 'false'
        run: |
          source venv/bin/activate
          python - <<EOF
          import json
          import os
          
          # Function to safely load JSON, handling empty files
          def safe_load_json(file_path):
              try:
                  if os.path.exists(file_path):
                      with open(file_path, 'r') as f:
                          content = f.read().strip()
                          if content:  # Check if file has content
                              return json.loads(content)
                  return {}  # Return empty dict for non-existent or empty files
              except json.JSONDecodeError:
                  print(f"Warning: {file_path} contains invalid JSON or is empty. Using empty dict instead.")
                  return {}
          
          # Read main benchmark results
          main_results = safe_load_json('main_benchmark_results.json')
          
          # Read mettagrid benchmark results if file exists
          mettagrid_path = 'mettagrid_benchmark_results.json'
          mettagrid_results = safe_load_json(mettagrid_path)
          
          # Check if we have any valid results to work with
          if not main_results and not mettagrid_results:
              print("Both result files are empty or invalid. Creating empty combined results.")
              combined_results = {}
          else:
              # Combine results
              combined_results = main_results.copy()
              
              # If mettagrid results exist, add its benchmarks to the combined results
              if mettagrid_results:
                  # Add benchmark entries
                  if "benchmarks" in mettagrid_results:
                      if "benchmarks" not in combined_results:
                          combined_results["benchmarks"] = []
                      combined_results["benchmarks"].extend(mettagrid_results["benchmarks"])
                  
                  # Merge other fields as needed
                  for key in mettagrid_results:
                      if key != "benchmarks" and key not in combined_results:
                          combined_results[key] = mettagrid_results[key]
          
          # Write combined results
          with open('combined_benchmark_results.json', 'w') as f:
              json.dump(combined_results, f, indent=2)
          
          print("Successfully combined benchmark results.")
          EOF

      - name: Install Bencher CLI
        if: env.EARLY_EXIT == 'false'
        uses: bencherdev/bencher@main

      - name: Validate Bencher Token
        if: env.EARLY_EXIT == 'false'
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Add extra validation to catch this error early
          if [ -z "$BENCHER_API_TOKEN" ]; then
            echo "Error: BENCHER_API_TOKEN is empty. Make sure the secret is properly set."
            exit 1
          fi

          echo "Bencher token length: ${#BENCHER_API_TOKEN}"
          echo "Bencher token SHA256: $(echo -n "$BENCHER_API_TOKEN" | sha256sum)"

      - name: Upload to Bencher (Main Branch Baseline)
        if: env.EARLY_EXIT == 'false' && github.ref == 'refs/heads/main'
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          bencher run \
            --project mettagrid-sv3f5i2k \
            --token "$BENCHER_API_TOKEN" \
            --branch main \
            --testbed ubuntu-latest \
            --threshold-measure latency \
            --threshold-test t_test \
            --threshold-max-sample-size 64 \
            --threshold-upper-boundary 0.99 \
            --thresholds-reset \
            --adapter python_pytest \
            --github-actions "$GITHUB_TOKEN" \
            --file combined_benchmark_results.json > /dev/null

      - name: Upload to Bencher (PR Performance Changes)
        if: env.EARLY_EXIT == 'false' && github.event_name == 'pull_request' && !github.event.pull_request.head.repo.fork
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          bencher run \
            --project mettagrid-sv3f5i2k \
            --token "$BENCHER_API_TOKEN" \
            --branch "$GITHUB_HEAD_REF" \
            --start-point "main" \
            --start-point-hash "${{ steps.get-main-sha.outputs.MAIN_SHA }}" \
            --start-point-clone-thresholds \
            --start-point-reset \
            --testbed ubuntu-latest \
            --adapter python_pytest \
            --github-actions "$GITHUB_TOKEN" \
            --file combined_benchmark_results.json > /dev/null