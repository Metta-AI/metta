# Environmental Context Conditioning

This document describes the implementation of environmental context conditioning in the Metta trainer, which allows agents to learn environment-specific embeddings and condition their behavior based on the current environment.

## Overview

The environmental context system allows agents to:
1. Learn embeddings for different environments/tasks
2. Condition their behavior based on the current environment
3. Potentially improve performance by adapting strategies to specific environments

## Architecture

### Components

1. **EnvironmentalContextEmbedding Layer** (`metta/agent/src/metta/agent/lib/environmental_context.py`)
   - Learns embeddings for task IDs
   - Can be integrated in two ways:
     - **Strategy I**: Linear sum with LSTM input
     - **Strategy II**: Initial LSTM state conditioning (not yet implemented)

2. **Modified Fast Agent** (`metta/configs/agent/fast_with_context.yaml`)
   - Based on the standard fast agent
   - Includes environmental context embedding layer
   - Configurable via command line parameters

3. **Environment Integration** (`metta/mettagrid/src/metta/mettagrid/mettagrid_env.py`)
   - Computes task ID hash from curriculum task name
   - Passes task ID in environment info

4. **Training Integration** (`metta/metta/rl/util/rollout.py`, `metta/metta/rl/trainer.py`)
   - Sets task ID on agent before inference
   - Processes observation components before value/action components

## Usage

### Training with Environmental Context

```bash
# Train with environmental context enabled
./metta/recipes/navigation_with_context.sh

# Or manually with command line overrides
./devops/skypilot/launch.py train \
  run=$USER.navigation.with_context.$(date +%m-%d) \
  trainer.curriculum=env/mettagrid/curriculum/navigation/prioritize_regressed \
  --gpus=1 \
  +trainer.env_overrides.game.num_agents=4 \
  +agent.environmental_context.enabled=true \
  +agent.environmental_context.strategy=input_sum \
  sim=navigation
```

### Configuration Options

The environmental context can be configured via the agent config:

```yaml
environmental_context:
  enabled: false  # can be overridden via command line
  strategy: "input_sum"  # or "initial_state"
  embedding_dim: 128  # matches LSTM hidden size
  num_task_embeddings: 1000  # reasonable upper bound for task hash space
```

Command line overrides:
```bash
--agent.environmental_context.enabled=true
--agent.environmental_context.strategy=input_sum
--agent.environmental_context.embedding_dim=128
--agent.environmental_context.num_task_embeddings=1000
```

## Analysis

### Analyzing Learned Embeddings

Use the analysis script to examine learned embeddings:

```bash
python metta/tools/analyze_environmental_context.py /path/to/trained/policy \
  --output_dir ./embedding_analysis \
  --visualization_method pca
```

The analysis script provides:
1. **Embedding Statistics**: Norm, similarity statistics
2. **Similarity Heatmap**: Visual representation of task similarities
3. **Embedding Space Visualization**: PCA or t-SNE visualization
4. **Top Similar Pairs**: Most similar task embeddings

### Output Files

The analysis generates:
- `embedding_similarities.png`: Heatmap of cosine similarities
- `embedding_space_pca.png` or `embedding_space_tsne.png`: 2D visualization
- `analysis_results.txt`: Detailed analysis results

## Implementation Details

### Task ID Generation

Task IDs are generated by hashing the curriculum task name:
```python
task_id_hash = hash(task_name) % num_task_embeddings
```

This ensures:
- Consistent mapping for the same task across episodes
- Reasonable distribution across the embedding space
- No collisions with proper choice of `num_task_embeddings`

### Integration Strategy

**Strategy I (Input Sum)**:
- Environmental context embedding is added to encoded observations
- Simple linear combination: `obs + context_embedding`
- Minimal changes to existing architecture

**Strategy II (Initial State)** (not yet implemented):
- Environmental context conditions the initial LSTM state
- More complex but potentially more powerful
- Requires modifications to LSTM layer

### Component Flow

1. Environment computes task ID hash and includes in info
2. Trainer passes info to `run_policy_inference`
3. `run_policy_inference` sets `current_task_id` on agent
4. Agent forward pass includes task ID in TensorDict
5. Environmental context layer generates embedding
6. Embedding is added to observation stream
7. LSTM processes conditioned observations

## Comparison and Evaluation

### Baseline Training

For comparison, train without environmental context:
```bash
./devops/skypilot/launch.py train \
  run=$USER.navigation.without_context.$(date +%m-%d) \
  trainer.curriculum=env/mettagrid/curriculum/navigation/prioritize_regressed \
  --gpus=1 \
  +trainer.env_overrides.game.num_agents=4 \
  +agent.environmental_context.enabled=false \
  sim=navigation
```

### Evaluation Metrics

Compare the following metrics between runs:
1. **Task-specific performance**: Performance on individual navigation tasks
2. **Overall performance**: Average performance across all tasks
3. **Convergence speed**: How quickly the agent learns
4. **Embedding quality**: Meaningful structure in learned embeddings

## Future Work

1. **Strategy II Implementation**: Implement initial state conditioning
2. **Dynamic Task Discovery**: Automatically discover and embed new tasks
3. **Hierarchical Context**: Multi-level environmental context
4. **Transfer Learning**: Use learned embeddings for new tasks
5. **Interpretability**: Better understanding of what embeddings represent

## Troubleshooting

### Common Issues

1. **Policy doesn't have environmental context**:
   - Ensure using `fast_with_context.yaml` config
   - Check that `environmental_context.enabled=true`

2. **Task ID not being set**:
   - Verify environment is passing task ID in info
   - Check that `run_policy_inference` is receiving info

3. **Embedding dimension mismatch**:
   - Ensure `embedding_dim` matches LSTM hidden size (128 for fast agent)

### Debugging

Enable debug logging to trace task ID flow:
```python
import logging
logging.getLogger('metta.agent').setLevel(logging.DEBUG)
``` 