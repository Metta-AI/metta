{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Interactive Heatmap Widget Example\n",
    "\n",
    "This notebook demonstrates how to use the `HeatmapWidget` - an anywidget-based implementation of the observatory heatmap component for Jupyter notebooks.\n",
    "\n",
    "The widget provides interactive policy evaluation heatmaps with:\n",
    "- Have control over the number of policies displayed\n",
    "- Decide and select metrics to render\n",
    "- Choose which runs to get policies from\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Installation\n",
    "\n",
    "First, make sure you have the required dependencies:\n",
    "\n",
    "`pip install anywidget traitlets httpx jupyter`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Import and Basic Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"default\")\n",
    "\n",
    "%load_ext anywidget\n",
    "\n",
    "print(\"Setup complete! Auto-reload enabled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example: Demo Heatmap with Sample Data\n",
    "\n",
    "Let's start with a simple demo that includes sample data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from experiments.notebooks.utils.heatmap_widget import create_demo_heatmap, create_heatmap_widget\n",
    "\n",
    "# Create a demo heatmap with sample data\n",
    "demo_widget = create_demo_heatmap()\n",
    "\n",
    "# Display the widget\n",
    "demo_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Real Data from Metta Database\n",
    "\n",
    "Now let's write code that fetches real evaluation data from metta's databases:\n",
    "\n",
    "First we'll need some API variables.\n",
    "\n",
    "Then we'll make an API client for the metta API.\n",
    "\n",
    "Then we'll use the client to retrieve policy data.\n",
    "\n",
    "Then we'll render it with the HeatmapWidget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out this module, it will help you use this widget.\n",
    "from experiments.notebooks.utils.heatmap_widget import MettaAPIClient, fetch_real_heatmap_data\n",
    "\n",
    "api_base_url = \"https://api.observatory.softmax-research.net\"\n",
    "auth_token = \"\"  # from ~/.metta/observatory_tokens.yaml\n",
    "\n",
    "if not auth_token:\n",
    "    raise ValueError(\"No auth token found. Please set auth_token in the notebook. Check ~/.metta/observatory_tokens.yaml\")\n",
    "\n",
    "client = MettaAPIClient(api_base_url, auth_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example: Using Real Data\n",
    "\n",
    "Now let's explore what's available in the database and create a heatmap with real data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, let's try with some common metrics and see what we find:\n",
    "real_heatmap = None\n",
    "try:\n",
    "    # specific_runs = [\n",
    "    #     \"daveey.arena.rnd.16x4.2\",\n",
    "    #     \"relh.skypilot.fff.j20.666\",\n",
    "    #     \"bullm.navigation.low_reward.baseline\",\n",
    "    #     \"bullm.navigation.low_reward.baseline.07-17\", \n",
    "    #     \"bullm.navigation.low_reward.baseline.07-23\",\n",
    "    #     \"relh.multigpu.fff.1\",\n",
    "    #     \"relh.skypilot.fff.j21.2\",\n",
    "    # ]\n",
    "    \n",
    "    # Common metrics that are likely to exist:\n",
    "    metrics_to_fetch = [\"reward\", \"heart.get\", \"ore_red.get\", \"action.move.success\"]\n",
    "\n",
    "    client = MettaAPIClient(api_base_url, auth_token)\n",
    "    runs = await client.get_all_training_runs()\n",
    "    run_names = [policy[\"name\"] for policy in runs[\"policies\"]]\n",
    "    \n",
    "    real_heatmap = await fetch_real_heatmap_data(\n",
    "        api_base_url=api_base_url,\n",
    "        auth_token=auth_token,\n",
    "        training_run_names=run_names,\n",
    "        metrics=metrics_to_fetch,\n",
    "        max_policies=50  # Limit display to keep it manageable\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error fetching real data: {e}\")\n",
    "    print(\"üí° This might happen if:\")\n",
    "    print(\"   - The database URI is incorrect\")\n",
    "    print(\"   - You're not authenticated with wandb\")\n",
    "    print(\"   - The specified metrics don't exist in the database\")\n",
    "    print(\"   - You don't have access to the database\")\n",
    "    print(\"\\nüîÑ Falling back to demo data...\")\n",
    "    \n",
    "    # Fall back to demo data if real data fails\n",
    "    from experiments.notebooks.utils.heatmap_widget import create_demo_heatmap\n",
    "    demo_fallback = create_demo_heatmap()\n",
    "    demo_fallback\n",
    "\n",
    "real_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example: Simply get metrics for policies from the API\n",
    "\n",
    "Here's how to create a heatmap with your own training runs and metrics using the smart policy selection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a custom heatmap with specific training runs and metrics\n",
    "\n",
    "# Option 1: Use training run names (recommended - uses smart selection)\n",
    "my_training_runs = [\n",
    "    # Add your training run names here, for example:\n",
    "    \"relh.multigpu.fff.1\",\n",
    "    \"relh.skypilot.fff.j21.2\",\n",
    "    \"relh.skypilot.fff.j20.666\",\n",
    "]\n",
    "\n",
    "# Option 2: Use exact policy URIs (if you know exactly which ones you want)\n",
    "my_specific_policies = [\n",
    "    # Add exact policy URIs here, for example:\n",
    "    # \"my_policy_name:v123\",\n",
    "    # \"baseline_experiment:v456\",\n",
    "    # \"new_approach:v789\"\n",
    "]\n",
    "\n",
    "# Step 2: Define metrics you want to compare\n",
    "my_metrics = [\n",
    "    \"reward\",\n",
    "    \"heart.get\",           # Example game-specific metric\n",
    "    \"action.move.success\", # Example action success rate\n",
    "    # Add more metrics as needed\n",
    "]\n",
    "\n",
    "# Step 3: Optional - filter to specific evaluations\n",
    "# eval_filter = \"sim_env LIKE '%maze%'\"  # Only maze environments\n",
    "# eval_filter = \"sim_env LIKE '%combat%'\"  # Only combat environments  \n",
    "eval_filter = None  # No filter - include all evaluations\n",
    "\n",
    "# Step 4: Create the heatmap\n",
    "custom_heatmap = None\n",
    "if my_training_runs:  # Use smart policy selection from training runs\n",
    "    print(\"üéØ Creating custom heatmap with best policies from training runs...\")\n",
    "    \n",
    "    # Select best policies from training runs\n",
    "    custom_heatmap = await fetch_real_heatmap_data(\n",
    "        api_base_url=api_base_url,\n",
    "        auth_token=auth_token,\n",
    "        training_run_names=my_training_runs,\n",
    "        metrics=my_metrics,\n",
    "        policy_selector=\"best\",\n",
    "        max_policies=20\n",
    "    )\n",
    "    \n",
    "    print(\"üìä Custom heatmap created! Try:\")\n",
    "    print(\"   - Hovering over cells to see detailed values\")\n",
    "    print(\"   - Changing metrics with: custom_heatmap.update_metric('heart.get')\")\n",
    "    print(\"   - Adjusting policies shown: custom_heatmap.set_num_policies(15)\")\n",
    "    \n",
    "    custom_heatmap\n",
    "    \n",
    "else:\n",
    "    print(\"üìù To use this example:\")\n",
    "    print(\"   - Add your training run names to 'my_training_runs' list above\")\n",
    "\n",
    "custom_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example: Multiple Metrics with Working selectedMetric\n",
    "\n",
    "Now let's see the `selectedMetric` functionality working properly! This example shows a heatmap where changing the metric actually changes the displayed values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-metric heatmap widget\n",
    "from experiments.notebooks.utils.heatmap_widget import create_multi_metric_demo\n",
    "\n",
    "multi_metric_widget = create_multi_metric_demo()\n",
    "\n",
    "# Display the widget\n",
    "multi_metric_widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try changing the metric to see the values actually change!\n",
    "print(\"üîÑ Changing metric to 'episode_length'...\")\n",
    "multi_metric_widget.update_metric('episode_length')\n",
    "\n",
    "# NOTE: Notice how the values in the heatmap widget change as you switch\n",
    "# metrics?  Do not display the widget again and try to change that. That ends up\n",
    "# creating a seperate copy of the widget in a new output cell.  Instead just\n",
    "# reference the one you originally rendered, call its functions, and watch it\n",
    "# change in its Juypter notebook cell. Like we just did. Let's do it again in\n",
    "# the next cell too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more time. Run this cell then scroll back up again to see the change.\n",
    "print(\"\\nüîÑ Changing metric to 'success_rate'...\")\n",
    "multi_metric_widget.update_metric('success_rate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last one. Scroll up again to see the change.\n",
    "print(\"\\nüîÑ Changing metric to 'success_rate'...\")\n",
    "multi_metric_widget.update_metric('success_rate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Custom metrics\n",
    "\n",
    "We can really define our cells to have any metric data we want. This is useful because we plan to have all sorts of metrics. Let's look at an example of using any old metric we decide.\n",
    "\n",
    "Here's how to create a heatmap with your own data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new heatmap widget\n",
    "custom_widget = create_heatmap_widget()\n",
    "\n",
    "# Define your data structure\n",
    "# This should match the format expected by the observatory dashboard\n",
    "cells_data = {\n",
    "    'my_policy_v1': {\n",
    "        'task_a/level1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 85.2,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay1.json', \n",
    "            'evalName': 'task_a/level1'\n",
    "        },\n",
    "        'task_a/level2': {\n",
    "            'metrics': {\n",
    "                'custom_score': 87.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay2.json', \n",
    "            'evalName': 'task_a/level2'\n",
    "        },\n",
    "        'task_b/challenge1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 92.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay3.json', \n",
    "            'evalName': 'task_b/challenge1'\n",
    "        },\n",
    "    },\n",
    "    'my_policy_v2': {\n",
    "        'task_a/level1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 22.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay4.json', \n",
    "            'evalName': 'task_a/level1'\n",
    "        },\n",
    "        'task_a/level2': {\n",
    "            'metrics': {\n",
    "                'custom_score': 42.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay5.json', \n",
    "            'evalName': 'task_a/level2'\n",
    "        },\n",
    "        'task_b/challenge1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 62.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay6.json', \n",
    "            'evalName': 'task_b/challenge1'\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "eval_names = ['task_a/level1', 'task_a/level2', 'task_b/challenge1']\n",
    "policy_names = ['my_policy_v1', 'my_policy_v2']\n",
    "policy_averages = {\n",
    "    'my_policy_v1': 91.6,\n",
    "    'my_policy_v2': 89.6,\n",
    "}\n",
    "\n",
    "# Set the data\n",
    "custom_widget.set_data(\n",
    "    cells=cells_data,\n",
    "    eval_names=eval_names,\n",
    "    policy_names=policy_names,\n",
    "    policy_average_scores=policy_averages,\n",
    "    selected_metric=\"custom_score\"\n",
    ")\n",
    "\n",
    "# Display the widget\n",
    "custom_widget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example: Adding Callbacks for Interactivity\n",
    "\n",
    "You can add Python callbacks to respond to user interactions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: these callbacks do not work with print(), and that's really just how\n",
    "# Jupyter widgets work.  Once the Jupyter python cell finishes running and\n",
    "# outputs a widget, that widget won't be able to affect the output of the cell\n",
    "# anymore. The only way to to print() from a python widget callback is to write\n",
    "# to a file (or use a thread maybe). I give an example below.\n",
    "\n",
    "# Create another widget for callback demonstration\n",
    "callback_widget = create_heatmap_widget()\n",
    "\n",
    "# Set up the same data as before\n",
    "callback_widget.set_data(\n",
    "    cells=cells_data,\n",
    "    eval_names=eval_names,\n",
    "    policy_names=policy_names,\n",
    "    policy_average_scores=policy_averages,\n",
    "    selected_metric=\"Interactive Score (%)\"\n",
    ")\n",
    "\n",
    "# Define callback functions\n",
    "def handle_cell_selection(cell_info):\n",
    "    \"\"\"Called when user hovers over a cell (not 'overall' column).\"\"\"\n",
    "    with open(\"output_cell_selection.txt\", \"w\") as f:\n",
    "        f.write(f\"üìç Cell selected: {cell_info['policyUri']} on evaluation '{cell_info['evalName']}'\")\n",
    "\n",
    "def handle_replay_opened(replay_info):\n",
    "    \"\"\"Called when user clicks to open a replay.\"\"\"\n",
    "    with open(\"output_replay_opened.txt\", \"w\") as f:\n",
    "        f.write(f\"üé¨ Replay opened: {replay_info['replayUrl']}\")\n",
    "        f.write(f\"   Policy: {replay_info['policyUri']}\")\n",
    "        f.write(f\"   Evaluation: {replay_info['evalName']}\")\n",
    "\n",
    "# Register the callbacks\n",
    "callback_widget.on_cell_selected(handle_cell_selection)\n",
    "callback_widget.on_replay_opened(handle_replay_opened)\n",
    "\n",
    "# Display the widget\n",
    "callback_widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the files created by the callbacks in the previous cell, if they exist\n",
    "\n",
    "for fname in [\"output_cell_selection.txt\", \"output_replay_opened.txt\"]:\n",
    "    try:\n",
    "        with open(fname, \"r\") as f:\n",
    "            print(f.read())\n",
    "        os.remove(fname)\n",
    "        print(f\"File {fname} deleted\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Try interacting with the heatmap above to see the callback messages printed to\n",
    "*output files!**\n",
    "\n",
    "## Info: Data Cell Format Reference\n",
    "\n",
    "The heatmap widget expects data in a specific format that matches the\n",
    "observatory dashboard:\n",
    "\n",
    "```python\n",
    "cells = {\n",
    "    'policy_name': {\n",
    "        'eval_name': {\n",
    "            'metrics': {\n",
    "                'reward': 50,\n",
    "                'heart.get': 98,\n",
    "                'action.move.success': 5,\n",
    "                'ore_red.get': 24.2,\n",
    "                # ... more metrics\n",
    "            },\n",
    "            'replayUrl': str,         # URL to replay file\n",
    "            'evalName': str,          # Should match the key\n",
    "        },\n",
    "        # ... more evaluations\n",
    "    },\n",
    "    # ... more policies\n",
    "}\n",
    "```\n",
    "\n",
    "**Important notes:**\n",
    "- Evaluation names with \"/\" will be grouped by category (the part before \"/\")\n",
    "- The heatmap shows policies sorted by average score (worst to best, bottom to top)\n",
    "- Policy names that contain \":v\" will have WandB URLs generated automatically\n",
    "- Replay URLs should be accessible URLs or file paths\n",
    "\n",
    "This widget provides the same interactive functionality as the observatory dashboard but in a python environment, making it perfect for exploratory analysis and sharing results via Jupyter notebooks!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
