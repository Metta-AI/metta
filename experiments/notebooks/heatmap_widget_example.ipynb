{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Interactive Heatmap Widget Example\n",
    "\n",
    "This notebook demonstrates how to use the `HeatmapWidget` - an anywidget-based implementation of the observatory heatmap component for Jupyter notebooks.\n",
    "\n",
    "The widget provides interactive policy evaluation heatmaps with:\n",
    "- Have control over the number of policies displayed\n",
    "- Decide and select metrics to render\n",
    "- Choose which runs to get policies from\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Installation\n",
    "\n",
    "First, make sure you have the required dependencies:\n",
    "`pip install anywidget traitlets`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Import and Basic Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from experiments.notebooks.utils.metrics import fetch_metrics\n",
    "from experiments.notebooks.utils.monitoring import monitor_training_statuses\n",
    "from experiments.notebooks.utils.replays import show_replay\n",
    "from experiments.notebooks.utils.training import launch_training\n",
    "from experiments.notebooks.utils.metrics import find_training_jobs\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"default\")\n",
    "\n",
    "%load_ext anywidget\n",
    "\n",
    "print(\"Setup complete! Auto-reload enabled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example: Demo Heatmap with Sample Data\n",
    "\n",
    "Let's start with a simple demo that includes sample data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.notebooks.utils.heatmap_widget import HeatmapWidget, create_demo_heatmap, create_heatmap_widget\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a demo heatmap with sample data\n",
    "demo_widget = create_demo_heatmap()\n",
    "\n",
    "# Display the widget\n",
    "demo_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Real Data from Metta Database\n",
    "\n",
    "Now let's write code that fetches real evaluation data from metta's databases:\n",
    "\n",
    "First we'll need some API variables.\n",
    "\n",
    "Then we'll make an API client for the metta API.\n",
    "\n",
    "Then we'll use the client to retrieve policy data.\n",
    "\n",
    "Then we'll render it with the HeatmapWidget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_base_url = \"https://api.observatory.softmax-research.net\"\n",
    "auth_token = \"\"  # from ~/.metta/observatory_tokens.yaml\n",
    "\n",
    "if not auth_token:\n",
    "    raise ValueError(\"No auth token found. Please set auth_token in the notebook. Check ~/.metta/observatory_tokens.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Fixed API Client Implementation\n",
    "import asyncio\n",
    "import httpx\n",
    "from typing import Dict, List, Optional, Any\n",
    "\n",
    "class MettaAPIClient:\n",
    "    \"\"\"Fixed client that properly handles authentication and response parsing.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, auth_token: Optional[str] = None):\n",
    "        print(base_url)\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "        if auth_token:\n",
    "            # Use X-Auth-Token header format like extract_training_rewards.py\n",
    "            self.headers[\"X-Auth-Token\"] = auth_token\n",
    "\n",
    "    async def _make_request(self, method: str, endpoint: str, **kwargs):\n",
    "        \"\"\"Make an HTTP request to the API.\"\"\"\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        print(f\"üîç Making {method} request to: {url}\")\n",
    "        print(f\"üîë Headers: {self.headers}\")\n",
    "        if 'json' in kwargs:\n",
    "            print(f\"üì¶ Payload: {kwargs['json']}\")\n",
    "            \n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.request(method, url, headers=self.headers, timeout=30.0, **kwargs)\n",
    "            print(f\"üì® Response status: {response.status_code}\")\n",
    "            if response.status_code >= 400:\n",
    "                print(f\"‚ùå Response body: {response.text}\")\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "    \n",
    "    async def get_policies(self, search_text: Optional[str] = None, page_size: int = 50):\n",
    "        \"\"\"Get available policies and training runs.\"\"\"\n",
    "        url = \"/heatmap/policies\"\n",
    "        payload = {\n",
    "            \"search_text\": search_text,  # Use None instead of empty string\n",
    "            \"pagination\": {\"page\": 1, \"page_size\": page_size}\n",
    "        }\n",
    "        return await self._make_request(\"POST\", url, json=payload)\n",
    "    \n",
    "    async def get_eval_names(self, training_run_ids: List[str], run_free_policy_ids: List[str] = []):\n",
    "        \"\"\"Get evaluation names for selected policies.\"\"\"\n",
    "        url = \"/heatmap/evals\"\n",
    "        payload = {\n",
    "            \"training_run_ids\": training_run_ids,\n",
    "            \"run_free_policy_ids\": run_free_policy_ids\n",
    "        }\n",
    "        return await self._make_request(\"POST\", url, json=payload)\n",
    "    \n",
    "    async def get_available_metrics(self, training_run_ids: List[str], run_free_policy_ids: List[str], eval_names: List[str]):\n",
    "        \"\"\"Get available metrics for selected policies and evaluations.\"\"\"\n",
    "        url = \"/heatmap/metrics\"\n",
    "        payload = {\n",
    "            \"training_run_ids\": training_run_ids,\n",
    "            \"run_free_policy_ids\": run_free_policy_ids,\n",
    "            \"eval_names\": eval_names\n",
    "        }\n",
    "        return await self._make_request(\"POST\", url, json=payload)\n",
    "    \n",
    "    async def generate_heatmap(self, training_run_ids: List[str], run_free_policy_ids: List[str], \n",
    "                        eval_names: List[str], metric: str, policy_selector: str = \"best\"):\n",
    "        \"\"\"Generate heatmap data.\"\"\"\n",
    "        url = \"/heatmap/heatmap\"\n",
    "        payload = {\n",
    "            \"training_run_ids\": training_run_ids,\n",
    "            \"run_free_policy_ids\": run_free_policy_ids,\n",
    "            \"eval_names\": eval_names,\n",
    "            \"metric\": metric,\n",
    "            \"training_run_policy_selector\": policy_selector\n",
    "        }\n",
    "        return await self._make_request(\"POST\", url, json=payload)\n",
    "\n",
    "    async def get_all_training_runs(self, search_text: Optional[str] = None, page_size: int = 100):\n",
    "        \"\"\"Get all training run names.\"\"\"\n",
    "        url = \"/heatmap/policies\"\n",
    "        payload = {\n",
    "            \"search_text\": search_text,  # Use None instead of empty string\n",
    "            \"pagination\": {\"page\": 1, \"page_size\": page_size}\n",
    "        }\n",
    "        return await self._make_request(\"POST\", url, json=payload)\n",
    "\n",
    "# Test the fixed client with better debugging\n",
    "async def test_api_connection_fixed(api_base_url: str, auth_token: str):\n",
    "    \"\"\"Test API connection with the fixed client.\"\"\"\n",
    "    print(f\"üß™ Testing fixed API client...\")\n",
    "    client = MettaAPIClient(api_base_url, auth_token)\n",
    "    \n",
    "    try:\n",
    "        # Test getting policies\n",
    "        print(\"\\nüìã Testing /heatmap/policies endpoint...\")\n",
    "        policies_response = await client.get_policies(page_size=5)\n",
    "        print(f\"‚úÖ Success! Got response: {type(policies_response)}\")\n",
    "        if isinstance(policies_response, dict) and 'policies' in policies_response:\n",
    "            print(f\"üìä Found {len(policies_response['policies'])} policies\")\n",
    "            if policies_response['policies']:\n",
    "                print(f\"üîç Sample policy: {policies_response['policies'][0]}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Unexpected response structure: {policies_response}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "print(\"üöÄ Testing the fixed API client...\")\n",
    "test_result = await test_api_connection_fixed(\n",
    "    api_base_url=api_base_url,\n",
    "    auth_token=auth_token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_real_heatmap_data(\n",
    "    training_run_names: List[str],\n",
    "    metrics: List[str],\n",
    "    policy_selector: str = \"best\",\n",
    "    api_base_url: str = \"http://localhost:8000\",\n",
    "    auth_token: Optional[str] = None,\n",
    "    max_policies: int = 20\n",
    ") -> HeatmapWidget:\n",
    "    \"\"\"\n",
    "    Fetch real evaluation data using the metta HTTP API (same as repo.ts).\n",
    "    \n",
    "    Args:\n",
    "        training_run_names: List of training run names (e.g., [\"daveey.arena.rnd.16x4.2\"])\n",
    "        metrics: List of metrics to include (e.g., [\"reward\", \"heart.get\"])\n",
    "        policy_selector: \"best\" or \"latest\" policy selection strategy\n",
    "        api_base_url: Base URL for the metta API from ~/.metta/observatory_tokens.yaml\n",
    "        auth_token: Auth token from ~/.metta/observatory_tokens.yaml\n",
    "        max_policies: Maximum number of policies to display\n",
    "        \n",
    "    Returns:\n",
    "        HeatmapWidget with real data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MettaAPIClient(api_base_url, auth_token)\n",
    "        \n",
    "        # Step 1: Get available policies to find training run IDs\n",
    "        policies_data = await client.get_policies(page_size=100)\n",
    "        \n",
    "        # Find training run IDs that match our training run names\n",
    "        training_run_ids = []\n",
    "        for policy in policies_data[\"policies\"]:\n",
    "            if policy[\"type\"] == \"training_run\" and any(run_name in policy[\"name\"] for run_name in training_run_names):\n",
    "                training_run_ids.append(policy[\"id\"])\n",
    "        \n",
    "        if not training_run_ids:\n",
    "            print(f\"‚ùå No training runs found matching: {training_run_names}\")\n",
    "            return create_heatmap_widget()\n",
    "        \n",
    "        # Step 2: Get available evaluations for these training runs\n",
    "        eval_names = await client.get_eval_names(training_run_ids, [])\n",
    "        if not eval_names:\n",
    "            print(\"‚ùå No evaluations found for selected training runs\")\n",
    "            return create_heatmap_widget()\n",
    "        \n",
    "        # Step 3: Get available metrics \n",
    "        available_metrics = await get_available_metrics(training_run_ids, eval_names, api_base_url)\n",
    "        if not available_metrics:\n",
    "            print(\"‚ùå No metrics found\")\n",
    "            return create_heatmap_widget()\n",
    "        \n",
    "        # Filter to requested metrics that actually exist\n",
    "        valid_metrics = [m for m in metrics if m in available_metrics]\n",
    "        if not valid_metrics:\n",
    "            print(f\"‚ùå None of the requested metrics {metrics} are available\")\n",
    "            return create_heatmap_widget()\n",
    "        \n",
    "        # Step 4: Generate heatmap for the first metric\n",
    "        primary_metric = valid_metrics[0]\n",
    "        keys = eval_names\n",
    "        heatmap_data = await client.generate_heatmap(\n",
    "            training_run_ids, [], keys, primary_metric, policy_selector\n",
    "        )\n",
    "        \n",
    "        if not heatmap_data[\"policyNames\"]:\n",
    "            print(\"‚ùå No heatmap data generated\")\n",
    "            return create_heatmap_widget()\n",
    "        \n",
    "        # Limit policies if requested\n",
    "        policy_names = heatmap_data[\"policyNames\"]\n",
    "        if len(policy_names) > max_policies:\n",
    "            # Sort by average score and take top N\n",
    "            avg_scores = heatmap_data[\"policyAverageScores\"]\n",
    "            top_policies = sorted(avg_scores.keys(), key=lambda p: avg_scores[p], reverse=True)[:max_policies]\n",
    "            \n",
    "            # Filter the data\n",
    "            filtered_cells = {p: heatmap_data[\"cells\"][p] for p in top_policies if p in heatmap_data[\"cells\"]}\n",
    "            heatmap_data[\"policyNames\"] = top_policies\n",
    "            heatmap_data[\"cells\"] = filtered_cells\n",
    "            heatmap_data[\"policyAverageScores\"] = {p: avg_scores[p] for p in top_policies if p in avg_scores}\n",
    "        \n",
    "        # Step 5: Convert to widget format\n",
    "        cells = {}\n",
    "        for policy_name in heatmap_data[\"policyNames\"]:\n",
    "            cells[policy_name] = {}\n",
    "            for eval_name in heatmap_data[\"evalNames\"]:\n",
    "                cell = heatmap_data[\"cells\"].get(policy_name, {}).get(eval_name, {})\n",
    "                cells[policy_name][eval_name] = {\n",
    "                    'metrics': {primary_metric: cell.get(\"value\", 0.0)},\n",
    "                    'replayUrl': cell.get(\"replayUrl\"),\n",
    "                    'evalName': eval_name\n",
    "                }\n",
    "        \n",
    "        # Create widget\n",
    "        widget = create_heatmap_widget()\n",
    "        widget.set_multi_metric_data(\n",
    "            cells=cells,\n",
    "            eval_names=heatmap_data[\"evalNames\"], \n",
    "            policy_names=heatmap_data[\"policyNames\"],\n",
    "            metrics=[primary_metric],\n",
    "            selected_metric=primary_metric\n",
    "        )\n",
    "        \n",
    "        return widget\n",
    "        \n",
    "    except httpx.ConnectError:\n",
    "        print(\"‚ùå Could not connect to metta API server\")\n",
    "        print(\"üí° Check ~/.metta/observatory_tokens.yaml for the correct API base URL and auth token\")\n",
    "        print(\"üí° Check if app_backend server is running on http://localhost:8000\")\n",
    "        print(\"üí° You can start it with: cd app_backend && uv run python server.py\")\n",
    "        return create_heatmap_widget()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching real data: {e}\")\n",
    "        return create_heatmap_widget()\n",
    "\n",
    "async def get_available_policies(api_base_url: str = \"http://localhost:8000\", limit: int = 50):\n",
    "    \"\"\"Get available policies and training runs.\"\"\"\n",
    "    try:\n",
    "        client = MettaAPIClient(api_base_url, auth_token)\n",
    "        return await client.get_policies(page_size=limit)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching policies: {e}\")\n",
    "        return {\"policies\": []}\n",
    "\n",
    "async def get_available_eval_names(training_run_ids: List[str], api_base_url: str = \"http://localhost:8000\"):\n",
    "    \"\"\"Get available evaluation names.\"\"\"\n",
    "    try:\n",
    "        client = MettaAPIClient(api_base_url, auth_token)\n",
    "        return await client.get_eval_names(training_run_ids, [])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching eval names: {e}\")\n",
    "        return []\n",
    "\n",
    "async def get_available_metrics(training_run_ids: List[str], eval_names: List[str], api_base_url: str = \"http://localhost:8000\"):\n",
    "    \"\"\"Get available metrics.\"\"\"\n",
    "    try:\n",
    "        client = MettaAPIClient(api_base_url, auth_token)\n",
    "        \n",
    "        # Debug: Let's try with a simplified version first\n",
    "        print(f\"üîç Attempting to get metrics for {len(eval_names)} eval names...\")\n",
    "        print(f\"üìã Eval names: {eval_names}\")\n",
    "        \n",
    "        return await client.get_available_metrics(training_run_ids, [], eval_names)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching metrics: {e}\")\n",
    "        print(f\"üìã Eval names that caused the error: {eval_names}\")\n",
    "        print(f\"üîß This might be a server-side issue with processing eval_names list\")\n",
    "        \n",
    "        # Let's try with a single eval name to see if it's a list processing issue\n",
    "        if eval_names:\n",
    "            try:\n",
    "                print(f\"üîÑ Trying with just the first eval name: {eval_names[0]}\")\n",
    "                result = await client.get_available_metrics(training_run_ids, [], [eval_names[0]])\n",
    "                print(f\"‚úÖ Single eval name worked! Got {len(result)} metrics\")\n",
    "                return result\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Single eval name also failed: {e2}\")\n",
    "        \n",
    "        print(f\"üí° Falling back to common metrics...\")\n",
    "        # Return some common metrics as fallback\n",
    "        return [\"reward\", \"heart.get\", \"ore_red.get\", \"action.move.success\"]\n",
    "\n",
    "print(\"üöÄ Metta HTTP API client loaded!\")\n",
    "print(\"üìã Available functions:\")\n",
    "print(\"   - fetch_real_heatmap_data(training_run_names, metrics, policy_selector)\")\n",
    "print(\"   - get_available_policies(api_base_url)\")  \n",
    "print(\"   - get_available_eval_names(training_run_ids, api_base_url)\")\n",
    "print(\"   - get_available_metrics(training_run_ids, eval_names, api_base_url)\")\n",
    "print(\"\")\n",
    "print(\"üí° This uses HTTP API calls exactly like repo.ts!\")\n",
    "print(\"üîó Local development needs app_backend server on http://localhost:8000 and an auth token from ~/.metta/observatory_tokens.yaml\")\n",
    "print(\"üöÄ Start with: cd app_backend && uv run python server.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example: Using Real Data\n",
    "\n",
    "Now let's explore what's available in the database and create a heatmap with real data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, let's try with some common metrics and see what we find:\n",
    "real_heatmap = None\n",
    "try:\n",
    "    # specific_runs = [\n",
    "    #     \"daveey.arena.rnd.16x4.2\",\n",
    "    #     \"relh.skypilot.fff.j20.666\",\n",
    "    #     \"bullm.navigation.low_reward.baseline\",\n",
    "    #     \"bullm.navigation.low_reward.baseline.07-17\", \n",
    "    #     \"bullm.navigation.low_reward.baseline.07-23\",\n",
    "    #     \"relh.multigpu.fff.1\",\n",
    "    #     \"relh.skypilot.fff.j21.2\",\n",
    "    # ]\n",
    "    \n",
    "    # Common metrics that are likely to exist:\n",
    "    metrics_to_fetch = [\"reward\", \"heart.get\", \"ore_red.get\", \"action.move.success\"]\n",
    "\n",
    "    print(api_base_url)\n",
    "    client = MettaAPIClient(api_base_url, auth_token)\n",
    "    runs = await client.get_all_training_runs()\n",
    "    run_names = [policy[\"name\"] for policy in runs[\"policies\"]]\n",
    "    \n",
    "    real_heatmap = await fetch_real_heatmap_data(\n",
    "        api_base_url=api_base_url,\n",
    "        auth_token=auth_token,\n",
    "        training_run_names=run_names,\n",
    "        metrics=metrics_to_fetch,\n",
    "        max_policies=50  # Limit display to keep it manageable\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error fetching real data: {e}\")\n",
    "    print(\"üí° This might happen if:\")\n",
    "    print(\"   - The database URI is incorrect\")\n",
    "    print(\"   - You're not authenticated with wandb\")\n",
    "    print(\"   - The specified metrics don't exist in the database\")\n",
    "    print(\"   - You don't have access to the database\")\n",
    "    print(\"\\nüîÑ Falling back to demo data...\")\n",
    "    \n",
    "    # Fall back to demo data if real data fails\n",
    "    from experiments.notebooks.utils.heatmap_widget import create_demo_heatmap\n",
    "    demo_fallback = create_demo_heatmap()\n",
    "    demo_fallback\n",
    "\n",
    "real_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example: Simply get metrics for policies from the API\n",
    "\n",
    "Here's how to create a heatmap with your own training runs and metrics using the smart policy selection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a custom heatmap with specific training runs and metrics\n",
    "\n",
    "# Option 1: Use training run names (recommended - uses smart selection)\n",
    "my_training_runs = [\n",
    "    # Add your training run names here, for example:\n",
    "    \"relh.multigpu.fff.1\",\n",
    "    \"relh.skypilot.fff.j21.2\",\n",
    "    \"relh.skypilot.fff.j20.666\",\n",
    "]\n",
    "\n",
    "# Option 2: Use exact policy URIs (if you know exactly which ones you want)\n",
    "my_specific_policies = [\n",
    "    # Add exact policy URIs here, for example:\n",
    "    # \"my_policy_name:v123\",\n",
    "    # \"baseline_experiment:v456\",\n",
    "    # \"new_approach:v789\"\n",
    "]\n",
    "\n",
    "# Step 2: Define metrics you want to compare\n",
    "my_metrics = [\n",
    "    \"reward\",\n",
    "    \"heart.get\",           # Example game-specific metric\n",
    "    \"action.move.success\", # Example action success rate\n",
    "    # Add more metrics as needed\n",
    "]\n",
    "\n",
    "# Step 3: Optional - filter to specific evaluations\n",
    "# eval_filter = \"sim_env LIKE '%maze%'\"  # Only maze environments\n",
    "# eval_filter = \"sim_env LIKE '%combat%'\"  # Only combat environments  \n",
    "eval_filter = None  # No filter - include all evaluations\n",
    "\n",
    "# Step 4: Create the heatmap\n",
    "custom_heatmap = None\n",
    "if my_training_runs:  # Use smart policy selection from training runs\n",
    "    print(\"üéØ Creating custom heatmap with best policies from training runs...\")\n",
    "    \n",
    "    # Select best policies from training runs\n",
    "    custom_heatmap = await fetch_real_heatmap_data(\n",
    "        api_base_url=api_base_url,\n",
    "        auth_token=auth_token,\n",
    "        training_run_names=my_training_runs,\n",
    "        metrics=my_metrics,\n",
    "        policy_selector=\"best\",\n",
    "        max_policies=20\n",
    "    )\n",
    "    \n",
    "    print(\"üìä Custom heatmap created! Try:\")\n",
    "    print(\"   - Hovering over cells to see detailed values\")\n",
    "    print(\"   - Changing metrics with: custom_heatmap.update_metric('heart.get')\")\n",
    "    print(\"   - Adjusting policies shown: custom_heatmap.set_num_policies(15)\")\n",
    "    \n",
    "    custom_heatmap\n",
    "    \n",
    "else:\n",
    "    print(\"üìù To use this example:\")\n",
    "    print(\"   - Add your training run names to 'my_training_runs' list above\")\n",
    "\n",
    "custom_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example Advanced Usage: Custom Policies and Metrics\n",
    "\n",
    "Here's how to create a heatmap with specific policies and metrics of your choice:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a custom heatmap with specific policies and metrics\n",
    "\n",
    "# Step 1: Define your policies of interest\n",
    "my_policies = [\n",
    "    # Add your policy names here, for example:\n",
    "    # \"my_experiment_1:v100\",\n",
    "    # \"my_experiment_2:v200\", \n",
    "    # \"baseline_policy:v50\"\n",
    "]\n",
    "\n",
    "# Step 2: Define metrics you want to compare\n",
    "my_metrics = [\n",
    "    \"reward\",\n",
    "    \"heart.get\",           # Example game-specific metric\n",
    "    \"action.move.success\", # Example action success rate\n",
    "    # Add more metrics as needed\n",
    "]\n",
    "\n",
    "# Step 3: Optional - filter to specific evaluations\n",
    "# eval_filter = \"sim_env LIKE '%maze%'\"  # Only maze environments\n",
    "# eval_filter = \"sim_env LIKE '%combat%'\"  # Only combat environments  \n",
    "eval_filter = None  # No filter - include all evaluations\n",
    "\n",
    "# Step 4: Create the heatmap\n",
    "custom_heatmap2 = None\n",
    "if my_policies:  # Only run if you've specified policies\n",
    "    print(\"üéØ Creating custom heatmap...\")\n",
    "    custom_heatmap2 = fetch_real_heatmap_data(\n",
    "        api_base_url=api_base_url,\n",
    "        auth_token=auth_token,\n",
    "        training_run_names=my_policies,\n",
    "        metrics=my_metrics,\n",
    "        max_policies=20\n",
    "    )\n",
    "    \n",
    "    # Step 5: Display and interact\n",
    "    print(\"üìä Custom heatmap created! Try:\")\n",
    "    print(\"   - Hovering over cells to see detailed values\")\n",
    "    print(\"   - Changing metrics with: custom_heatmap.update_metric('heart.get')\")\n",
    "    print(\"   - Adjusting policies shown: custom_heatmap.set_num_policies(15)\")\n",
    "\n",
    "else:\n",
    "    print(\"üìù To use this example:\")\n",
    "    print(\"1. Uncomment the exploration code in the previous cell to see available policies\")\n",
    "    print(\"2. Add your policy names to the 'my_policies' list above\") \n",
    "    print(\"3. Customize the 'my_metrics' list with metrics you're interested in\")\n",
    "    print(\"4. Run this cell again\")\n",
    "    print(\"\\nüí° Example policy names might look like:\")\n",
    "    print(\"   - 'my_policy_name:v123'\")\n",
    "    print(\"   - 'baseline_experiment:v456'\") \n",
    "    print(\"   - 'new_approach:v789'\")\n",
    "\n",
    "custom_heatmap2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example: Multiple Metrics with Working selectedMetric\n",
    "\n",
    "Now let's see the `selectedMetric` functionality working properly! This example shows a heatmap where changing the metric actually changes the displayed values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-metric heatmap widget\n",
    "from experiments.notebooks.utils.heatmap_widget import create_multi_metric_demo\n",
    "\n",
    "multi_metric_widget = create_multi_metric_demo()\n",
    "\n",
    "# Display the widget\n",
    "multi_metric_widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try changing the metric to see the values actually change!\n",
    "print(\"üîÑ Changing metric to 'episode_length'...\")\n",
    "multi_metric_widget.update_metric('episode_length')\n",
    "\n",
    "# NOTE: Notice how the values in the heatmap widget change as you switch\n",
    "# metrics?  Do not display the widget again and try to change that. That ends up\n",
    "# creating a seperate copy of the widget in a new output cell.  Instead just\n",
    "# reference the one you originally rendered, call its functions, and watch it\n",
    "# change in its Juypter notebook cell. Like we just did. Let's do it again in\n",
    "# the next cell too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more time. Run this cell then scroll back up again to see the change.\n",
    "print(\"\\nüîÑ Changing metric to 'success_rate'...\")\n",
    "multi_metric_widget.update_metric('success_rate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last one. Scroll up again to see the change.\n",
    "print(\"\\nüîÑ Changing metric to 'success_rate'...\")\n",
    "multi_metric_widget.update_metric('success_rate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Custom metrics\n",
    "\n",
    "We can really define our cells to have any metric data we want. This is useful because we plan to have all sorts of metrics. Let's look at an example of using any old metric we decide.\n",
    "\n",
    "Here's how to create a heatmap with your own data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new heatmap widget\n",
    "custom_widget = create_heatmap_widget()\n",
    "\n",
    "# Define your data structure\n",
    "# This should match the format expected by the observatory dashboard\n",
    "cells_data = {\n",
    "    'my_policy_v1': {\n",
    "        'task_a/level1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 85.2,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay1.json', \n",
    "            'evalName': 'task_a/level1'\n",
    "        },\n",
    "        'task_a/level2': {\n",
    "            'metrics': {\n",
    "                'custom_score': 87.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay2.json', \n",
    "            'evalName': 'task_a/level2'\n",
    "        },\n",
    "        'task_b/challenge1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 92.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay3.json', \n",
    "            'evalName': 'task_b/challenge1'\n",
    "        },\n",
    "    },\n",
    "    'my_policy_v2': {\n",
    "        'task_a/level1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 22.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay4.json', \n",
    "            'evalName': 'task_a/level1'\n",
    "        },\n",
    "        'task_a/level2': {\n",
    "            'metrics': {\n",
    "                'custom_score': 42.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay5.json', \n",
    "            'evalName': 'task_a/level2'\n",
    "        },\n",
    "        'task_b/challenge1': {\n",
    "            'metrics': {\n",
    "                'custom_score': 62.5,\n",
    "            },\n",
    "            'replayUrl': 'https://example.com/replay6.json', \n",
    "            'evalName': 'task_b/challenge1'\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "eval_names = ['task_a/level1', 'task_a/level2', 'task_b/challenge1']\n",
    "policy_names = ['my_policy_v1', 'my_policy_v2']\n",
    "policy_averages = {\n",
    "    'my_policy_v1': 91.6,\n",
    "    'my_policy_v2': 89.6,\n",
    "}\n",
    "\n",
    "# Set the data\n",
    "custom_widget.set_data(\n",
    "    cells=cells_data,\n",
    "    eval_names=eval_names,\n",
    "    policy_names=policy_names,\n",
    "    policy_average_scores=policy_averages,\n",
    "    selected_metric=\"custom_score\"\n",
    ")\n",
    "\n",
    "# Display the widget\n",
    "custom_widget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Example: Adding Callbacks for Interactivity\n",
    "\n",
    "You can add Python callbacks to respond to user interactions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: these callbacks do not work with print(), and that's really just how\n",
    "# Jupyter widgets work.  Once the Jupyter python cell finishes running and\n",
    "# outputs a widget, that widget won't be able to affect the output of the cell\n",
    "# anymore. The only way to to print() from a python widget callback is to write\n",
    "# to a file (or use a thread maybe). I give an example below.\n",
    "\n",
    "# Create another widget for callback demonstration\n",
    "callback_widget = create_heatmap_widget()\n",
    "\n",
    "# Set up the same data as before\n",
    "callback_widget.set_data(\n",
    "    cells=cells_data,\n",
    "    eval_names=eval_names,\n",
    "    policy_names=policy_names,\n",
    "    policy_average_scores=policy_averages,\n",
    "    selected_metric=\"Interactive Score (%)\"\n",
    ")\n",
    "\n",
    "# Define callback functions\n",
    "def handle_cell_selection(cell_info):\n",
    "    \"\"\"Called when user hovers over a cell (not 'overall' column).\"\"\"\n",
    "    with open(\"output_cell_selection.txt\", \"w\") as f:\n",
    "        f.write(f\"üìç Cell selected: {cell_info['policyUri']} on evaluation '{cell_info['evalName']}'\")\n",
    "\n",
    "def handle_replay_opened(replay_info):\n",
    "    \"\"\"Called when user clicks to open a replay.\"\"\"\n",
    "    with open(\"output_replay_opened.txt\", \"w\") as f:\n",
    "        f.write(f\"üé¨ Replay opened: {replay_info['replayUrl']}\")\n",
    "        f.write(f\"   Policy: {replay_info['policyUri']}\")\n",
    "        f.write(f\"   Evaluation: {replay_info['evalName']}\")\n",
    "\n",
    "# Register the callbacks\n",
    "callback_widget.on_cell_selected(handle_cell_selection)\n",
    "callback_widget.on_replay_opened(handle_replay_opened)\n",
    "\n",
    "# Display the widget\n",
    "callback_widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the files created by the callbacks in the previous cell, if they exist\n",
    "import os\n",
    "\n",
    "for fname in [\"output_cell_selection.txt\", \"output_replay_opened.txt\"]:\n",
    "    try:\n",
    "        with open(fname, \"r\") as f:\n",
    "            print(f.read())\n",
    "        os.remove(fname)\n",
    "        print(f\"File {fname} deleted\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Try interacting with the heatmap above to see the callback messages printed to\n",
    "*output files!**\n",
    "\n",
    "## Info: Data Cell Format Reference\n",
    "\n",
    "The heatmap widget expects data in a specific format that matches the\n",
    "observatory dashboard:\n",
    "\n",
    "```python\n",
    "cells = {\n",
    "    'policy_name': {\n",
    "        'eval_name': {\n",
    "            'metrics': {\n",
    "                'reward': 50,\n",
    "                'heart.get': 98,\n",
    "                'action.move.success': 5,\n",
    "                'ore_red.get': 24.2,\n",
    "                # ... more metrics\n",
    "            },\n",
    "            'replayUrl': str,         # URL to replay file\n",
    "            'evalName': str,          # Should match the key\n",
    "        },\n",
    "        # ... more evaluations\n",
    "    },\n",
    "    # ... more policies\n",
    "}\n",
    "```\n",
    "\n",
    "**Important notes:**\n",
    "- Evaluation names with \"/\" will be grouped by category (the part before \"/\")\n",
    "- The heatmap shows policies sorted by average score (worst to best, bottom to top)\n",
    "- Policy names that contain \":v\" will have WandB URLs generated automatically\n",
    "- Replay URLs should be accessible URLs or file paths\n",
    "\n",
    "This widget provides the same interactive functionality as the observatory dashboard but in a python environment, making it perfect for exploratory analysis and sharing results via Jupyter notebooks!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
