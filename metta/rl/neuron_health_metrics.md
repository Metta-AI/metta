# Approach and caveat

When youâ€™re using ReLU activations, the â€œdead neuronâ€ check is simple because the condition is baked into the math: a
ReLU is either on or off. If its output is always zero, you know itâ€™s permanently inactive â€” and you can detect that
just by looking at the outputs after a forward pass. Thatâ€™s possible because ReLU has a literal zero-derivative region,
so a neuron that never crosses zero will never receive a gradient again. The situation with ReLU activation is unique in
having a clear definition of a dead or dormant neuron. In general, itâ€™s not so clear â€” a good definition is that a
neuron doesnâ€™t have any discriminatory power over values in the actual input distribution, and that this state is locked
in over time. To meet that definition you need a region where the output is constant and the derivative is zero, and
this simply doesnâ€™t happen for tanh, sigmoid, or leaky ReLU. In those cases, the derivative is always nonzero at zero,
and what youâ€™re really measuring is whether neurons are saturated, meaning they spend most of their time in the flat
tails of the activation curve. Thatâ€™s a harder call and depends on the data distribution.

That's why the code for these checks looks more complex and "intrusive." To measure saturation or vanishing gradients,
we can't just look at weights or outputs after training; we need to tap into both the forward and backward passes. The
forward hooks record what each neuron is doing with the real data (activation means, variances, slopes), while the
backward hooks track whether gradients are still flowing through it.

For a [technical discussion](hook_architecture.md) of how hooks are implemented in the forward and backward passes, see
`hook_architecture.md`.

# Neuron Health Diagnostics Summary

This document summarizes diagnostic measures for detecting dead, dormant, or saturated neurons â€” separated by activation
type.

---

## ğŸŸ© ReLU-family activations (ReLU, ReLU6, etc.)

| **Measure**               | **What it measures**             | **Interpretation**                          | **Needs data?** | **Implemented** | **Notes**                                          |
| ------------------------- | -------------------------------- | ------------------------------------------- | --------------- | --------------- | -------------------------------------------------- |
| Activation on-rate        | Frequency of neuron firing       | 0 â†’ dead; low â†’ dormant; mid â†’ healthy      | âœ… Yes          | âœ… Yes          | Canonical dead ReLU metric                         |
| Pre-activation statistics | Mean & variance of z = WÂ·x + b   | mean â‰ª 0, var small â†’ dead                  | âœ… Yes          | âŒ No           | Explains why itâ€™s dead                             |
| Gradient flow (EMA)       | How often backward signal passes | 0 â†’ neuron never updated                    | âœ… Yes          | âœ… Yes          | Equivalent to derivative magnitude Ã— upstream grad |
| Fisher proxy              | Long-term information content    | Near-zero â†’ unused parameter                | âœ… Yes          | âœ… Yes          | Cheap curvature estimate                           |
| Activation entropy        | Variability of outputs           | Low entropy near 0 â†’ always off             | âœ… Yes          | âŒ No           | Detects static outputs                             |
| Ablation sensitivity      | Functional importance            | Î”â‰ˆ0 â†’ redundant/dead                        | âœ… Yes          | âŒ No           | Costly but definitive                              |
| Weight norm               | Magnitude of w                   | Very small â†’ negligible influence           | âŒ No           | âŒ No           | Weak heuristic                                     |
| Bias negativity           | Offsetting bias                  | b â‰ª âˆ’â€–wâ€– â†’ likely always z<0                | âŒ No           | âŒ No           | Weight-only prior                                  |
| Positive-weight sum       | Geometry of reachable z>0 region | Î£max(wáµ¢,0)+b < 0 â†’ cannot fire if inputs â‰¥0 | âŒ No           | âŒ No           | Works only if prev layer ReLU                      |
| Redundancy (cosine sim)   | Duplicated neurons               | >0.99 â†’ redundant                           | âŒ No           | âŒ No           | Weight-space check                                 |

### Implementation and formula notes

- **Activation on-rate:** `on_rate = (a > 0).float().mean()`
- **Gradient flow (EMA):** track `ema(|âˆ‚L/âˆ‚a|)` or fraction of active gradients.
- **Fisher proxy:** maintain EMA of `(âˆ‚L/âˆ‚Î¸)Â²` during backprop to measure importance.
- **Weight-only heuristics:** `Î£max(w,0)+b`, bias negativity, and weight norm can be computed at epoch end.

---

## ğŸŸ¦ Smooth activations (tanh, sigmoid, GELU, Swish, etc.)

| **Measure**                   | **What it measures**          | **Interpretation**                      | **Needs data?**       | **Implemented**                      | **Notes**                          |
| ----------------------------- | ----------------------------- | --------------------------------------- | --------------------- | ------------------------------------ | ---------------------------------- | ----- | ------------------- |
| Pre-activation magnitude      | How deep neurons sit in tails | High â†’ saturated                        | âœ… Yes                | âŒ No                                | Direct, interpretable              |
| Derivative magnitude          | Local slope                   | Small â†’ saturated, vanishing grad       | âœ… Yes                | âœ… Yes                               | Analogue of gradient flow for ReLU |
| Average gradient norm         | Effective gradient strength   | Low â†’ poor gradient flow                | âœ… Yes                | âŒ No                                | Core training-health metric        |
| Activation entropy / variance | Output diversity              | Low â†’ outputs constant â‡’ saturated      | âœ… Yes                | âŒ No                                | Easy to monitor                    |
| Gradient variance per layer   | Stability of backprop         | Collapse â†’ saturation or dead gradients | âœ… Yes                | âŒ No                                | Layer-level check                  |
| Fisher proxy                  | Parameter importance          | Low â†’ unused parameter                  | âœ… Yes                | âœ… Yes                               | Data-driven                        |
| Ablation sensitivity          | Functional importance         | Î”â‰ˆ0 â†’ redundant                         | âœ… Yes                | âŒ No                                | Confirms functional irrelevance    |
| Bias/weight ratio heuristic   | Static saturation risk        |                                         | b                     | / (â€–wâ€–+Îµ) > 2â€“3 â†’ likely saturated   | âŒ No                              | âŒ No | Weight-only prior   |
| Pre-activation prior          | Prob(                         | z                                       | >T) under z~N(b,â€–wâ€–Â²) | Large probability â†’ likely saturated | âŒ No                              | âŒ No | Rough offline prior |

### Implementation and formula notes

- **Derivative magnitude:** `mean(|f'(z)|)` where `f'(z)=1âˆ’tanhÂ²(z)` or `Ïƒ(z)(1âˆ’Ïƒ(z))`.
- **Average gradient norm:** `mean(|âˆ‚L/âˆ‚a|)` or `mean(|âˆ‚L/âˆ‚z|)` recorded during backprop.
- **Pre-activation magnitude:** `frac_saturated = (|z| > 2.5).mean()` (tanh) or `|z| > 5` (sigmoid).
- **Fisher proxy:** running EMA of squared gradients for parameters.
- **Bias/weight ratio heuristic:** offline diagnostic using `|b| / (â€–wâ€–+Îµ)`.

---

## ğŸ§­ Conceptual summary

| **Category**                  | **ReLU** (piecewise linear)          | **Smooth (tanh/sigmoid/GELU)**                     |
| ----------------------------- | ------------------------------------ | -------------------------------------------------- | ----- | ---------------- | --- | ------------- |
| Kind of inactivity            | _Structural_ (hard zero region)      | _Statistical_ (data-dependent saturation)          |
| Derivative behavior           | Exactly zero on one side             | Small but nonzero in tails                         |
| Can infer from weights alone? | Partially (geometry & bias)          | No â€” must use data                                 |
| Best metrics                  | On-rate, gradient flow, Fisher proxy |                                                    | fâ€²(z) | , gradient norm, | z   | tail fraction |
| Functional test               | Ablation sensitivity                 | Same                                               |
| Interpretation                | Permanently zero output, no gradient | Output nearly constant, tiny gradient, recoverable |

---

### âœ… TL;DR

- **ReLU:** Deadness is _binary and structural._ Diagnose via _activation on-rate_, _gradient flow_, or _Fisher proxy_;
  sometimes weights/biases suffice.
- **Smooth activations:** Saturation is _continuous and data-dependent._ Measure _derivative magnitude_, _average
  gradient norm_, or _activation variance_ **in place**.
- **Universal:** Functional tests (_ablation sensitivity_, _Fisher proxy_) work for both.
