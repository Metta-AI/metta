{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train Cogames\n",
        "\n",
        "this notebook will help you to get started with cogames with the simplest training example.\n",
        "\n",
        "Make sure you have selected T4 runtime!"
      ],
      "metadata": {
        "id": "cjidF68qkVqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "MeQylF-oknqM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eeiO_R7u_1-",
        "outputId": "37260422-d0fd-44da-fce3-43445328cfda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'metta'...\n",
            "remote: Enumerating objects: 246671, done.\u001b[K\n",
            "remote: Counting objects: 100% (2633/2633), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1151/1151), done.\u001b[K\n",
            "remote: Total 246671 (delta 1923), reused 1545 (delta 1459), pack-reused 244038 (from 3)\u001b[K\n",
            "Receiving objects: 100% (246671/246671), 499.71 MiB | 31.24 MiB/s, done.\n",
            "Resolving deltas: 100% (195728/195728), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Metta-AI/metta.git\n",
        "%cd metta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Bazel"
      ],
      "metadata": {
        "id": "TrLJ7-ozk0NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Update and install Bazel\n",
        "!sudo apt update && sudo apt install bazel -y\n",
        "!bazel --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yj4JbbVv6aA",
        "outputId": "da2e935d-b832-4f40-b24e-e4516ca48784"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Hit:2 https://storage.googleapis.com/bazel-apt stable InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,820 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,406 kB]\n",
            "Fetched 12.2 MB in 2s (5,620 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mhttps://storage.googleapis.com/bazel-apt/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "bazel is already the newest version (8.4.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n",
            "bazel 8.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Nim"
      ],
      "metadata": {
        "id": "pyJU9uEVk7M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y curl git build-essential\n",
        "\n",
        "#Install Nim via choosenim (official installer)\n",
        "!curl https://nim-lang.org/choosenim/init.sh -sSf | sh -s -- -y\n",
        "# Add Nim to PATH for this Colab session\n",
        "import os\n",
        "os.environ[\"PATH\"] += \":/root/.nimble/bin\"\n",
        "\n",
        "# Verify installation\n",
        "!nim --version\n",
        "!nimble --version\n"
      ],
      "metadata": {
        "id": "wIIstuLy0dGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a83f1c-d17c-4da8-ce27-a405de082209"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to security.ub\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Con\r                                                                               \rHit:3 https://storage.googleapis.com/bazel-apt stable InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rHit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: https://storage.googleapis.com/bazel-apt/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.21).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n",
            "choosenim-init: Downloading choosenim-0.8.16_linux_amd64\n",
            "\u001b[36m\u001b[1mDownloading \u001b[0m\u001b[0mNim 2.2.6 from nim-lang.org\n",
            "\u001b[2K\u001b[1G[                                                  ] 0.08587% 5kb/s\u001b[2K\u001b[1G[#############################                     ] 59.68% 39434kb/s\u001b[2K\u001b[1G[##################################################] 100.0% 0kb/s\n",
            "\u001b[36m\u001b[1m Extracting \u001b[0m\u001b[0mnim-2.2.6-linux_x64.tar.xz\n",
            "\u001b[36m\u001b[1m Extracting \u001b[0m\u001b[0mnim-2.2.6-linux_x64.tar\n",
            "\u001b[36m\u001b[1m   Building \u001b[0m\u001b[0mNim 2.2.6\n",
            "\u001b[36m\u001b[1m  Compiler: \u001b[0m\u001b[0mAlready built\n",
            "\u001b[36m\u001b[1m  Installed \u001b[0m\u001b[0mcomponent 'nim'\n",
            "\u001b[36m\u001b[1m  Installed \u001b[0m\u001b[0mcomponent 'nimble'\n",
            "\u001b[36m\u001b[1m  Installed \u001b[0m\u001b[0mcomponent 'nimgrep'\n",
            "\u001b[36m\u001b[1m  Installed \u001b[0m\u001b[0mcomponent 'nimpretty'\n",
            "\u001b[36m\u001b[1m  Installed \u001b[0m\u001b[0mcomponent 'nimsuggest'\n",
            "\u001b[36m\u001b[1m  Installed \u001b[0m\u001b[0mcomponent 'testament'\n",
            "\u001b[36m\u001b[1m  Installed \u001b[0m\u001b[0mcomponent 'nim-gdb'\n",
            "\u001b[32m\u001b[1m   Switched \u001b[0m\u001b[0mto Nim 2.2.6\n",
            "choosenim-init: ChooseNim installed in /root/.nimble/bin\n",
            "choosenim-init: You must now ensure that the Nimble bin dir is in your PATH.\n",
            "choosenim-init: Place the following line in the ~/.profile or ~/.bashrc file.\n",
            "choosenim-init:     export PATH=/root/.nimble/bin:$PATH\n",
            "Nim Compiler Version 2.2.6 [Linux: amd64]\n",
            "Compiled at 2025-10-31\n",
            "Copyright (c) 2006-2025 by Andreas Rumpf\n",
            "\n",
            "git hash: ab00c56904e3126ad826bb520d243513a139436a\n",
            "active boot switches: -d:release\n",
            "nimble v0.20.1 compiled at 2025-10-31 02:15:47\n",
            "git hash: couldn't determine git hash\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install all the required packages"
      ],
      "metadata": {
        "id": "Ic3PnnxKlASi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install .\n",
        "!uv pip install git+https://github.com/PufferAI/PufferLib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XXCUuCF6y0A",
        "outputId": "840b515a-9ee4-40fa-8f04-f80e626530ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m286 packages\u001b[0m \u001b[2min 1.00s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 766ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m4 packages\u001b[0m \u001b[2min 12ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m4 packages\u001b[0m \u001b[2min 11ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgymnasium\u001b[0m\u001b[2m==0.29.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgymnasium\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mheavyball\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mheavyball\u001b[0m\u001b[2m==2.1.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mmetta\u001b[0m\u001b[2m==0.1 (from file:///content/metta)\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mpettingzoo\u001b[0m\u001b[2m==1.24.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpettingzoo\u001b[0m\u001b[2m==1.25.0\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ignore errors and warning in this.\n",
        "!pip install numpy --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fioeeuDPzZ4B",
        "outputId": "85e337cd-0374-4735-9d20-2ba831d93251"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Code + PPO Loss"
      ],
      "metadata": {
        "id": "DhIusBoTlUUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It includes a complete reinforcement learning training loop that implements a rollout buffer for generalized advantage estimation (GAE), a PPO update function with clipping, entropy regularization, and value loss computation, and supports both simple and LSTM-based policies. It interacts with the MettaGridEnv simulation, collecting observations, actions, and rewards over multiple epochs to optimize the policy network.\n",
        "\n",
        "You can modify mission configurations, hyperparameters, and policy types to experiment with different training setups."
      ],
      "metadata": {
        "id": "Su4IUPJOz3tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from mettagrid import MettaGridEnv\n",
        "import torch\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "dHVR0UQw0R3r"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cogames_mission(mission_name=\"training_facility.harvest\", variants=None):\n",
        "    from cogames.cli.mission import get_mission\n",
        "    _, config, _ = get_mission(mission_name, variants_arg=variants)\n",
        "    return config\n",
        "\n"
      ],
      "metadata": {
        "id": "IgV2JmRz0cHh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rollout Buffer Class which stores all the Expereince\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self, device, gamma, gae_lambda):\n",
        "        self.device = device\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.clear()\n",
        "\n",
        "    def add(self, obs, action, reward, done, log_prob, value):\n",
        "        self.observations.append(torch.as_tensor(obs, device=self.device, dtype=torch.float32).clone())\n",
        "        self.actions.append(torch.as_tensor(action, device=self.device, dtype=torch.long).clone())\n",
        "        self.rewards.append(torch.as_tensor(reward, device=self.device, dtype=torch.float32).clone())\n",
        "        self.dones.append(torch.as_tensor(done, device=self.device, dtype=torch.float32).clone())\n",
        "        self.log_probs.append(torch.as_tensor(log_prob, device=self.device, dtype=torch.float32).clone())\n",
        "        self.values.append(torch.as_tensor(value, device=self.device, dtype=torch.float32).clone())\n",
        "\n",
        "    def build_training_batch(self, last_value, last_done):\n",
        "        if not self.observations:\n",
        "            raise ValueError(\"RolloutBuffer is empty\")\n",
        "\n",
        "        obs = torch.stack(self.observations)\n",
        "        actions = torch.stack(self.actions)\n",
        "        rewards = torch.stack(self.rewards)\n",
        "        dones = torch.stack(self.dones)\n",
        "        old_log_probs = torch.stack(self.log_probs)\n",
        "        old_values = torch.stack(self.values)\n",
        "\n",
        "        values = old_values.view(old_values.shape[0], -1)\n",
        "        rewards = rewards.view(rewards.shape[0], -1)\n",
        "        dones = dones.view(dones.shape[0], -1)\n",
        "\n",
        "        last_value = torch.as_tensor(last_value, device=self.device, dtype=torch.float32).view(-1)\n",
        "        last_done = torch.as_tensor(last_done, device=self.device, dtype=torch.float32).view(-1)\n",
        "\n",
        "        advantages = torch.zeros_like(values)\n",
        "        next_advantage = torch.zeros_like(last_value)\n",
        "        next_value = last_value\n",
        "        next_nonterminal = 1.0 - last_done\n",
        "\n",
        "        for step in reversed(range(values.shape[0])):\n",
        "            reward = rewards[step]\n",
        "            value = values[step]\n",
        "            done = dones[step]\n",
        "            delta = reward + self.gamma * next_value * next_nonterminal - value\n",
        "            next_advantage = delta + self.gamma * self.gae_lambda * next_nonterminal * next_advantage\n",
        "            advantages[step] = next_advantage\n",
        "            next_value = value\n",
        "            next_nonterminal = 1.0 - done\n",
        "\n",
        "        returns = advantages + values\n",
        "\n",
        "        batch = {\n",
        "            \"obs\": obs,\n",
        "            \"actions\": actions,\n",
        "            \"old_log_probs\": old_log_probs,\n",
        "            \"old_values\": values,\n",
        "            \"advantages\": advantages,\n",
        "            \"returns\": returns,\n",
        "        }\n",
        "\n",
        "        return batch\n",
        "\n",
        "    def clear(self):\n",
        "        self.observations = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.log_probs = []\n",
        "        self.values = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.observations)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J96sgkQb0qSu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function give the action\n",
        "\n",
        "def get_action(policy, obs, device, lstm_state=None):\n",
        "    obs_tensor = torch.from_numpy(obs).float().unsqueeze(0).to(device)\n",
        "\n",
        "    state_argument = None\n",
        "    if policy.is_recurrent():\n",
        "        if lstm_state is None:\n",
        "            state_argument = {\"lstm_h\": None, \"lstm_c\": None}\n",
        "        else:\n",
        "            h, c = lstm_state\n",
        "            state_argument = {\"lstm_h\": h, \"lstm_c\": c}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        policy.network().eval()\n",
        "        if policy.is_recurrent():\n",
        "            logits, values = policy.network().forward_eval(obs_tensor, state_argument)\n",
        "            state_h, state_c = state_argument.get(\"lstm_h\"), state_argument.get(\"lstm_c\")\n",
        "            if state_h is not None and state_c is not None:\n",
        "                new_state = (state_h.detach(), state_c.detach())\n",
        "            else:\n",
        "                new_state = None\n",
        "        else:\n",
        "            logits, values = policy.network().forward_eval(obs_tensor)\n",
        "            new_state = None\n",
        "\n",
        "    dist = Categorical(logits=logits)\n",
        "    actions = dist.sample()\n",
        "    log_probs = dist.log_prob(actions)\n",
        "\n",
        "    if actions.dim() == 0:\n",
        "        actions = actions.unsqueeze(0)\n",
        "        log_probs = log_probs.unsqueeze(0)\n",
        "\n",
        "    return actions.cpu().numpy(), log_probs.detach(), values.detach(), new_state"
      ],
      "metadata": {
        "id": "VJr7Y4Cb0w9L"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PPO Update function\n",
        "\n",
        "\n",
        "def ppo_update(\n",
        "    policy,\n",
        "    optimizer,\n",
        "    batch,\n",
        "    device,\n",
        "    clip_coef,\n",
        "    vf_clip_coef,\n",
        "    vf_coef,\n",
        "    ent_coef,\n",
        "    max_grad_norm,\n",
        "    ppo_epochs,\n",
        "    minibatch_size,\n",
        "):\n",
        "    obs = batch[\"obs\"].to(device)\n",
        "    actions = batch[\"actions\"].to(device)\n",
        "    old_log_probs = batch[\"old_log_probs\"].to(device)\n",
        "    old_values = batch[\"old_values\"].to(device)\n",
        "    advantages = batch[\"advantages\"].to(device)\n",
        "    returns = batch[\"returns\"].to(device)\n",
        "\n",
        "    if advantages.numel() == 0:\n",
        "        raise ValueError(\"Advantages tensor is empty\")\n",
        "\n",
        "    policy.network().train()\n",
        "\n",
        "    advantages = advantages - advantages.mean()\n",
        "    advantages = advantages / (advantages.std(unbiased=False) + 1e-8)\n",
        "\n",
        "    returns = advantages + old_values\n",
        "\n",
        "    batch_size = obs.shape[0]\n",
        "\n",
        "    policy_losses = []\n",
        "    value_losses = []\n",
        "    entropies = []\n",
        "    clip_fractions = []\n",
        "    approx_kls = []\n",
        "\n",
        "    for _ in range(ppo_epochs):\n",
        "        indices = torch.randperm(batch_size, device=device)\n",
        "        for start in range(0, batch_size, minibatch_size):\n",
        "            mb_idx = indices[start : start + minibatch_size]\n",
        "            mb_obs = obs[mb_idx]\n",
        "            mb_actions = actions[mb_idx]\n",
        "            mb_old_log_probs = old_log_probs[mb_idx]\n",
        "            mb_old_values = old_values[mb_idx]\n",
        "            mb_advantages = advantages[mb_idx]\n",
        "            mb_returns = returns[mb_idx]\n",
        "\n",
        "            # Flatten agent dimension so PPO treats each agent-step as one sample\n",
        "            mb_actions = mb_actions.reshape(mb_actions.shape[0], -1)\n",
        "            mb_old_log_probs = mb_old_log_probs.reshape(mb_old_log_probs.shape[0], -1)\n",
        "            mb_old_values = mb_old_values.reshape(mb_old_values.shape[0], -1)\n",
        "            mb_advantages = mb_advantages.reshape(mb_advantages.shape[0], -1)\n",
        "            mb_returns = mb_returns.reshape(mb_returns.shape[0], -1)\n",
        "\n",
        "            if policy.is_recurrent():\n",
        "                logits, values = policy.network().forward_eval(mb_obs, None)\n",
        "            else:\n",
        "                logits, values = policy.network().forward_eval(mb_obs)\n",
        "\n",
        "            dist = Categorical(logits=logits)\n",
        "            new_log_probs = dist.log_prob(mb_actions.squeeze(-1) if mb_actions.shape[-1] == 1 else mb_actions)\n",
        "            entropy = dist.entropy()\n",
        "\n",
        "            new_log_probs = new_log_probs.unsqueeze(-1) if new_log_probs.dim() == 1 else new_log_probs\n",
        "            log_ratio = new_log_probs - mb_old_log_probs\n",
        "            ratio = log_ratio.exp()\n",
        "            surr1 = ratio * mb_advantages\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_coef, 1.0 + clip_coef) * mb_advantages\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "            value_pred = values.reshape(values.shape[0], -1)\n",
        "            value_pred_clipped = mb_old_values + (value_pred - mb_old_values).clamp(-vf_clip_coef, vf_clip_coef)\n",
        "            value_loss_unclipped = (value_pred - mb_returns) ** 2\n",
        "            value_loss_clipped = (value_pred_clipped - mb_returns) ** 2\n",
        "            value_loss = 0.5 * torch.max(value_loss_unclipped, value_loss_clipped).mean()\n",
        "\n",
        "            kld = 0.5 * log_ratio.pow(2).mean()\n",
        "            clip_fraction = (torch.abs(ratio - 1.0) > clip_coef).float().mean()\n",
        "\n",
        "            loss = policy_loss + vf_coef * value_loss - ent_coef * entropy.mean()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(policy.network().parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            policy_losses.append(policy_loss.detach())\n",
        "            value_losses.append(value_loss.detach())\n",
        "            entropies.append(entropy.mean().detach())\n",
        "            clip_fractions.append(clip_fraction.detach())\n",
        "            approx_kls.append(kld.detach())\n",
        "\n",
        "    metrics = {\n",
        "        \"policy_loss\": torch.stack(policy_losses).mean().item(),\n",
        "        \"value_loss\": torch.stack(value_losses).mean().item(),\n",
        "        \"entropy\": torch.stack(entropies).mean().item(),\n",
        "        \"clip_fraction\": torch.stack(clip_fractions).mean().item(),\n",
        "        \"approx_kl\": torch.stack(approx_kls).mean().item(),\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "tgKK6rmx05S9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Training loop\n",
        "\n",
        "def train_cogames(\n",
        "    mission_name=\"training_facility.harvest\",\n",
        "    policy_type=\"simple\",\n",
        "    num_epochs=10,\n",
        "    batch_size=256,\n",
        "    minibatch_size=64,\n",
        "    ppo_epochs=4,\n",
        "    learning_rate=3e-4,\n",
        "    # Metta-tuned hyperparameters from PPOConfig\n",
        "    gamma=0.977,\n",
        "    gae_lambda=0.891477,\n",
        "    clip_coef=0.264407,\n",
        "    vf_clip_coef=0.1,\n",
        "    vf_coef=0.897619,\n",
        "    ent_coef=0.01,\n",
        "    max_grad_norm=0.5,\n",
        "    save_path=\"cogames_policy.pt\",\n",
        "    device=\"cpu\",\n",
        "    variants=None,\n",
        "    seed=42,\n",
        "):\n",
        "    if batch_size <= 0:\n",
        "        raise ValueError(\"batch_size must be positive\")\n",
        "    if minibatch_size <= 0:\n",
        "        raise ValueError(\"minibatch_size must be positive\")\n",
        "    if minibatch_size > batch_size:\n",
        "        raise ValueError(\"minibatch_size must be <= batch_size\")\n",
        "\n",
        "    device = torch.device(device)\n",
        "    config = get_cogames_mission(mission_name, variants=variants)\n",
        "    env = MettaGridEnv(env_cfg=config)\n",
        "    obs, _ = env.reset(seed=seed)\n",
        "\n",
        "    if policy_type == \"simple\":\n",
        "        from cogames.policy.simple import SimplePolicy\n",
        "\n",
        "        policy = SimplePolicy(env, device)\n",
        "    elif policy_type == \"lstm\":\n",
        "        from cogames.policy.lstm import LSTMPolicy\n",
        "\n",
        "        policy = LSTMPolicy(env, device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown policy type: {policy_type}\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(policy.network().parameters(), lr=learning_rate)\n",
        "    best_reward = -float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        buffer = RolloutBuffer(device=device, gamma=gamma, gae_lambda=gae_lambda)\n",
        "        obs, _ = env.reset(seed=seed + epoch)\n",
        "        epoch_reward = 0.0\n",
        "        hearts_collected = 0.0\n",
        "        lstm_state = None\n",
        "        steps_collected = 0\n",
        "        last_done_flags = np.zeros(getattr(env, \"num_agents\", 1), dtype=np.float32)\n",
        "\n",
        "        while len(buffer) < batch_size:\n",
        "            actions_np, log_probs, values, next_state = get_action(policy, obs, device, lstm_state)\n",
        "            next_obs, rewards, terminals, truncations, _ = env.step(actions_np)\n",
        "\n",
        "            dones = np.logical_or(terminals, truncations)\n",
        "            buffer.add(obs, actions_np, rewards, dones, log_probs, values)\n",
        "\n",
        "            step_reward = float(np.sum(rewards))\n",
        "            epoch_reward += step_reward\n",
        "            if step_reward > 0:\n",
        "                hearts_collected += step_reward\n",
        "\n",
        "            last_done_flags = np.asarray(dones, dtype=np.float32).copy()\n",
        "\n",
        "            if np.any(dones):\n",
        "                next_obs, _ = env.reset()\n",
        "                lstm_state = None\n",
        "            else:\n",
        "                lstm_state = next_state\n",
        "\n",
        "            obs = next_obs\n",
        "            steps_collected += 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            obs_tensor = torch.from_numpy(obs).float().unsqueeze(0).to(device)\n",
        "            if policy.is_recurrent():\n",
        "                state_arg = None\n",
        "                if lstm_state is not None:\n",
        "                    h, c = lstm_state\n",
        "                    state_arg = {\"lstm_h\": h, \"lstm_c\": c}\n",
        "                _, last_value_tensor = policy.network().forward_eval(obs_tensor, state_arg)\n",
        "            else:\n",
        "                _, last_value_tensor = policy.network().forward_eval(obs_tensor)\n",
        "\n",
        "        last_value = last_value_tensor.squeeze(0)\n",
        "        if last_value.dim() > 1:\n",
        "            last_value = last_value.squeeze(-1)\n",
        "        last_value = last_value.detach().cpu()\n",
        "\n",
        "        last_done_tensor = torch.as_tensor(last_done_flags, dtype=torch.float32)\n",
        "        if last_done_tensor.dim() == 0:\n",
        "            last_done_tensor = last_done_tensor.unsqueeze(0)\n",
        "\n",
        "        last_value = last_value * (1.0 - last_done_tensor)\n",
        "\n",
        "        batch = buffer.build_training_batch(last_value=last_value, last_done=last_done_tensor)\n",
        "\n",
        "        metrics = ppo_update(\n",
        "            policy=policy,\n",
        "            optimizer=optimizer,\n",
        "            batch=batch,\n",
        "            device=device,\n",
        "            clip_coef=clip_coef,\n",
        "            vf_clip_coef=vf_clip_coef,\n",
        "            vf_coef=vf_coef,\n",
        "            ent_coef=ent_coef,\n",
        "            max_grad_norm=max_grad_norm,\n",
        "            ppo_epochs=ppo_epochs,\n",
        "            minibatch_size=minibatch_size,\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}/{num_epochs} - Reward: {epoch_reward:.2f}, \"\n",
        "            f\"Policy loss: {metrics['policy_loss']:.4f}, Value loss: {metrics['value_loss']:.4f}\"\n",
        "        )\n",
        "\n",
        "        if epoch_reward > best_reward:\n",
        "            best_reward = epoch_reward\n",
        "            policy.save_policy_data(save_path)\n",
        "\n",
        "    env.close()\n"
      ],
      "metadata": {
        "id": "JiG5E2GcBnZ-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Policy"
      ],
      "metadata": {
        "id": "SKgx5Wt9lXyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_cogames(\n",
        "    mission_name=\"training_facility.harvest\",\n",
        "    policy_type=\"simple\", # or use lstm\n",
        "    num_epochs=50000,\n",
        "    batch_size=2048,\n",
        "    minibatch_size=512,\n",
        "    ppo_epochs=4,\n",
        "    learning_rate=3e-4,\n",
        "    save_path=\"cogames_policy.pt\",\n",
        "    variants=[\"neutral_faced\"], # Available variants: mined_out, dark_side, super_charged, rough_terrain, solar_flare, desert, forest, city, caves, store_base, extractor_base, both_base, lonely_heart, pack_rat, energized, neutral_faced\n",
        "    device=\"cuda\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "PHXUnV_cz6F2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b804917f-468d-4341-8de7-c1a3c10fb2ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50000 - Reward: 0.00, Policy loss: -0.0170, Value loss: 0.4780\n",
            "Epoch 2/50000 - Reward: 2.00, Policy loss: -0.0103, Value loss: 0.4743\n",
            "Epoch 3/50000 - Reward: 2.00, Policy loss: -0.0253, Value loss: 0.4733\n",
            "Epoch 4/50000 - Reward: 0.00, Policy loss: 0.0074, Value loss: 0.4599\n",
            "Epoch 5/50000 - Reward: 2.00, Policy loss: -0.0132, Value loss: 0.4835\n",
            "Epoch 6/50000 - Reward: 0.00, Policy loss: -0.0035, Value loss: 0.4949\n",
            "Epoch 7/50000 - Reward: 0.00, Policy loss: -0.0043, Value loss: 0.4693\n",
            "Epoch 8/50000 - Reward: 0.00, Policy loss: -0.0081, Value loss: 0.4688\n",
            "Epoch 9/50000 - Reward: 4.00, Policy loss: -0.0063, Value loss: 0.4885\n",
            "Epoch 10/50000 - Reward: 8.00, Policy loss: -0.0072, Value loss: 0.4879\n",
            "Epoch 11/50000 - Reward: 0.00, Policy loss: -0.0286, Value loss: 0.4649\n",
            "Epoch 12/50000 - Reward: 0.00, Policy loss: 0.0287, Value loss: 0.4587\n",
            "Epoch 13/50000 - Reward: 0.00, Policy loss: -0.0094, Value loss: 0.4945\n",
            "Epoch 14/50000 - Reward: 3.00, Policy loss: -0.0071, Value loss: 0.4958\n",
            "Epoch 15/50000 - Reward: 0.00, Policy loss: -0.0216, Value loss: 0.4589\n",
            "Epoch 16/50000 - Reward: 0.00, Policy loss: -0.0154, Value loss: 0.4624\n",
            "Epoch 17/50000 - Reward: 0.00, Policy loss: -0.0163, Value loss: 0.4722\n",
            "Epoch 18/50000 - Reward: 5.00, Policy loss: -0.0096, Value loss: 0.4919\n",
            "Epoch 19/50000 - Reward: 0.00, Policy loss: -0.0256, Value loss: 0.4661\n",
            "Epoch 20/50000 - Reward: 0.00, Policy loss: -0.0016, Value loss: 0.4666\n",
            "Epoch 21/50000 - Reward: 0.00, Policy loss: -0.0002, Value loss: 0.4669\n",
            "Epoch 22/50000 - Reward: 0.00, Policy loss: -0.0327, Value loss: 0.4574\n",
            "Epoch 23/50000 - Reward: 1.00, Policy loss: -0.0046, Value loss: 0.4862\n",
            "Epoch 24/50000 - Reward: 0.00, Policy loss: -0.0203, Value loss: 0.4748\n",
            "Epoch 25/50000 - Reward: 0.00, Policy loss: 0.0018, Value loss: 0.4671\n",
            "Epoch 26/50000 - Reward: 0.00, Policy loss: -0.0032, Value loss: 0.4806\n",
            "Epoch 27/50000 - Reward: 0.00, Policy loss: -0.0088, Value loss: 0.4806\n",
            "Epoch 28/50000 - Reward: 2.00, Policy loss: -0.0151, Value loss: 0.4951\n",
            "Epoch 29/50000 - Reward: 0.00, Policy loss: -0.0132, Value loss: 0.4807\n",
            "Epoch 30/50000 - Reward: 0.00, Policy loss: 0.0030, Value loss: 0.4798\n",
            "Epoch 31/50000 - Reward: 0.00, Policy loss: -0.0043, Value loss: 0.4735\n",
            "Epoch 32/50000 - Reward: 0.00, Policy loss: -0.0148, Value loss: 0.4812\n",
            "Epoch 33/50000 - Reward: 0.00, Policy loss: -0.0071, Value loss: 0.4748\n",
            "Epoch 34/50000 - Reward: 0.00, Policy loss: -0.0059, Value loss: 0.4814\n",
            "Epoch 35/50000 - Reward: 0.00, Policy loss: 0.0011, Value loss: 0.4879\n",
            "Epoch 36/50000 - Reward: 0.00, Policy loss: -0.0162, Value loss: 0.4918\n",
            "Epoch 37/50000 - Reward: 0.00, Policy loss: 0.0177, Value loss: 0.4849\n",
            "Epoch 38/50000 - Reward: 0.00, Policy loss: -0.0012, Value loss: 0.4907\n",
            "Epoch 39/50000 - Reward: 0.00, Policy loss: -0.0066, Value loss: 0.4969\n",
            "Epoch 40/50000 - Reward: 0.00, Policy loss: -0.0118, Value loss: 0.4871\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1031837785.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_cogames(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmission_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training_facility.harvest\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpolicy_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"simple\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1950678826.py\u001b[0m in \u001b[0;36mtrain_cogames\u001b[0;34m(mission_name, policy_type, num_epochs, batch_size, minibatch_size, ppo_epochs, learning_rate, gamma, gae_lambda, clip_coef, vf_clip_coef, vf_coef, ent_coef, max_grad_norm, save_path, device, variants, seed)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mactions_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1950678826.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(policy, obs, device, lstm_state)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "56JNCb5HAD9T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}