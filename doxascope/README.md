# Doxascope

The Doxascope is a tool for investigating the internal states of a reinforcement learning agent. It uses a neural network that takes an agent's LSTM memory vectors as input to predict the agent's relative past and future positions on the grid. This allows us to probe what an agent "knows" or "expects" about its own trajectory.

This directory contains scripts for data logging, preprocessing, training, and analysis of the Doxascope network.

## Core Workflow

The Doxascope is designed for an iterative workflow where you can collect data, train models, and compare their performance as you gather more data.

1.  **Collect Data**: Run standard evaluation simulations. The Doxascope logger, when enabled, will automatically save agent memory and position data from these runs.
2.  **Train a Model**: Use `doxascope_train.py` to train a network on all currently available raw data. This creates a unique, timestamped "run" directory for the results.
3.  **Analyze a Run**: Use `doxascope_analysis.py analyze` to generate detailed plots for a specific training run.
4.  **Compare Runs**: Use `doxascope_analysis.py compare` to generate a single plot that overlays the accuracy curves from all training runs for a policy, making it easy to see how performance changes over time.

## Directory Structure

All data generated by the Doxascope is stored in the `train_dir/doxascope/` directory, which is ignored by Git to prevent large data files from being committed to the repository.

-   `train_dir/doxascope/raw_data/<policy_name>/`: Contains raw JSON logs from simulation runs. Each simulation generates a unique file.
-   `train_dir/doxascope/results/<policy_name>/<run_name>/`: Contains all artifacts from a single training run, including the trained model, preprocessed data cache, and analysis plots.
-   `train_dir/doxascope/sweeps/<policy_name>/<sweep_name>/`: Contains the results from a hyperparameter sweep.

## Usage

### 1. Data Collection

To enable data collection, add the `doxascope` configuration to your user config file (e.g., `configs/user/your_name.yaml`):

```yaml
sim_job:
  simulation_suite:
    doxascope:
      enabled: true
```

When you run an evaluation, the logger will save the raw data to `train_dir/doxascope/raw_data/<policy_name>/`. You can run the evaluation multiple times to accumulate more data.

### 2. Training the Doxascope Network

Once you have collected data, train a network using `doxascope_train.py`.

**Usage:**

```bash
python -m doxascope.doxascope_train <policy_name> [options]
```

**Key Options:**

-   `--num-future-timesteps <n>`: Number of future positions to predict (default: 1).
-   `--num-past-timesteps <n>`: Number of past positions to predict (default: 0).
-   `--run-name <name>`: A unique name for this training run. If not provided, a timestamp will be used.
-   `--train-random-baseline` / `--no-train-random-baseline`: Enable/disable training of baseline model with random memory vectors (default: enabled).

**Additional Options:**

**Data & Training:**
-   `--raw-data-dir <path>`: Directory containing raw data files (default: `train_dir/doxascope/raw_data`).
-   `--output-dir <path>`: Directory to save results (default: `train_dir/doxascope/results`).
-   `--test-split <ratio>`: Proportion of data for testing (default: 0.15).
-   `--val-split <ratio>`: Proportion of data for validation (default: 0.15).
-   `--batch-size <n>`: Batch size for training (default: 32).
-   `--learning-rate <lr>`: Learning rate for optimizer (default: 0.001).
-   `--num-epochs <n>`: Maximum number of training epochs (default: 100).
-   `--patience <n>`: Early stopping patience (default: 10).
-   `--device <device>`: Training device: 'cpu', 'cuda', or 'auto' (default: 'auto').

**Model Architecture:**
-   `--hidden_dim <n>`: Hidden layer dimension (default: 512).
-   `--dropout_rate <rate>`: Dropout rate (default: 0.4).
-   `--activation_fn <fn>`: Activation function: 'relu', 'silu', or 'gelu' (default: 'silu').
-   `--main_net_depth <n>`: Depth of main network (default: 3).
-   `--processor_depth <n>`: Depth of state processors (default: 1).

The script will create a new run directory (e.g., `.../results/my_policy/20231027-153000/`) and save all artifacts there.

**Note on Data Splitting:** The system uses file-size-based splitting to balance label distributions across train/validation/test sets while maintaining strict file-level separation to prevent data leakage. This ensures that samples from the same agent trajectory don't appear in multiple splits.

### 3. Analysis

The `doxascope_analysis.py` script provides tools for analyzing and comparing training runs.

#### Analyzing a Single Run

To generate all standard plots for a specific run:

**Usage:**

```bash
python -m doxascope.doxascope_analysis analyze <policy_name> <run_name>
```

This will create an `analysis` subdirectory within the run directory containing plots like `multistep_accuracy.png` and `training_history.png`. If a baseline model was trained, a `multistep_accuracy_comparison.png` will also be generated.

#### Comparing Runs and Policies

The `compare` command has two modes depending on how many policy names you provide.

**1. Compare all training runs for a *single* policy:**

This is useful for seeing how performance improves as more training data is collected over time.

**Usage:**

```bash
python -m doxascope.doxascope_analysis compare <policy_name>
```

This will generate a `comparison_<policy_name>.png` plot in the policy's main results directory, overlaying the accuracy curves from each individual training run.

**2. Compare the latest run of *multiple* policies:**

This is useful for seeing how different policies perform against each other, using the most up-to-date model for each.

**Usage:**

```bash
python -m doxascope.doxascope_analysis compare <policy_1> <policy_2> ...
```

This will find the most recent training run for each policy listed and generate a single plot comparing their accuracy curves.

### 4. Hyperparameter Sweep

To find the best hyperparameters or network architecture, use the `doxascope_sweep.py` script.

**Usage:**

```bash
python -m doxascope.doxascope_sweep <policy_name> [options]
```

**Key Options:**

-   `--num-configs <n>`: Number of random configurations to test (default: 30).
-   `--sweep-type <type>`: Type of sweep: `hyper` (for hyperparameters) or `arch` (for architecture).

The script will save all sweep results to a unique, timestamped directory under `train_dir/doxascope/sweeps/`.
