# Single-Node Multi-GPU Learning Progress Training Configuration
# Run with: sky launch configs/sweep/learning_progress_multi_gpu.yaml

name: learning-progress-multi-gpu

resources:
  # Single node with multiple GPUs
  accelerators: L4:4  # 4 L4 GPUs on one node (minimum available)
  # Alternative configurations:
  # accelerators: L4:8  # 8 L4 GPUs
  # accelerators: A100:2  # 2 A100 GPUs
  # accelerators: {L4:1, A100:1}  # Mixed GPU types

setup: |
  # Setup commands
  echo "Setting up multi-GPU learning progress training environment..."

  # Install dependencies
  pip install -r requirements.txt

  # Setup distributed environment variables
  export WORLD_SIZE=4  # Number of GPUs
  export MASTER_ADDR=localhost
  export MASTER_PORT=29500

run: |
  # Main training script
  echo "Starting learning progress training with $WORLD_SIZE GPUs..."

  # Run distributed training with PyTorch DDP
  python -m torch.distributed.launch \
    --nproc_per_node=$WORLD_SIZE \
    --master_port=$MASTER_PORT \
    experiments/user/arena_lp_test.py \
    --distributed \
    --num_episodes=2000 \
    --learning_progress=True \
    --wandb_project=metta \
    --wandb_run_name=msb_lpdehyd_multi_gpu
