resources:
  cloud: aws
  any_of:
    - region: us-east-1
      use_spot: true
      accelerators: "L4:1"
    - region: us-east-1
      use_spot: true
      accelerators: "A10G:1"
    - region: us-east-1
      accelerators: "L4:1"
    - region: us-east-1
      accelerators: "A10G:1"
    - region: us-west-2
      use_spot: true
      accelerators: "L4:1"
    - region: us-west-2
      use_spot: true
      accelerators: "A10G:1"
    - region: us-west-2
      accelerators: "L4:1"
    - region: us-west-2
      accelerators: "A10G:1"
  cpus: 8+
  image_id: docker:metta:latest
  job_recovery:
    strategy: EAGER_NEXT_REGION
    max_restarts_on_errors: 20

config:
  docker:
    run_options:
      - --cap-add=IPC_LOCK
      - --ulimit memlock=-1:-1
      - --ipc=host
      - --userns=host

file_mounts:
  /mnt/s3/softmax-public:
    source: s3://softmax-public
    mode: MOUNT_CACHED
  /mnt/s3/train_dir:
    source: s3://softmax-train-dir
    mode: MOUNT_CACHED

setup: |
  set -e
  cd /workspace/metta

  echo "[SETUP] Fetching latest from origin..."
  git fetch origin "$METTA_GIT_REF" || git fetch --depth=1000 origin
  git checkout "$METTA_GIT_REF"
  echo "[SETUP] Checked out: $(git rev-parse HEAD)"

  # Note that different sets of skypilot environment variables are available in "run" vs "setup"
  # see https://docs.skypilot.co/en/latest/running-jobs/environment-variables.html

run: |
  set -euo pipefail

  cd /workspace/metta

  # Drop any preloaded venv; activate your own
  if [ -n "${VIRTUAL_ENV:-}" ]; then
    deactivate 2>/dev/null || true
  fi
  . .venv/bin/activate

  # Python environment setup
  echo "[SETUP] Setting up Python environment..."
  uv sync

  # Create required directories
  mkdir -p "$WANDB_DIR"

  bash ./devops/skypilot/config/configure_environment.sh

  echo "[RUN] Starting run phase..."
  echo "[RUN] METTA_RUN_ID: ${METTA_RUN_ID:-}"
  echo "[RUN] SKYPILOT_TASK_ID: ${SKYPILOT_TASK_ID:-}"

  # Optional metrics (best-effort)
  if [ -f common/src/metta/common/util/skypilot_latency.py ]; then
    echo "[RUN] Collecting skypilot latency..."
    uv run python common/src/metta/common/util/skypilot_latency.py || true
  else
    echo "[RUN] Latency script is missing!"
  fi

  METTA_ENV_FILE="$(uv run ./common/src/metta/common/util/constants.py METTA_ENV_FILE)"

  if [ -f common/src/metta/common/util/cost_monitor.py ]; then
    echo "[RUN] Collecting instance cost..."
    METTA_HOURLY_COST="$(uv run python common/src/metta/common/util/cost_monitor.py 2>/dev/null | tail -1 || true)"
    if [ -n "${METTA_HOURLY_COST:-}" ]; then
      echo "[RUN] METTA_HOURLY_COST set to: $METTA_HOURLY_COST in $METTA_ENV_FILE"
    else
      echo "[RUN] Cost monitor script failed to run or returned no value."
    fi
  else
    echo "[RUN] Cost monitor script is missing!"
  fi

  # load any environment vars written by child scripts
  source "$METTA_ENV_FILE"

  # Run comprehensive GPU diagnostics and NCCL tests
  echo "[RUN] Running GPU diagnostics and NCCL tests..."
  if ! uv run python ./devops/skypilot/test_nccl.py; then
      echo "Pre-flight check failed!"
      exit 1
  fi

  export HEARTBEAT_FILE=${HEARTBEAT_FILE:-$WANDB_DIR/heartbeat.txt}
  echo "[DEBUG] WANDB_DIR: ${WANDB_DIR:-'NOT SET'}"
  echo "[DEBUG] HEARTBEAT_FILE: ${HEARTBEAT_FILE:-'NOT SET'}"
  echo "[DEBUG] HEARTBEAT_TIMEOUT: ${HEARTBEAT_TIMEOUT:-'NOT SET'}"

  # Start train.sh in background
  ./devops/"$METTA_CMD".sh run="$METTA_RUN_ID" $METTA_CMD_ARGS &
  TRAIN_PID=$!

  # Start heartbeat *watching the train PID*
  if [[ "${HEARTBEAT_TIMEOUT}" != "0" ]]; then
    echo "[INFO] Starting heartbeat monitor ${HEARTBEAT_TIMEOUT}s on $HEARTBEAT_FILE (pid=$TRAIN_PID)"
    uv run python -m metta.common.util.heartbeat monitor "$HEARTBEAT_FILE" \
      --pid "$TRAIN_PID" --timeout "$HEARTBEAT_TIMEOUT" \
      >/dev/null 2>&1 &
    HEARTBEAT_PID=$!
  fi

  # Wait for training to finish, capture exit, then kill heartbeat cleanly
  wait "$TRAIN_PID"
  CMD_EXIT=$?

  if [[ -n "${HEARTBEAT_PID:-}" ]]; then
    kill "$HEARTBEAT_PID" 2>/dev/null || true
    wait "$HEARTBEAT_PID" 2>/dev/null || true
  fi

  # if a github token is provided, report training status back to github
  export COMMIT_SHA=$(git rev-parse HEAD)
  GITHUB_REPOSITORY="$(uv run ./common/src/metta/common/util/constants.py METTA_GITHUB_REPO)"

  # Just export the variables and run the script
  if [ -n "${GITHUB_TOKEN:-}" ]; then
      echo "Posting status to GitHubâ€¦"

      # The script reads CMD_EXIT, COMMIT_SHA, and SKYPILOT_TASK_ID from environment
      uv run python ./devops/skypilot/post_commit_status.py
  else
      echo "GITHUB_TOKEN not set; skipping GitHub status update"
  fi

  echo "$METTA_CMD job complete. (Exit code: $CMD_EXIT)"
  exit $CMD_EXIT

envs:
  # status callback
  GITHUB_REPOSITORY: Metta-AI/metta
  GITHUB_STATUS_CONTEXT: ""
  GITHUB_STATUS_DESCRIPTION: ""
  GITHUB_STATUS_STATE: "" # success/failure/error/pending
  GITHUB_PAT: ""

  METTA_RUN_ID: ""
  METTA_CMD: train
  METTA_CMD_ARGS: ""
  METTA_GIT_REF: main
  WANDB_DIR: ./wandb
  HEARTBEAT_TIMEOUT: 600

  # if a github token is provided, we will report training status back to the branch
  GITHUB_TOKEN: ""

  # s3 mount slows down uv, so we put DATA_DIR outside of /workspace/metta
  DATA_DIR: /mnt/s3/train_dir
  SKYPILOT_DOCKER_USERNAME: ""
  SKYPILOT_DOCKER_PASSWORD: ""
  SKYPILOT_DOCKER_SERVER: 751442549699.dkr.ecr.us-east-1.amazonaws.com

secrets:
  # configured by launch script based on local credentials
  WANDB_PASSWORD: ""
  OBSERVATORY_TOKEN: ""
