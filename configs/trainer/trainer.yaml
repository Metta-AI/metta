num_workers: null
curriculum: ???
env_overrides: {}

initial_policy:
  uri: null
  type: top
  range: 1
  metric: epoch
  filters: {}

checkpoint:
  checkpoint_dir: ${run_dir}/checkpoints
  checkpoint_interval: 50
  wandb_checkpoint_interval: 50

simulation:
  evaluate_interval: 200
  replay_dir: s3://softmax-public/replays/${run}
  evaluate_remote: true
  evaluate_local: true
  git_hash: null
  skip_git_check: true # av revert to false

grad_mean_variance_interval: 0 # 0 to disable

total_timesteps: 10_000_000_000

batch_size: 524288
minibatch_size: 16384
bptt_horizon: 64
update_epochs: 1

zero_copy: true
require_contiguous_env_ids: false
verbose: true

cpu_offload: false
compile: false
compile_mode: reduce-overhead
profiler:
  interval_epochs: 10000
  profile_dir: s3://softmax-public/torch_traces/${run}

forward_pass_minibatch_target_size: 4096
async_factor: 2
scale_batches_by_world_size: false

# # Hyperparameter scheduler configuration
# hyperparameter_scheduler:
#   learning_rate_schedule:
#     _target_: metta.rl.hyperparameter_scheduler.CosineSchedule
#     min_value: 0.00003
#     initial_value: ${...optimizer.learning_rate}

#   ppo_clip_schedule:
#     _target_: metta.rl.hyperparameter_scheduler.LogarithmicSchedule
#     min_value: 0.05
#     decay_rate: 0.1
#     initial_value: ${...ppo.clip_coef}

#   ppo_ent_coef_schedule:
#     _target_: metta.rl.hyperparameter_scheduler.LinearSchedule
#     min_value: 0.0
#     initial_value: ${...ppo.ent_coef}

#   ppo_vf_clip_schedule:
#     _target_: metta.rl.hyperparameter_scheduler.LinearSchedule
#     min_value: 0.05
#     initial_value: ${...ppo.vf_clip_coef}

#   ppo_l2_reg_loss_schedule:
#     _target_: metta.rl.hyperparameter_scheduler.ConstantSchedule
#     initial_value: ${...ppo.l2_reg_loss_coef}

#   ppo_l2_init_loss_schedule:
#     _target_: metta.rl.hyperparameter_scheduler.ConstantSchedule
#     initial_value: ${...ppo.l2_init_loss_coef}

