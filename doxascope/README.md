# Doxascope

The Doxascope is a tool for investigating the internal states of a reinforcement learning agent. It uses a neural
network that takes an agent's LSTM memory vectors as input to predict the agent's relative past and future positions on
the grid. This allows us to probe what an agent "knows" or "expects" about its own trajectory.

This directory contains scripts for data logging, preprocessing, training, and analysis of the Doxascope network.

## Core Workflow

The Doxascope is designed for an iterative workflow where you can collect data, train models, and compare their
performance as you gather more data.

1.  **Collect Data**: Run standard evaluation simulations. The Doxascope logger, when enabled, will automatically save
    agent memory and position data from these runs.
2.  **Train a Model**: Use `python -m doxascope.cli train` to train a network on all currently available raw data. This
    creates a unique, timestamped "run" directory for the results.
3.  **Analyze a Run**: Use `python -m doxascope.cli analyze` to generate detailed plots for a specific training run.
4.  **Compare Runs**: Use `python -m doxascope.cli compare` to generate a single plot that overlays the accuracy curves
    from all training runs for a policy, making it easy to see how performance changes over time.

## Directory Structure

All data generated by the Doxascope is stored in the `train_dir/doxascope/` directory, which is ignored by Git to
prevent large data files from being committed to the repository.

- `train_dir/doxascope/raw_data/<policy_name>/`: Contains raw JSON logs from simulation runs. Each simulation generates
  a unique file.
- `train_dir/doxascope/results/<policy_name>/<run_name>/`: Contains all artifacts from a single training run, including
  the trained model, preprocessed data cache, and analysis plots.
- `train_dir/doxascope/sweeps/<policy_name>/<sweep_name>/`: Contains the results from a hyperparameter sweep.

## Usage

The Doxascope module provides a unified command-line interface (CLI) for all its functionalities. You can run it using
`python -m doxascope.cli`.

```bash
python -m doxascope.cli <command> [options]
```

### 1. Data Collection

To enable data collection, add the `doxascope_enabled=true` flag when running an evaluation using the main `run.py`
script. The logger will automatically save agent memory and position data from the simulation run.

**Usage:**

```bash
uv run ./tools/run.py experiments.recipes.arena.evaluate policy_uri=<path_to_policy> doxascope_enabled=true
```

When you run an evaluation with this flag, the logger will save the raw data to
`train_dir/doxascope/raw_data/<policy_name>/`. You can run the evaluation multiple times to accumulate more data.

### 2. Training the Doxascope Network

Once you have collected data, train a network using the `train` command.

**Usage:**

```bash
python -m doxascope.cli train <policy_name> [options]
```

**Key Options:**

- `--num-future-timesteps <n>`: Number of future positions to predict (default: 1).
- `--num-past-timesteps <n>`: Number of past positions to predict (default: 0).
- `--run-name <name>`: A unique name for this training run. If not provided, a timestamp will be used.
- `--train-random-baseline`: Enable training of a baseline model with random memory vectors (default: enabled).
- `--force-reprocess`: Force reprocessing of raw data even if a preprocessed cache exists.

**Data & Training Options:**

- `--raw-data-dir <path>`: Directory containing raw data files (default: `train_dir/doxascope/raw_data`).
- `--output-dir <path>`: Directory to save results (default: `train_dir/doxascope/results`).
- `--test-split <ratio>`: Proportion of data for testing (default: 0.15).
- `--val-split <ratio>`: Proportion of data for validation (default: 0.15).
- `--batch-size <n>`: Batch size for training (default: 32).
- `--learning-rate <lr>`: Learning rate for optimizer (default: 0.001).
- `--num-epochs <n>`: Maximum number of training epochs (default: 100).
- `--patience <n>`: Early stopping patience (default: 10).
- `--device <device>`: Training device: 'cpu', 'cuda', or 'auto' (default: 'auto').

**Model Architecture Options:**

- `--hidden_dim <n>`: Hidden layer dimension (default: 512).
- `--dropout_rate <rate>`: Dropout rate (default: 0.4).
- `--activation_fn <fn>`: Activation function: 'relu', 'silu', or 'gelu' (default: 'silu').
- `--main_net_depth <n>`: Depth of main network (default: 3).
- `--processor_depth <n>`: Depth of state processors (default: 1).

### 3. Analysis

The `analyze` command provides tools for analyzing and comparing training runs. It features an interactive mode to help
you select which policy and run to analyze.

#### Analyzing a Single Run

To generate all standard plots for a specific run, you can provide the policy and run name directly.

**Usage:**

```bash
python -m doxascope.cli analyze <policy_name> <run_name>
```

**Interactive Mode:**

If you run the command without arguments, it will guide you through the process:

```bash
python -m doxascope.cli analyze
```

The script will scan for available policies and runs and present you with a numbered list to choose from. This is the
easiest way to analyze a specific result without needing to remember or type out full directory names.

#### Comparing Runs and Policies

The `compare` command allows you to generate plots that compare multiple training runs.

**1. Compare all training runs for a _single_ policy:**

This is useful for seeing how performance improves as more training data is collected over time.

**Usage:**

```bash
python -m doxascope.cli compare <policy_name>
```

This will generate a `comparison_<policy_name>.png` plot in the policy's main results directory, overlaying the accuracy
curves from each individual training run.

**2. Compare the latest run of _multiple_ policies:**

This is useful for seeing how different policies perform against each other, using the most up-to-date model for each.

**Usage:**

```bash
python -m doxascope.cli compare <policy_1> <policy_2> ...
```

This will find the most recent training run for each policy listed and generate a single plot comparing their accuracy
curves.

### 4. Hyperparameter Sweep

To find the best hyperparameters or network architecture, use the `sweep` command.

**Usage:**

```bash
python -m doxascope.cli sweep <policy_name> <num_future_timesteps> [options]
```

**Key Options:**

- `<policy_name>`: Name of the policy to sweep.
- `<num_future_timesteps>`: Number of future steps to predict.
- `--num-past-timesteps <n>`: Number of past steps to predict (default: 0).
- `--num-configs <n>`: Number of random configurations to test (default: 30).
- `--max-epochs <n>`: Maximum number of epochs for each trial (default: 50).
- `--patience <n>`: Early stopping patience for each trial (default: 10).
- `--sweep-type <type>`: Type of sweep: `hyper` (for hyperparameters) or `arch` (for architecture).

The script will save all sweep results to a unique, timestamped directory under `train_dir/doxascope/sweeps/`.
