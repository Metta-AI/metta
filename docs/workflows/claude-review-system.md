# Claude Review System Documentation

## Overview

The Claude Review System is a suite of AI-powered code review workflows that automatically analyze pull requests for
specific improvements. Built on Anthropic's Claude AI, these workflows provide targeted, actionable feedback without the
noise of traditional linters.

**Key Philosophy**: Only comment when there are genuine improvements to suggest. If everything looks good, stay silent.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PR Event or    â”‚â”€â”€â”€â”€â–¶â”‚   Orchestrator   â”‚â”€â”€â”€â”€â–¶â”‚ Validate PR     â”‚
â”‚  Manual Trigger â”‚     â”‚    Workflow      â”‚     â”‚ Number          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                           â”‚
                                â–¼                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Parallel Review Jobs     â”‚   â”‚ Individual      â”‚
                    â”‚  - README Accuracy        â”‚â”€â”€â–¶â”‚ Review Base     â”‚
                    â”‚  - Code Comments          â”‚   â”‚ Workflow        â”‚
                    â”‚  - Type Annotations       â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  - Einops Suggestions     â”‚           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â–¼
                                â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â–¼                   â”‚ Claude Analysis â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ (Conditional)   â”‚
                    â”‚  Consolidation Script     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  - Download artifacts     â”‚           â”‚
                    â”‚  - Merge suggestions      â”‚           â–¼
                    â”‚  - Create GitHub review   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ Artifact Upload â”‚
                                â”‚                   â”‚ (if issues)     â”‚
                                â–¼                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Unified GitHub  â”‚
                        â”‚ PR Review       â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## System Components

### 1. Orchestrator Workflow (`claude-review-orchestrator.yml`)

The main entry point that:

- Validates PR context (fails fast if no valid PR number)
- Runs all review types in parallel
- Consolidates results into a single GitHub review
- Provides summary in GitHub Actions

### 2. Base Workflow (`claude-review-base.yml`)

Shared foundation for all review types:

- Handles PR checkout and file filtering
- Sets up optional Python environment
- Runs Claude analysis with specialized prompts
- Uploads artifacts only when issues are found

### 3. Individual Review Workflows

Four specialized review types, each a thin wrapper around the base:

- `claude-review-readme.yml` - Documentation accuracy
- `claude-review-comments.yml` - Comment cleanup
- `claude-review-einops.yml` - Tensor operation improvements
- `claude-review-types.yml` - Type annotation coverage

### 4. Consolidation Script (`claude_review.py`)

Single Python script that:

- Downloads all review artifacts
- Merges suggestions
- Creates unified GitHub review with inline comments
- Falls back to PR comment if review API fails

## Review Types

### 1. ðŸ“ README Accuracy Review

**Purpose**: Ensures documentation stays accurate when code changes.

**Triggers**:

- Pull request opened or reopened
- Manual workflow dispatch

**What it checks**:

- Commands or CLI usage that no longer work
- Installation instructions that are now incorrect
- Removed dependencies still documented
- API endpoints or functions that were removed/renamed
- File paths that no longer exist
- Examples that would throw errors

**What it ignores**:

- Missing documentation (doesn't suggest additions)
- Style or formatting issues
- Opportunities for better documentation

### 2. ðŸ’¬ Code Comments Review

**Purpose**: Identifies and removes unnecessary comments that clutter code.

**Triggers**:

- Pull request opened or reopened
- Manual workflow dispatch

**Comments flagged for removal**:

- Restating obvious code (e.g., `# increment counter` before `counter += 1`)
- Explaining self-evident operations
- Outdated comments that don't match implementation
- Duplicating information from variable/function names
- Stating obvious commands

**Comments preserved**:

- Explaining WHY something is done
- Important context or warnings
- TODO/FIXME comments (unless obsolete)
- Complex algorithm explanations
- License headers

### 3. ðŸ”„ Einops Suggestions Review

**Purpose**: Suggests using `einops.rearrange` for complex tensor operations.

**Triggers**:

- Pull request opened or reopened (Python files only)
- Manual workflow dispatch

**Good candidates for einops**:

```python
# Complex and unclear:
x = x.permute(0, 2, 1).reshape(batch_size, -1)
# Suggested:
x = rearrange(x, 'b h w -> b (w h)')
```

**Ignored operations**:

- Simple, clear operations (`x.transpose(0, 1)`)
- When existing code is already readable
- Performance-critical sections
- Codebases not using einops

### 4. ðŸ·ï¸ Type Annotations Review

**Purpose**: Identifies missing type annotations that would improve code quality.

**Triggers**:

- Pull request opened or reopened (Python files only)
- Manual workflow dispatch

**Priority levels**:

- **High**: Missing parameter types, public API return types, Optional returns
- **Medium**: Complex functions, non-obvious returns, empty collections
- **Ignored**: Private methods, obvious getters, simple functions, clear inference

**Modern syntax preferred**:

- `list[str]` over `List[str]`
- `type | None` over `Optional[type]`
- `dict[str, int]` over `Dict[str, int]`

## How It Works

### 1. PR Validation

The orchestrator first validates that a PR number exists:

```yaml
validate-pr:
  steps:
    - name: Validate PR Number
      run: |
        PR_NUMBER="${{ inputs.pr_number || github.event.pull_request.number }}"
        if [ -z "$PR_NUMBER" ] || [ "$PR_NUMBER" = "0" ]; then
          echo "âŒ Error: No valid PR number found"
          exit 1
        fi
```

### 2. Parallel Review Execution

All review types run simultaneously for efficiency:

- Each review only processes relevant files (based on file patterns)
- Reviews that find no issues create no artifacts
- Claude is instructed to respond with "No issues found" when appropriate

### 3. Artifact-Based Communication

Reviews communicate through artifacts:

- Only created when issues are found
- Contains `claude-review-analysis.json` with suggestions
- Artifacts are downloaded and consolidated by the Python script

### 4. Unified Review Creation

The consolidation script:

1. Downloads all artifacts from the workflow run
2. Merges suggestions from all review types
3. Creates a single GitHub review with inline comments
4. Groups feedback by review type in the summary

## JSON Output Format

When issues are found, Claude creates:

```json
{
  "review_summary": "Brief overall assessment",
  "review_status": "COMMENT|CHANGES_REQUESTED",
  "suggestions": [
    {
      "file": "path/to/file.py",
      "start_line": 23,
      "end_line": 24,
      "side": "RIGHT",
      "severity": "minor|major|blocking|nitpick",
      "reason": "Why this change improves the code",
      "original_code": "current code",
      "suggested_code": "improved code"
    }
  ],
  "tldr": ["Quick summary of changes"]
}
```

## Configuration

### Required Secrets

```yaml
ANTHROPIC_API_KEY # Claude API access key
```

### Required Python Dependencies

```txt
PyGithub>=2.1.1
requests>=2.31.0
```

### Base Workflow Inputs

```yaml
review_name: string # Display name for the review
review_type: string # Unique identifier for artifact naming
file_pattern: string # Regex for file filtering (default: ".*")
setup_python: boolean # Whether to setup Python (default: false)
tools: string # Comma-separated Claude tools
prompt: string # Review-specific instructions
pr_number: string # PR number to review
```

## Usage Examples

### Manual Trigger

```bash
# Using GitHub CLI
gh workflow run "Claude Review: Orchestrator" -f pr_number=123

# Using GitHub UI
# Navigate to Actions â†’ "Claude Review: Orchestrator" â†’ Run workflow â†’ Enter PR number
```

### Direct Review Type Trigger

```bash
# Run only the type annotations review
gh workflow run "Claude Review: Types" -f pr_number=123
```

### Creating a Custom Review

1. Create workflow file: `.github/workflows/claude-review-security.yml`

```yaml
name: 'Claude Review: Security'
on:
  workflow_call:
    inputs:
      pr_number:
        required: true
        type: string

jobs:
  review:
    uses: ./.github/workflows/claude-review-base.yml
    with:
      review_name: 'Security Analysis'
      review_type: 'security'
      file_pattern: "\\.(py|js|ts)$"
      tools: 'Edit,Replace,Bash(git diff HEAD~1)'
      pr_number: ${{ inputs.pr_number }}
      prompt: |
        Review code for security vulnerabilities.

        **CRITICAL INSTRUCTIONS**:
        1. If you find NO security issues:
           - Simply respond with "No issues found."
           - DO NOT create any review
        2. ONLY create a review if you find actual vulnerabilities

        **Security issues to flag**:
        - SQL injection vulnerabilities
        - XSS possibilities
        - Hardcoded credentials
        - Insecure random number generation
        - Path traversal risks

        **What to ignore**:
        - Best practices that aren't vulnerabilities
        - Performance issues
        - Code style
```

2. Add to orchestrator's review list
3. Update the `review_types` array in `claude_review.py`

## Best Practices

### 1. Writing Review Prompts

**DO:**

- Start with clear "no issues found" instructions
- Define specific criteria for flagging issues
- Provide concrete examples of patterns to find
- Focus on actionable improvements
- Use severity levels appropriately

**DON'T:**

- Ask for general "improvements"
- Flag style preferences
- Suggest additions (only fixes)
- Create reviews for minor issues
- Be vague about what to look for

### 2. File Pattern Strategy

Use specific patterns to reduce analysis time:

```yaml
file_pattern: "\\.py$"           # Python files only
file_pattern: "\\.(js|ts)$"      # JavaScript and TypeScript
file_pattern: "^src/.*\\.py$"    # Python files in src/ directory
```

### 3. Tool Selection

**Common tool combinations:**

```yaml
# Basic code review
tools: "Edit,Replace,Bash(git diff HEAD~1)"

# With file discovery
tools: "Edit,Replace,Bash(git diff HEAD~1),Bash(find . -name '*.py')"

# With tool verification
tools: "Edit,Replace,Bash(git diff HEAD~1),Bash(python -m mypy --version)"
```

## Performance Considerations

1. **Parallel Execution**: All reviews run simultaneously
2. **File Filtering**: Use patterns to reduce scope
3. **Early Exit**: No artifact creation when no issues found
4. **Single API Call**: One GitHub review instead of multiple
5. **Conditional Steps**: Python setup only when needed

## Troubleshooting

### No Review Appearing

1. **Check PR validation**:

   - Ensure PR number is valid
   - Check orchestrator logs for validation errors

2. **Verify issues were found**:

   - Check individual review job logs
   - Look for "No issues found" responses
   - Verify artifacts were created

3. **Review consolidation script logs**:
   - Check for artifact download failures
   - Verify PR permissions
   - Look for API rate limit errors

### Suggestions Not Inline

Common causes:

- File not in PR diff
- Incorrect line numbers
- GitHub API validation failure

Debug by checking:

- Skipped suggestions in review body
- Script output for specific errors
- PR file list vs suggestion files

### Performance Issues

1. **Reduce file scope**: Use specific file patterns
2. **Check for large diffs**: Very large PRs may timeout
3. **Monitor API limits**: Both Claude and GitHub have rate limits

## System Limitations

1. **Artifact size**: Large PRs with many issues may hit limits
2. **API rate limits**: Both Claude and GitHub endpoints
3. **Concurrent reviews**: Maximum 4 parallel reviews currently
4. **File patterns**: Regex complexity affects performance

## Future Enhancements

Potential improvements:

1. **Incremental reviews**: Only analyze changed lines
2. **Custom severity thresholds**: Configure what triggers reviews
3. **Team preferences**: `.claude-review.yml` configuration files
4. **Review metrics**: Track suggestion acceptance rates
5. **Caching layer**: Reuse reviews for unchanged files
6. **More review types**: Security, performance, accessibility
7. **IDE integration**: Direct integration with VS Code/IntelliJ

## Maintenance

### Adding Review Types

1. Create workflow following the pattern
2. Add to orchestrator job list
3. Update `review_types` array in Python script
4. Test with manual dispatch
5. Document in this guide

### Updating Claude Model

Change in `claude-review-base.yml`:

```yaml
env:
  CLAUDE_MODEL: 'claude-sonnet-4-20250514'
```

### Monitoring Usage

Track in GitHub Actions:

- Review frequency per type
- Issues found vs PRs reviewed
- Time to complete reviews
- API usage patterns
