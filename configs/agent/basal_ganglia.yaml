# Basal Ganglia Model Configuration
# This implements a two-layer architecture where:
# 1. Upstream Network (Reward Generator): Learns to generate dense reward signals
# 2. Downstream Network (Policy Learner): Standard policy network that receives augmented rewards

_target_: metta.agent.metta_agent.MettaAgent

observations:
  obs_key: grid_obs

clip_range: 0 # set to 0 to disable clipping
analyze_weights_interval: 300
l2_init_weight_update_interval: 0

components:
  # Input processing layers
  _obs_:
    _target_: metta.agent.lib.basal_ganglia_reward_shaper.GridObservationShaper
    sources:
      null

  obs_normalizer:
    _target_: metta.agent.lib.observation_normalizer.ObservationNormalizer
    sources:
      - name: _obs_

  # Observation encoding for both networks
  cnn1:
    _target_: metta.agent.lib.nn_layer_library.Conv2d
    sources:
      - name: obs_normalizer
    nn_params:
      out_channels: 64
      kernel_size: 3
      stride: 1

  cnn2:
    _target_: metta.agent.lib.nn_layer_library.Conv2d
    sources:
      - name: cnn1
    nn_params:
      out_channels: 64
      kernel_size: 3
      stride: 1

  obs_flattener:
    _target_: metta.agent.lib.nn_layer_library.Flatten
    sources:
      - name: cnn2

  encoded_obs:
    _target_: metta.agent.lib.nn_layer_library.Linear
    sources:
      - name: obs_flattener
    nn_params:
      out_features: 128

  # Upstream Network (Reward Generator) - Basal Ganglia
  # This generates intrinsic rewards based on observations
  intrinsic_reward:
    _target_: metta.agent.lib.basal_ganglia_reward_shaper.BasalGangliaRewardShaper
    sources:
      - name: encoded_obs
    reward_scale: 0.1
    novelty_weight: 0.3
    exploration_weight: 0.3
    mastery_weight: 0.4

  # Downstream Network (Policy Learner) - Cortex
  # This processes observations and intrinsic rewards together
  policy_representation:
    _target_: metta.agent.lib.basal_ganglia_reward_shaper.BasalGangliaPolicyNetwork
    sources:
      - name: encoded_obs
      - name: intrinsic_reward

  # Main core output for both value and action
  _core_:
    _target_: metta.agent.lib.lstm.LSTM
    sources:
      - name: policy_representation
    output_size: 128
    nn_params:
      num_layers: 2

  # Value head
  critic_1:
    _target_: metta.agent.lib.nn_layer_library.Linear
    sources:
      - name: _core_
    nn_params:
      out_features: 512
    nonlinearity: nn.Tanh

  _value_:
    _target_: metta.agent.lib.nn_layer_library.Linear
    sources:
      - name: critic_1
    nn_params:
      out_features: 1
    nonlinearity: null

  # Action head
  actor_1:
    _target_: metta.agent.lib.nn_layer_library.Linear
    sources:
      - name: _core_
    nn_params:
      out_features: 512

  _action_embeds_:
    _target_: metta.agent.lib.action.ActionEmbedding
    sources:
      null
    nn_params:
      num_embeddings: 100
      embedding_dim: 16

  _action_:
    _target_: metta.agent.lib.actor.MettaActorSingleHead
    sources:
      - name: actor_1
      - name: _action_embeds_
